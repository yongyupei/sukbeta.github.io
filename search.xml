<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>hadoop上命令行查看lzo、gz、bz文件</title>
      <link href="/hadoop-lzo-gz-bz2/"/>
      <url>/hadoop-lzo-gz-bz2/</url>
      <content type="html"><![CDATA[<h2 id="hadoop上命令行查看lzo、gz、bz文件"><a href="#hadoop上命令行查看lzo、gz、bz文件" class="headerlink" title="hadoop上命令行查看lzo、gz、bz文件"></a>hadoop上命令行查看lzo、gz、bz文件</h2><p>HDFS上的文件可能是压缩的，所以用cat不能直接查看。hadoop上默认支持lzo、gz、bz2、snappy压缩格式。</p><p>我们用命令行查看HDFS上压缩文件，也是可以的。</p><h4 id="lzo文件"><a href="#lzo文件" class="headerlink" title="lzo文件"></a>lzo文件</h4><ul><li>查看 HDFS 上 lzo 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /Data/Logs/2018-08-22/2018-08-22_log.lzo | lzop -dc | head -1</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /Data/Logs/2018-08-22/2018-08-22_log.lzo | lzop -dc | head -1     lzop 会接受输入流然后解压输出流给head显示第一行</span><br></pre></td></tr></table></figure><h4 id="gzip-文件"><a href="#gzip-文件" class="headerlink" title="gzip 文件"></a>gzip 文件</h4><ul><li>查看 HDFS 上 gzip 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /shining/temp.txt.gz | gzip -d </span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /shining/temp.txt.gz | zcat</span><br></pre></td></tr></table></figure><h4 id="bz2-文件"><a href="#bz2-文件" class="headerlink" title="bz2 文件"></a>bz2 文件</h4><ul><li>查看 HDFS 上 bz2 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /temp/b.bz2 | bzip2 -d</span><br></pre></td></tr></table></figure><h4 id="text-命令"><a href="#text-命令" class="headerlink" title="text 命令"></a>text 命令</h4><p>Hadoop text 命令可以查看HDFS上的文本、压缩文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text /temp/b.bz2</span><br><span class="line"></span><br><span class="line">hadoop fs -text /temp/test_hive.txt.gz</span><br><span class="line"></span><br><span class="line">hadoop fs -text /temp/l.lzo</span><br><span class="line"></span><br><span class="line">hadoop fs -text /tmp/out1/part-r-00000</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title></title>
      <link href="/hadoop%E4%B8%8A%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9F%A5%E7%9C%8Blzo%E3%80%81gz%E3%80%81bz%E6%96%87%E4%BB%B6/"/>
      <url>/hadoop%E4%B8%8A%E5%91%BD%E4%BB%A4%E8%A1%8C%E6%9F%A5%E7%9C%8Blzo%E3%80%81gz%E3%80%81bz%E6%96%87%E4%BB%B6/</url>
      <content type="html"><![CDATA[<h2 id="hadoop上命令行查看lzo、gz、bz文件"><a href="#hadoop上命令行查看lzo、gz、bz文件" class="headerlink" title="hadoop上命令行查看lzo、gz、bz文件"></a>hadoop上命令行查看lzo、gz、bz文件</h2><p>HDFS上的文件可能是压缩的，所以用cat不能直接查看。hadoop上默认支持lzo、gz、bz2、snappy压缩格式。</p><p>我们用命令行查看HDFS上压缩文件，也是可以的。</p><h4 id="lzo文件"><a href="#lzo文件" class="headerlink" title="lzo文件"></a>lzo文件</h4><ul><li>查看 HDFS 上 lzo 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /Data/Logs/2018-08-22/2018-08-22_log.lzo | lzop -dc | head -1</span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /Data/Logs/2018-08-22/2018-08-22_log.lzo | lzop -dc | head -1     lzop 会接受输入流然后解压输出流给head显示第一行</span><br></pre></td></tr></table></figure><h4 id="gzip-文件"><a href="#gzip-文件" class="headerlink" title="gzip 文件"></a>gzip 文件</h4><ul><li>查看 HDFS 上 gzip 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /shining/temp.txt.gz | gzip -d </span><br><span class="line"></span><br><span class="line">或</span><br><span class="line"></span><br><span class="line">hadoop fs -cat /shining/temp.txt.gz | zcat</span><br></pre></td></tr></table></figure><h4 id="bz2-文件"><a href="#bz2-文件" class="headerlink" title="bz2 文件"></a>bz2 文件</h4><ul><li>查看 HDFS 上 bz2 文件的命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /temp/b.bz2 | bzip2 -d</span><br></pre></td></tr></table></figure><h4 id="text-命令"><a href="#text-命令" class="headerlink" title="text 命令"></a>text 命令</h4><p>Hadoop text 命令可以查看HDFS上的文本、压缩文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -text /temp/b.bz2</span><br><span class="line"></span><br><span class="line">hadoop fs -text /temp/test_hive.txt.gz</span><br><span class="line"></span><br><span class="line">hadoop fs -text /temp/l.lzo</span><br><span class="line"></span><br><span class="line">hadoop fs -text /tmp/out1/part-r-00000</span><br></pre></td></tr></table></figure>]]></content>
      
      
    </entry>
    
    <entry>
      <title>web</title>
      <link href="/web/"/>
      <url>/web/</url>
      <content type="html"><![CDATA[<h5 id="本页面为个人收藏的常用web访问-持续更新-……"><a href="#本页面为个人收藏的常用web访问-持续更新-……" class="headerlink" title="本页面为个人收藏的常用web访问  持续更新 ……"></a>本页面为个人收藏的常用web访问  持续更新 ……</h5><h5 id="常用网站"><a href="#常用网站" class="headerlink" title="常用网站"></a><center>常用网站</center></h5><table><thead><tr><th style="text-align:left">web url</th><th style="text-align:center">web url</th><th style="text-align:right">web url</th></tr></thead><tbody><tr><td style="text-align:left"><a href="https://www.baidu.com/" target="_blank" rel="noopener">Baidu</a></td><td style="text-align:center"><a href="https://www.google.com" target="_blank" rel="noopener">Google</a></td><td style="text-align:right"><a href="https://cn.bing.com" target="_blank" rel="noopener">Bing</a></td></tr><tr><td style="text-align:left"><a href="https://bandwagonhost.com/" target="_blank" rel="noopener">搬瓦工</a></td><td style="text-align:center"><a href="https://python3-cookbook.readthedocs.io/zh_CN/latest/" target="_blank" rel="noopener">python coolbook</a></td><td style="text-align:right"><a href="https://www.hbg.com/zh-cn/" target="_blank" rel="noopener">火币网</a></td></tr><tr><td style="text-align:left"><a href="https://github.com/shadowsocks" target="_blank" rel="noopener">Shadowsocks github</a></td><td style="text-align:center"><a href="https://sourceforge.net/projects/shadowsocksgui/files/dist/" target="_blank" rel="noopener">Shadowsocks sourceforge</a></td><td style="text-align:right"><a href="https://www.coingecko.com/zh" target="_blank" rel="noopener">虚拟货币走势</a></td></tr><tr><td style="text-align:left"><a href="https://www.processon.com/" target="_blank" rel="noopener">在线画图 processon</a></td><td style="text-align:center"></td></tr></tbody></table><hr><h5 id="大数据相关"><a href="#大数据相关" class="headerlink" title="大数据相关"></a><center>大数据相关</center></h5><table><thead><tr><th style="text-align:left">web url</th><th style="text-align:center">web url</th><th style="text-align:right">web url</th></tr></thead><tbody><tr><td style="text-align:left"><a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html" target="_blank" rel="noopener">ResourceManager API</a></td><td style="text-align:center"><a href="http://hadoop.apache.org/docs/r1.0.4/webhdfs.html" target="_blank" rel="noopener">WebHDFS API</a></td><td style="text-align:right"><a href="https://hadoop.apache.org/docs/r1.0.4/cn/commands_manual.html" target="_blank" rel="noopener">Apache Hadoop 命令手册</a></td></tr><tr><td style="text-align:left"><a href="http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/HistoryServerRest.html#Overview" target="_blank" rel="noopener">History Server API</a></td><td style="text-align:center"><a href="https://mesos-cn.gitbooks.io/mesos-cn/content/" target="_blank" rel="noopener">Mesos 中文手册</a></td><td style="text-align:right"><a href="http://www.apachecn.org/classification/" target="_blank" rel="noopener">ApacheCN 社区</a></td></tr></tbody></table><hr><h5 id="容器相关"><a href="#容器相关" class="headerlink" title="容器相关"></a><center>容器相关</center></h5><table><thead><tr><th style="text-align:left">web url</th><th style="text-align:center">web url</th><th style="text-align:right">web url</th></tr></thead><tbody><tr><td style="text-align:left"><a href="http://docs.ceph.org.cn/start/quick-start-preflight/" target="_blank" rel="noopener">Ceph中文官网</a></td><td style="text-align:center"><a href="https://www.widuu.com/docker/index.html" target="_blank" rel="noopener">Docker 中文指南</a></td><td style="text-align:right"><a href="https://docs.docker.com/" target="_blank" rel="noopener">Docker 官方文档</a></td></tr><tr><td style="text-align:left"><a href="https://github.com/goharbor/harbor" target="_blank" rel="noopener">Harbor镜像仓库</a></td><td style="text-align:center"></td></tr></tbody></table><hr><h5 id="GM-amp-amp-edu"><a href="#GM-amp-amp-edu" class="headerlink" title=" GM &amp;&amp; edu "></a><center> GM &amp;&amp; edu </center></h5><table><thead><tr><th style="text-align:left">web url</th><th style="text-align:center">web url</th><th style="text-align:right">web url</th></tr></thead><tbody><tr><td style="text-align:left"><a href="http://www.bjrbj.gov.cn/csibiz/indinfo/index.jsp" target="_blank" rel="noopener">北京社保查询</a></td><td style="text-align:center"><a href="https://ipcrs.pbccrc.org.cn/" target="_blank" rel="noopener">个人征信查询</a></td><td style="text-align:right"><a href="https://www.bjhjyd.gov.cn/" target="_blank" rel="noopener">北京小汽车摇号</a></td></tr><tr><td style="text-align:left"><a href="http://www.cdgdc.edu.cn/xwyyjsjyxx/bqxx/265705.shtml" target="_blank" rel="noopener">学位信息网</a></td><td style="text-align:center"><a href="https://www.chsi.com.cn/" target="_blank" rel="noopener">学信网</a></td><td style="text-align:right"><a href="http://gjj.beijing.gov.cn/" target="_blank" rel="noopener">北京住房公积金网</a></td></tr></tbody></table><hr><h5 id="资源-amp-amp-电影"><a href="#资源-amp-amp-电影" class="headerlink" title=" 资源 &amp;&amp; 电影 "></a><center> 资源 &amp;&amp; 电影 </center></h5><table><thead><tr><th style="text-align:left">web url</th><th style="text-align:center">web url</th><th style="text-align:right">web url</th></tr></thead><tbody><tr><td style="text-align:left"><a href="http://www.duniang.tv/" target="_blank" rel="noopener">度娘TV</a></td><td style="text-align:center"><a href="http://hao.xsldh.com/search" target="_blank" rel="noopener">小森林资源搜索</a></td><td style="text-align:right"><a href="https://www.agmj.tv/" target="_blank" rel="noopener">阿哥美剧</a></td></tr><tr><td style="text-align:left"><a href="https://www.cilimao.cc/" target="_blank" rel="noopener">磁力猫</a></td><td style="text-align:center"><a href="https://www.80s.tw/" target="_blank" rel="noopener">80s手机电影</a></td><td style="text-align:right"><a href="http://www.hao6v.com/" target="_blank" rel="noopener">6v电影网</a></td></tr><tr><td style="text-align:left"><a href="https://limetorrents.unblocked.krd/" target="_blank" rel="noopener">lime</a></td><td style="text-align:center"></td></tr></tbody></table>]]></content>
      
      <categories>
          
          <category> web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> web </tag>
            
            <tag> url </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive自定义参数</title>
      <link href="/hive-custom-variables/"/>
      <url>/hive-custom-variables/</url>
      <content type="html"><![CDATA[<h2 id="hive-自定义参数"><a href="#hive-自定义参数" class="headerlink" title="hive 自定义参数"></a>hive 自定义参数</h2><p>有的时候我们需要执行hive的时候带一些参数，以便我们后期任务分析、排查等问题。所以这里我们说一下自定义参数的作用。</p><h3 id="hive-命令指定参数"><a href="#hive-命令指定参数" class="headerlink" title="hive 命令指定参数"></a>hive 命令指定参数</h3><p>执行hive命令的时候指定参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.job.submit.username=shining -e &quot;select count(*) from temp.test_hive where pt=&apos;2019-03-21&apos;;&quot;</span><br></pre></td></tr></table></figure><p>也可以指定多个参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --hiveconf hive.job.submit.username=shining  --hiveconf hive.job.submit.web=sukbeta.github.io  -e &quot;select count(*) from temp.test_hive where pt=&apos;2019-03-31&apos;;&quot;</span><br></pre></td></tr></table></figure><p>这时你可以在yarn中找到job的页面，在job的 Configuration 页面中可以查找到你得传的参数。</p><p>如：<br><img src="/images/hive-custom-variables-1.png" alt="hive-custom-variables-images"></p><h3 id="全局配置"><a href="#全局配置" class="headerlink" title="全局配置"></a>全局配置</h3><p>可以修改 hive 命令中的 HIVE_OPTS， 这样执行所有hive的命令都添加这个参数了。</p><p>例如：<br>编辑 hive 命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim $HIVE_HOME/bin/hive</span><br><span class="line"></span><br><span class="line">添加：</span><br><span class="line"></span><br><span class="line">HIVE_OPTS=&quot;$HIVE_OPTS --hiveconf scheduler.job.submit.username=$&#123;BUILD_USER_ID&#125; --hiveconf scheduler.job.submit.jobname=$&#123;JOB_NAME&#125;&quot;</span><br></pre></td></tr></table></figure><ul><li>BUILD_USER_ID 和 JOB_NAME 是两个参数，需要有这两个参数。否则是查不到的。</li></ul><p>我是用jenkins调用的hive，jenkins会自动传入 JOB_NAME 参数的，所有就能取到。</p><p>在jenkins上安装 Build User Vars Plugin 插件可以获取一些执行用户信息。</p><p>可以参考插件说明 ：<br><a href="https://wiki.jenkins.io/display/JENKINS/Build+User+Vars+Plugin" target="_blank" rel="noopener">https://wiki.jenkins.io/display/JENKINS/Build+User+Vars+Plugin</a></p><p>hive自定义参数这个更能用好了还是很强大的。</p>]]></content>
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>elasticsearch导出、导入工具-elasticdump</title>
      <link href="/elasticsearch-elasticdump/"/>
      <url>/elasticsearch-elasticdump/</url>
      <content type="html"><![CDATA[<h2 id="elasticsearch导出、导入工具-elasticdump"><a href="#elasticsearch导出、导入工具-elasticdump" class="headerlink" title="elasticsearch导出、导入工具-elasticdump"></a>elasticsearch导出、导入工具-elasticdump</h2><p>elasticsearch 数据导入到本地，或本地数据导入到elasticsearch中，或集群间的数据迁移，可以用elasticsearch的工具—elasticdump</p><p>elasticdump github 地址： <a href="https://github.com/taskrabbit/elasticsearch-dump?utm_source=dbweekly&amp;utm_medium=email" target="_blank" rel="noopener">https://github.com/taskrabbit/elasticsearch-dump?utm_source=dbweekly&amp;utm_medium=email</a></p><p>elasticdump 可以用用npm安装本地运行，也可以用docker容器运行。在这里我说一下npm安装本地运行、docker运行可以参考github文章。</p><h4 id="npm-安装-elasticdump"><a href="#npm-安装-elasticdump" class="headerlink" title="npm 安装 elasticdump"></a>npm 安装 elasticdump</h4><p>先下载安装npm</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># 以下我尽量都用绝对路径吧。 发现好多文章里的路径写的不清晰......</span><br><span class="line">cd /home/shining</span><br><span class="line">wget https://nodejs.org/dist/v8.11.2/node-v8.11.2-linux-x64.tar.xz</span><br><span class="line">tar -xf node-v8.11.2-linux-x64.tar.xz</span><br><span class="line">cd /home/shining/node-v8.11.2-linux-x64/bin</span><br><span class="line">ln -s /home/shining/node-v8.11.2-linux-x64/bin/npm /usr/local/bin/npm</span><br><span class="line">ln -s /home/shining/node-v8.11.2-linux-x64/bin/node /usr/local/bin/node</span><br><span class="line"></span><br><span class="line"># 安装 elasticdump</span><br><span class="line">./npm init -f</span><br><span class="line"></span><br><span class="line">./npm install elasticdump</span><br><span class="line"># 如果你需要全局安装的话就添加 -g 参数， 我这里没有配置全局。</span><br><span class="line"></span><br><span class="line">cd /home/shining/node-v8.11.2-linux-x64/bin/node_modules/elasticdump/bin</span><br><span class="line"></span><br><span class="line">./elasticdump --help  </span><br><span class="line"></span><br><span class="line"># 这样 elasticdump 就安装好了</span><br></pre></td></tr></table></figure><p>导出数据， 他在他的官网中已经介绍的很详细了， es导出到es，es导出到文件，导出数据直接压缩等方式。</p><p>主要记住的是， 导出的时候不仅仅的data，还需要导出mapping信息。</p><h4 id="导出数据到文件："><a href="#导出数据到文件：" class="headerlink" title="导出数据到文件："></a>导出数据到文件：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">elasticdump \</span><br><span class="line">  --input=http://172.16.3.5:9200/shining_index \</span><br><span class="line">  --output=/data/shining_index_mapping.json \</span><br><span class="line">  --type=mapping</span><br><span class="line">elasticdump \</span><br><span class="line">  --input=http://172.16.3.5:9200/shining_index \</span><br><span class="line">  --output=/data/shining_index.json \</span><br><span class="line">  --type=data</span><br></pre></td></tr></table></figure><p>这样的话数据和mapping信息就都导出来了。</p><p>For Example：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">导出Mapping信息  </span><br><span class="line">elasticdump --ignore-errors=true  --scrollTime=120m  --bulk=true --input=http://10.10.20.164:9200/xmonitor-2015.04.29   --output=http://192.168.100.72:9200/xmonitor-prd-2015.04.29  --type=mapping  </span><br><span class="line">  </span><br><span class="line">导出数据  </span><br><span class="line">elasticdump --ignore-errors=true  --scrollTime=120m  --bulk=true --input=http://10.10.20.164:9200/xmonitor-2015.04.28   --output=/usr/local/esdump/node-v0.12.2-linux-x64/data/xmonitor-prd-2015.04.28.json --type=data  </span><br><span class="line">  </span><br><span class="line">导出数据到本地集群  </span><br><span class="line">elasticdump --ignore-errors=true  --scrollTime=120m  --bulk=true --input=http://10.10.20.164:9200/xmonitor-2015.04.29   --output=http://192.168.100.72:9200/xmonitor-prd-2015.04.29 --type=data </span><br><span class="line"></span><br><span class="line">迁移mapping</span><br><span class="line">./elasticdump  --input=http://10.214.228.44:9200/ehruserindex --output=http://10.214.226.64:9200/ehruserindex --type=mapping</span><br><span class="line">迁移数据</span><br><span class="line">./elasticdump  --input=http://10.214.228.44:9200/ehruserindex --output=http://10.214.226.64:9200/ehruserindex --limit=10000 --type=data</span><br></pre></td></tr></table></figure><h4 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h4><p>正常导入数据是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">elasticdump --input /data/shining_index.json --output http://172.16.3.5:9200/shining_index</span><br></pre></td></tr></table></figure><p>如果你得ES集群配置用有配置： action.auto_create_index 参数为 false 或 为 +aaa<em>,-bbb</em>，’+’号意味着允许创建aaa开头的索引，’-‘号意味着不允许创建bbb开头的索引 有规则的话（详细可以查看这个参数的说明和配置）， 会导致导入失败。</p><p>这时候需要先创建索引和mappping之后再导入数据。</p><p>先编辑一下我们导出来的mapping.json文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /data/shining_index_mapping.json</span><br><span class="line">数据类似于：</span><br><span class="line">&#123;&quot;shining_index&quot;:&#123;&quot;mappings&quot;:&#123;&quot;logs&quot;.............&#125;&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">需要保留 mappings 之后的信息</span><br><span class="line">类似于：</span><br><span class="line">&#123;&quot;mappings&quot;:&#123;&quot;logs&quot;.............&#125;&#125;&#125;&#125;</span><br><span class="line"></span><br><span class="line">删除 &#123;&quot;shining_index&quot;: 和 最后一个 &#125;</span><br></pre></td></tr></table></figure><p>创建索引</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -X PUT &apos;http://172.16.3.5:9200/shining_index&apos; -d@/data/shining_index_mapping.json</span><br></pre></td></tr></table></figure><p>创建成功之后再导入数据就可以了。就不会报错了。</p>]]></content>
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kafka查看消费组消费详情</title>
      <link href="/kafka-console-groups/"/>
      <url>/kafka-console-groups/</url>
      <content type="html"><![CDATA[<p>kafka 在 0.9 版本之后，kafka的消费者组和offset信息就不存zookeeper了。</p><h4 id="0-9-之前版本查看所有消费组："><a href="#0-9-之前版本查看所有消费组：" class="headerlink" title="0.9 之前版本查看所有消费组："></a>0.9 之前版本查看所有消费组：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">./kafka-consumer-groups.sh --zookeeper 127.0.0.1:2181 --list</span><br><span class="line"></span><br><span class="line">erro-info-group</span><br><span class="line">console-consumer-64036</span><br><span class="line">console-consumer-98298</span><br><span class="line">logstash-new</span><br><span class="line">console-consumer-89310</span><br><span class="line">console-consumer-48800</span><br></pre></td></tr></table></figure><h4 id="新版本-0-9-版本之后查看所有消费组"><a href="#新版本-0-9-版本之后查看所有消费组" class="headerlink" title="新版本 0.9 版本之后查看所有消费组"></a>新版本 0.9 版本之后查看所有消费组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --list </span><br><span class="line"></span><br><span class="line">test-info</span><br><span class="line">nginxlog</span><br></pre></td></tr></table></figure><h4 id="显示某个消费组的消费详情（仅支持offset存储在zookeeper上的）"><a href="#显示某个消费组的消费详情（仅支持offset存储在zookeeper上的）" class="headerlink" title="显示某个消费组的消费详情（仅支持offset存储在zookeeper上的）"></a>显示某个消费组的消费详情（仅支持offset存储在zookeeper上的）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --zkconnect localhost:2181 --group logstash-new</span><br></pre></td></tr></table></figure><h4 id="显示某个消费组的消费详情（支持0-9版本-）"><a href="#显示某个消费组的消费详情（支持0-9版本-）" class="headerlink" title="显示某个消费组的消费详情（支持0.9版本+）"></a>显示某个消费组的消费详情（支持0.9版本+）</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server localhost:9092 --describe --group nginxlog</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop常用命令(整理笔记)</title>
      <link href="/hadoop-command/"/>
      <url>/hadoop-command/</url>
      <content type="html"><![CDATA[<h1 id="hadoop常用命令（整理笔记）"><a href="#hadoop常用命令（整理笔记）" class="headerlink" title="hadoop常用命令（整理笔记）"></a>hadoop常用命令（整理笔记）</h1><p>个人笔记，自己查看使用。就不一一整理了。</p><h3 id="hadoop"><a href="#hadoop" class="headerlink" title="hadoop"></a>hadoop</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">hdfs zkfc -formatZK（格式化zookeeper）</span><br><span class="line">hadoop-daemon.sh start journalnode(启动journalnode)</span><br><span class="line">hdfs namenode -format（格式化namenode metadata）</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start namenode -bootstrapStandby( standy namenode)</span><br><span class="line">stop-dfs.sh</span><br><span class="line">hadoop fsck / (检测hadoop数据状态)</span><br><span class="line">start-yarn.sh (相当jobtracker)</span><br><span class="line">start-dfs.sh (启动hdfs)</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver (启动historyserver 服务)</span><br><span class="line"></span><br><span class="line">hdfs haadmin -failover nn2 nn1(切换namenode)</span><br><span class="line">hdfs zkfc -formatZK（格式化zookeeper）</span><br><span class="line"> </span><br><span class="line">hadoop-daemon.sh start journalnode(启动journalnode)</span><br><span class="line">hdfs namenode -format（格式化namenode metadata）</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start namenode -bootstrapStandby( standy namenode)</span><br><span class="line">stop-dfs.sh</span><br><span class="line">hadoop fsck / (检测hadoop数据状态)</span><br><span class="line">start-yarn.sh (相当jobtracker)</span><br><span class="line">start-dfs.sh (启动hdfs)</span><br><span class="line"> </span><br><span class="line">mr-jobhistory-daemon.sh start historyserver   启动historyserver 服务 (在 mapred-site.xml 里配置的， 在哪台机器上就在哪台机器上启动)</span><br><span class="line"></span><br><span class="line">bin/hadoop dfsadmin -safemode enter   将集群置于安全模式</span><br><span class="line"></span><br><span class="line">bin/hadoop dfsadmin -report  显示Datanode列表   </span><br><span class="line"></span><br><span class="line">bin/hadoop dfsadmin -decommission datanodename  使Datanode节点 datanodename退役   </span><br><span class="line"></span><br><span class="line">bin/hadoop dfsadmin -help 命令能列出所有当前支持的命令。比如：</span><br><span class="line">    * -report：报告HDFS的基本统计信息。有些信息也可以在NameNode Web服务首页看到。</span><br><span class="line">    * -safemode：虽然通常并不需要，但是管理员的确可以手动让NameNode进入或离开安全模式。</span><br><span class="line">    * -finalizeUpgrade：删除上一次升级时制作的集群备份。</span><br></pre></td></tr></table></figure><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">hadoop-daemon.sh start zkfc</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line"></span><br><span class="line">hdfs haadmin -getServiceState nn1   得到nn1 或 nn2 状态</span><br><span class="line">hdfs haadmin -failover nn2 nn1       (切换namenode)</span><br><span class="line"></span><br><span class="line">DFS Used hadoop文件系统所使用的空间</span><br><span class="line">Non DFS Used 非hadoop文件系统所使用的空间，比如说本身的linux系统使用的，或者存放的其它文件</span><br><span class="line">Configured Capacity = DFS Used + Non-DFSUsed+ DFS Remaining</span><br><span class="line"></span><br><span class="line">hadoop job -kill job_1441084173757_265601    停止正在执行的job</span><br><span class="line"></span><br><span class="line">hadoop checknative -a    检查hdfs lib库，支持的压缩格式等信息</span><br><span class="line"></span><br><span class="line"> 终端命令方式查看文件的块大小</span><br><span class="line">[sukbeta@hadoop02 hadoop-1.1.2]$ ./bin/hadoop fs -stat &quot;%o&quot; TEST/jdk-7u25-linux-x64.gz</span><br><span class="line">51200      单位为B（字节）</span><br><span class="line"></span><br><span class="line">start-balancer.sh  可以执行-threshold参数。 </span><br><span class="line">-threshold参数是指定平衡的阈值。 </span><br><span class="line">-threshold的默认是10，即每个datanode节点的实际hdfs存储使用量/集群hdfs存储量 </span><br><span class="line">hdfs balancer -threshold 5 </span><br><span class="line"></span><br><span class="line">hadoop-daemon.sh start namenode -rollingUpgrade started  如果hadoop里有数据的话， 第一次启动namenode的时候用这个命令。</span><br><span class="line">hadoop-daemon.sh start namenode -rollingUpgrade</span><br><span class="line"></span><br><span class="line">export HADOOP_SLAVE_SLEEP=1</span><br><span class="line">slaves.sh jps    就可以1秒查看一个节点。 HADOOP_SLAVE_SLEEP=2 2秒 </span><br><span class="line"></span><br><span class="line">hdfs namenode -recover  </span><br><span class="line"></span><br><span class="line">统计文件的行数 ， 代表 统计 /tmp/wc/目录下所有文件的行数，并输出到 /tmp/out_wc  查看结果  hadoop fs -cat /tmp/out_wc/* 即可</span><br><span class="line">hadoop jar /home/hadoop/apache-hadoop/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.7.2.jar -input /tmp/wc/ -output /tmp/out_wc -mapper /bin/cat -reducer /usr/bin/wc</span><br><span class="line"></span><br><span class="line">目录配额</span><br><span class="line">hadoop fs -count -q /user/tc   查看配置设置</span><br><span class="line">hadoop dfsadmin -setSpaceQuota 100g  /user/tc  设置配置 100g</span><br><span class="line">hadoop dfsadmin -clrSpaceQuota /user/tc  取消配额</span><br><span class="line"></span><br><span class="line">hadoop fs -setrep 3 /input/test.txt --设置hdfs的文件副本数量</span><br><span class="line">hadoop fs -setrep -R 3 &lt; hdfs path &gt;</span><br><span class="line"></span><br><span class="line">改变一个文件在hdfs中的副本个数，上述命令中数字3为所设置的副本个数，-R选项可以对一个人目录下的所有目录+文件递归执行改变副本个数的操作</span><br><span class="line"></span><br><span class="line">bin/hadoop fs -help command-name  显示关于某个命令的详细信息</span><br><span class="line"></span><br><span class="line">bin/hadoop job -history output-dir 用户可使用以下命令在指定路径下查看历史日志汇总</span><br><span class="line"></span><br><span class="line">hadoop archive -archiveName NAME *</span><br><span class="line">                        -archiveName NAME  要创建的档案的名字。</span><br><span class="line">                        src 文件系统的路径名，和通常含正则表达的一样。</span><br><span class="line">                        dest 保存档案文件的目标目录。</span><br><span class="line"></span><br><span class="line">递归地拷贝文件或目录</span><br><span class="line">  $ hadoop distcp</span><br><span class="line">                  srcurl      源Url</span><br><span class="line">                  desturl     目标Url</span><br><span class="line">                  </span><br><span class="line">运行HDFS文件系统检查工具(fsck tools)</span><br><span class="line"></span><br><span class="line">用法：hadoop fsck [GENERIC_OPTIONS]</span><br><span class="line">命令选项     描述</span><br><span class="line">-move             移动受损文件到/lost+found</span><br><span class="line">-delete     删除受损文件。</span><br><span class="line">-openforwrite     打印出写打开的文件。</span><br><span class="line">-files             打印出正被检查的文件。</span><br><span class="line">-blocks     打印出块信息报告。</span><br><span class="line">-locations     打印出每个块的位置信息。</span><br><span class="line">-racks             打印出data-node的网络拓扑结构。</span><br></pre></td></tr></table></figure><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">yarn rmadmin -getServiceState rm2   得到 rm1 或 rm2 的状态</span><br><span class="line">yarn rmadmin -transitionToActive rm2 --forcemanual      切换 rm2 为主</span><br><span class="line"></span><br><span class="line">yarn node -list</span><br><span class="line">hdfs dfsadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshNodes</span><br><span class="line">yarn rmadmin -refreshQueues  队列刷新</span><br><span class="line"></span><br><span class="line">yarn application -list</span><br><span class="line">yarn application -status application_1499826928702_868175</span><br><span class="line">yarn application -kill application_1499826928702_868175</span><br><span class="line">yarn logs  -applicationId   appid  查看日志</span><br><span class="line"></span><br><span class="line">ws/v1/cluster/apps?states=RUNNING</span><br><span class="line">http://namenode00.host-mtime.com:8088/ws/v1/cluster/apps?states=RUNNING</span><br><span class="line">curl --compressed -H &quot;Accept: application/json&quot; -X GET &quot;http://lyhadoop4.com:8088/ws/v1/cluster/apps/application_1465461051654_0001&quot; </span><br><span class="line"></span><br><span class="line">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html   api 接口</span><br><span class="line"></span><br><span class="line">job 的cpu使用</span><br><span class="line">http://namenode01.host-mtime.com:19888/jobhistory/jobcounters/job_1494493840980_428367 </span><br><span class="line">查找 CPU time spent (ms)  中的 total 值</span><br><span class="line"></span><br><span class="line">job 内存</span><br><span class="line">http://namenode01.host-mtime.com:19888/jobhistory/conf/job_1494493840980_428367</span><br><span class="line">查找 （mapreduce.map.memory.mb * map数） + （mapreduce.reduce.memory.mb * reduce数）</span><br><span class="line">map 和 reduce 数 可以在  http://namenode01.host-mtime.com:19888/jobhistory/job/job_1494493840980_428367  中找到</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">job 的 io</span><br><span class="line">http://namenode01.host-mtime.com:19888/jobhistory/jobcounters/job_1494493840980_428367</span><br><span class="line">查找 FILE: Number of bytes read + FILE: Number of bytes written + HDFS: Number of bytes read + （ HDFS: Number of bytes written * 3 ）</span><br><span class="line">HDFS: Number of bytes written *3 是因为一个文件要写三份。</span><br><span class="line"></span><br><span class="line">job的sql</span><br><span class="line">http://namenode01.host-mtime.com:19888/jobhistory/conf/job_1494493840980_428367</span><br><span class="line">查找 hive.query.string  得到的是 urlencode 可以在转码网站上解码。</span><br><span class="line">正在运行的job查看sql  http://namenode01.host-mtime.com:8088/proxy/application_1499826928702_1021497/mapreduce/job/job_1499826928702_1021497</span><br><span class="line">点击 running -&gt; application ID -&gt; Tracking URL: ApplicationMaster -&gt; job ID -&gt; Configuration 页面中查找 hive.query.string</span><br><span class="line"></span><br><span class="line">shell 解码 urlencode</span><br><span class="line">echo &quot;hive.query.string urlencode 内容&quot; &gt; urlfile.txt</span><br><span class="line">for url in `cat urlfile.txt`</span><br><span class="line">do</span><br><span class="line">printf $(echo -n $url | sed &apos;s/\\/\\\\/g;s/\(%\)\([0-9a-fA-F][0-9a-fA-F]\)/\\x\2/g&apos;)&quot;\n&quot;</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">yarn 测试</span><br><span class="line">1）  hadoop jar /home/hadoop/apache-hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar pi 100 10000</span><br><span class="line"></span><br><span class="line">2） hdfs dfs -put words.txt hdfs://cloud01:9000/</span><br><span class="line">       hadoop jar //home/hadoop/apache-hadoop/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar wordcount  /words.txt /output</span><br><span class="line">       </span><br><span class="line"></span><br><span class="line">用于和Map Reduce作业交互和命令(jar)</span><br><span class="line"></span><br><span class="line">用法：hadoop job [GENERIC_OPTIONS] [-submit ] | [-status ] | [-counter ] | [-kill ] | [-events &lt;#-of-events&gt;] | [-history [all] ] | [-list [all]] | [-kill-task ] | [-fail-task ]</span><br><span class="line">命令选项         描述</span><br><span class="line">-submit      提交作业</span><br><span class="line">-status      打印map和reduce完成百分比和所有计数器。</span><br><span class="line">-counter     打印计数器的值。</span><br><span class="line">-kill      杀死指定作业。</span><br><span class="line">-events &lt;#-of-events&gt;    打印给定范围内jobtracker接收到的事件细节。</span><br><span class="line">-history [all]     -history      打印作业的细节、失败及被杀死原因的细节。更多的关于一个作业的细节比如   成功的任务，做过的任务尝试等信息可以通过指定[all]选项查看。</span><br><span class="line">-list [all]     -list all     显示所有作业。-list只显示将要完成的作业。</span><br><span class="line">-kill-task     杀死任务。被杀死的任务不会不利于失败尝试。</span><br><span class="line">-fail-task    使任务失败。被失败的任务会对失败尝试不利。</span><br></pre></td></tr></table></figure><h3 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark slave 单独启动</span><br><span class="line">./start-slave.sh spark://namenode00.host-mtime.com:7077  </span><br><span class="line"></span><br><span class="line">/sbin/start-slave.sh spark://shdx006:7077,shdx007:7077</span><br><span class="line"></span><br><span class="line">http://spark.apache.org/docs/latest/monitoring.html#rest-api  api 接口</span><br></pre></td></tr></table></figure><h3 id="Hbase"><a href="#Hbase" class="headerlink" title="Hbase"></a>Hbase</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">./bin/hbase-daemon.sh start master         启动master</span><br><span class="line">./bin/hbase-daemon.sh start master --backup   启动baskup</span><br><span class="line"></span><br><span class="line">./hbase-daemon.sh stop master   停掉master </span><br><span class="line">hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>pycharm安装package出现报错：module &#39;pip&#39; has no attribute &#39;main&#39;</title>
      <link href="/pycham-pip-no-attr/"/>
      <url>/pycham-pip-no-attr/</url>
      <content type="html"><![CDATA[<p>python 3.5<br>pip 18.1 更新之后，pycharm安装package出现报错：module ‘pip’ has no attribute ‘main’</p><p>找到安装目录下 helpers/packaging_tool.py文件。 pycham报错信息里会有显示 packaging_tool.py 文件的配置。</p><ul><li>找到下面代码： 第一部分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def do_install(pkgs):</span><br><span class="line">    try:</span><br><span class="line">        import pip</span><br><span class="line">    except ImportError:</span><br><span class="line">        error_no_pip()</span><br><span class="line">    return pip.main([&apos;install&apos;] + pkgs)</span><br></pre></td></tr></table></figure><p>替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def do_install(pkgs):</span><br><span class="line">    try:</span><br><span class="line">        # import pip</span><br><span class="line">        try:</span><br><span class="line">            from pip._internal import main</span><br><span class="line">        except Exception:</span><br><span class="line">            from pip import main</span><br><span class="line">    except ImportError:</span><br><span class="line">        error_no_pip()</span><br><span class="line">    return main([&apos;install&apos;] + pkgs)</span><br></pre></td></tr></table></figure><ul><li>找到下面代码： 第二部分</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def do_uninstall(pkgs):</span><br><span class="line">    try:</span><br><span class="line">        import pip</span><br><span class="line">    except ImportError:</span><br><span class="line">        error_no_pip()</span><br><span class="line">    return pip.main([&apos;uninstall&apos;, &apos;-y&apos;] + pkgs)</span><br></pre></td></tr></table></figure><p>替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def do_uninstall(pkgs):</span><br><span class="line">    try:</span><br><span class="line">        # import pip</span><br><span class="line">        try:</span><br><span class="line">            from pip._internal import main</span><br><span class="line">        except Exception:</span><br><span class="line">            from pip import main</span><br><span class="line">    except ImportError:</span><br><span class="line">        error_no_pip()</span><br><span class="line">    return main([&apos;uninstall&apos;, &apos;-y&apos;] + pkgs)</span><br></pre></td></tr></table></figure><p>保存退出，之后就可以了！</p>]]></content>
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>nginx url跳转、rewrite实例</title>
      <link href="/nginx-url-rewrite/"/>
      <url>/nginx-url-rewrite/</url>
      <content type="html"><![CDATA[<h2 id="nginx-url跳转、rewrite实例"><a href="#nginx-url跳转、rewrite实例" class="headerlink" title="nginx url跳转、rewrite实例"></a>nginx url跳转、rewrite实例</h2><p>nginx 的 rewrite 语法<br>语法: rewrite regex replacement flag<br>默认: none<br>作用域: server, location, if<br>此指令根据表达式来更改URI，或修改字符串。<br>指令根据配置文件中的顺序来执行。<br>﻿注意：<br>重写表达式只对相对路径有效。如果想配对主机名，应该使用if语句。<br>rewrite只是会改写路径部分的东东，不会改动用户的输入参数，因此这里的if规则里面，你无需关心用户在浏览器里输入的参数，rewrite后会自动添加的，因此，只是加上了一个？号和后面我们想要的一个小小的参数 ***https=1就可以了。<br>nginx的rewrite规则参考：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">~ 为区分大小写匹配</span><br><span class="line">~* 为不区分大小写匹配</span><br><span class="line">!~和!~*分别为区分大小写不匹配及不区分大小写不匹</span><br><span class="line">-f和!-f用来判断是否存在文件</span><br><span class="line">-d和!-d用来判断是否存在目录</span><br><span class="line">-e和!-e用来判断是否存在文件或目录</span><br><span class="line">-x和!-x用来判断文件是否可执行</span><br><span class="line">last 相当于Apache里的[L]标记，表示完成rewrite，呵呵这应该是最常用的</span><br><span class="line">break 终止匹配, 不再匹配后面的规则</span><br><span class="line">redirect 返回302临时重定向 地址栏会显示跳转后的地址</span><br><span class="line">permanent 返回301永久重定向 地址栏会显示跳转后的地址</span><br><span class="line">$args</span><br><span class="line">$content_length</span><br><span class="line">$content_type</span><br><span class="line">$document_root</span><br><span class="line">$document_uri</span><br><span class="line">$host</span><br><span class="line">$http_user_agent</span><br><span class="line">$http_cookie</span><br><span class="line">$limit_rate</span><br><span class="line">$request_body_file</span><br><span class="line">$request_method</span><br><span class="line">$remote_addr</span><br><span class="line">$remote_port</span><br><span class="line">$remote_user</span><br><span class="line">$request_filename</span><br><span class="line">$request_uri</span><br><span class="line">$query_string</span><br><span class="line">$scheme</span><br><span class="line">$server_protocol</span><br><span class="line">$server_addr</span><br><span class="line">$server_name</span><br><span class="line">$server_port</span><br><span class="line">$uri</span><br></pre></td></tr></table></figure><p>多目录转成参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if (!-d $request_filename) &#123;</span><br><span class="line">rewrite ^/([a-z-A-Z]+)/([a-z-A-Z]+)/?(.*)$ /index.php?namespace=user&amp;amp;controller=$1&amp;amp;action=$2&amp;amp;$3 last;</span><br><span class="line">rewrite ^/([a-z-A-Z]+)/?$ /index.php?namespace=user&amp;amp;controller=$1 last;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>目录对换</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/123456/xxxx -&gt; /xxxx?id=123456</span><br><span class="line">rewrite ^/(\d+)/(.+)/ /$2?id=$1 last;</span><br></pre></td></tr></table></figure><p>例如下面设定nginx在用户使用ie的使用重定向到/nginx-ie目录下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if ($http_user_agent ~ MSIE) &#123;</span><br><span class="line">rewrite ^(.*)$ /nginx-ie/$1 break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>目录自动加“/”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (-d $request_filename)&#123;</span><br><span class="line">rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>禁止 ht access</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location ~/\.ht &#123;</span><br><span class="line">deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>禁止多个目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">location ~ ^/(cron|templates)/ &#123;</span><br><span class="line">deny all;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>禁止以/data开头的文件<br>可以禁止/data/下多级目录下.log.txt等请求;</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location ~ ^/data &#123;</span><br><span class="line">deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>禁止单个目录<br>不能禁止.log.txt能请求</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location /searchword/cron/ &#123;</span><br><span class="line">deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>禁止单个文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">location ~ /data/sql/data.sql &#123;</span><br><span class="line">deny all;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>给favicon.ico和robots.txt设置过期时间;<br>这里为favicon.ico为99天,robots.txt为7天并不记录404错误日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">location ~(favicon.ico) &#123;</span><br><span class="line">log_not_found off;</span><br><span class="line">expires 99d;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">location ~(robots.txt) &#123;</span><br><span class="line">log_not_found off;</span><br><span class="line">expires 7d;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>设定某个文件的过期时间;这里为600秒，并不记录访问日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">location ^~ /html/scripts/loadhead_1.js &#123;</span><br><span class="line">access_log   off;</span><br><span class="line">root /opt/lampp/htdocs/web;</span><br><span class="line">expires 600;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>文件反盗链并设置过期时间<br>这里的return 412 为自定义的http状态码，默认为403，方便找出正确的盗链的请求<br>“rewrite ^/ <a href="http://www.jbxue.com/leech.gif;”显示一张防盗链图片" target="_blank" rel="noopener">http://www.jbxue.com/leech.gif;”显示一张防盗链图片</a><br>“access_log off;”不记录访问日志，减轻压力<br>“expires 3d”所有文件3天的浏览器缓存</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">location ~* ^.+\.(jpg|jpeg|gif|png|swf|rar|zip|css|js)$ &#123;</span><br><span class="line">valid_referers none blocked *.c1gstudio.com *.c1gstudio.net localhost 208.97.167.194;</span><br><span class="line">if ($invalid_referer) &#123;</span><br><span class="line">rewrite ^/ http://www.jbxue.com/leech.gif;</span><br><span class="line">return 412;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">access_log   off;</span><br><span class="line">root /opt/lampp/htdocs/web;</span><br><span class="line">expires 3d;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>只充许固定ip访问网站，并加上密码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root  /opt/htdocs/www;</span><br><span class="line">allow   208.97.167.194;</span><br><span class="line">allow   222.33.1.2;</span><br><span class="line">allow   231.152.49.4;</span><br><span class="line">deny    all;</span><br><span class="line">auth_basic “C1G_ADMIN”;</span><br><span class="line">auth_basic_user_file htpasswd;</span><br></pre></td></tr></table></figure><p>将多级目录下的文件转成一个文件，增强seo效果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/job-123-456-789.html 指向/job/123/456/789.html</span><br><span class="line">rewrite ^/job-([0-9]+)-([0-9]+)-([0-9]+)\.html$ /job/$1/$2/jobshow_$3.html last;</span><br></pre></td></tr></table></figure><p>将根目录下某个文件夹指向2级目录<br>如/shanghaijob/ 指向 /area/shanghai/<br>如果你将last改成permanent，那么浏览器地址栏显是/location/shanghai/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;</span><br></pre></td></tr></table></figure><p>上面例子有个问题是访问/shanghai 时将不会匹配</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rewrite ^/([0-9a-z]+)job$ /area/$1/ last;</span><br><span class="line">rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;</span><br></pre></td></tr></table></figure><p>这样/shanghai 也可以访问了，但页面中的相对链接无法使用，<br>如./list_1.html真实地址是/area/shanghia/list_1.html会变成/list_1.html,导至无法访问。<br>那我加上自动跳转也是不行咯<br>(-d $request_filename)它有个条件是必需为真实目录，而我的rewrite不是的，所以没有效果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (-d $request_filename)&#123;</span><br><span class="line">rewrite ^/(.*)([^/])$ http://$host/$1$2/ permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>知道原因后就好办了，手动跳转：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rewrite ^/([0-9a-z]+)job$ /$1job/ permanent;</span><br><span class="line">rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2 last;</span><br></pre></td></tr></table></figure><p>文件和目录不存在的时候重定向：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">if (!-e $request_filename) &#123;</span><br><span class="line">proxy_pass http://127.0.0.1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>域名跳转</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server</span><br><span class="line">&#123;</span><br><span class="line">listen       80;</span><br><span class="line">server_name  jump.jbxue.com;</span><br><span class="line">index index.html index.htm index.php;</span><br><span class="line">root  /opt/lampp/htdocs/www;</span><br><span class="line">rewrite ^/ http://www.jbxue.com/;</span><br><span class="line">access_log  off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>多域名转向</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">server_name  www.jbxue.com/  www.jbxue.com/;</span><br><span class="line">index index.html index.htm index.php;</span><br><span class="line">root  /opt/lampp/htdocs;</span><br><span class="line">if ($host ~ “c1gstudio\.net”) &#123;</span><br><span class="line">rewrite ^(.*) http://www.jbxue.com$1/ permanent;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>三级域名跳转</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if ($http_host ~* “^(.*)\.i\.c1gstudio\.com$”) &#123;</span><br><span class="line">rewrite ^(.*) http://top.jbxue.com$1/;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>域名镜向</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">server</span><br><span class="line">&#123;</span><br><span class="line">listen       80;</span><br><span class="line">server_name  mirror.c1gstudio.com;</span><br><span class="line">index index.html index.htm index.php;</span><br><span class="line">root  /opt/lampp/htdocs/www;</span><br><span class="line">rewrite ^/(.*) http://www.jbxue.com/$1 last;</span><br><span class="line">access_log  off;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>某个子目录作镜向</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">location ^~ /zhaopinhui &#123;</span><br><span class="line">rewrite ^.+ http://zph.jbxue.com/ last;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">discuz ucenter home (uchome) rewrite</span><br><span class="line">rewrite ^/(space|network)-(.+)\.html$ /$1.php?rewrite=$2 last;</span><br><span class="line">rewrite ^/(space|network)\.html$ /$1.php last;</span><br><span class="line">rewrite ^/([0-9]+)$ /space.php?uid=$1 last;</span><br><span class="line">discuz 7 rewrite</span><br><span class="line">rewrite ^(.*)/archiver/((fid|tid)-[\w\-]+\.html)$ $1/archiver/index.php?$2 last;</span><br><span class="line">rewrite ^(.*)/forum-([0-9]+)-([0-9]+)\.html$ $1/forumdisplay.php?fid=$2&amp;page=$3 last;</span><br><span class="line">rewrite ^(.*)/thread-([0-9]+)-([0-9]+)-([0-9]+)\.html$ $1/viewthread.php?tid=$2&amp;extra=page\%3D$4&amp;page=$3 last;</span><br><span class="line">rewrite ^(.*)/profile-(username|uid)-(.+)\.html$ $1/viewpro.php?$2=$3 last;</span><br><span class="line">rewrite ^(.*)/space-(username|uid)-(.+)\.html$ $1/space.php?$2=$3 last;</span><br><span class="line">rewrite ^(.*)/tag-(.+)\.html$ $1/tag.php?name=$2 last;</span><br></pre></td></tr></table></figure><p>给discuz某版块单独配置域名</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">server_name  bbs.c1gstudio.com news.c1gstudio.com;</span><br><span class="line">location = / &#123;</span><br><span class="line">if ($http_host ~ news\.jbxue.com$) &#123;</span><br><span class="line">rewrite ^.+ http://news.jbxue.com/forum-831-1.html last;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>discuz ucenter 头像 rewrite 优化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">location ^~ /ucenter &#123;</span><br><span class="line">location ~ .*\.php?$</span><br><span class="line">&#123;</span><br><span class="line">#fastcgi_pass  unix:/tmp/php-cgi.sock;</span><br><span class="line">fastcgi_pass  127.0.0.1:9000;</span><br><span class="line">fastcgi_index index.php;</span><br><span class="line">include fcgi.conf;</span><br><span class="line">&#125;</span><br><span class="line">location /ucenter/data/avatar &#123;</span><br><span class="line">log_not_found off;</span><br><span class="line">access_log   off;</span><br><span class="line">location ~ /(.*)_big\.jpg$ &#123;</span><br><span class="line">error_page 404 /ucenter/images/noavatar_big.gif;</span><br><span class="line">&#125;</span><br><span class="line">location ~ /(.*)_middle\.jpg$ &#123;</span><br><span class="line">error_page 404 /ucenter/images/noavatar_middle.gif;</span><br><span class="line">&#125;</span><br><span class="line">location ~ /(.*)_small\.jpg$ &#123;</span><br><span class="line">error_page 404 /ucenter/images/noavatar_small.gif;</span><br><span class="line">&#125;</span><br><span class="line">expires 300;</span><br><span class="line">break;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">jspace rewrite</span><br><span class="line">location ~ .*\.php?$</span><br><span class="line">&#123;</span><br><span class="line">#fastcgi_pass  unix:/tmp/php-cgi.sock;</span><br><span class="line">fastcgi_pass  127.0.0.1:9000;</span><br><span class="line">fastcgi_index index.php;</span><br><span class="line">include fcgi.conf;</span><br><span class="line">&#125;</span><br><span class="line">location ~* ^/index.php/</span><br><span class="line">&#123;</span><br><span class="line">rewrite ^/index.php/(.*) /index.php?$1 break;</span><br><span class="line">fastcgi_pass  127.0.0.1:9000;</span><br><span class="line">fastcgi_index index.php;</span><br><span class="line">include fcgi.conf;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>nginx配置nginx.conf详解</title>
      <link href="/nginx-conf-explain/"/>
      <url>/nginx-conf-explain/</url>
      <content type="html"><![CDATA[<p>nginx.conf 配置文件详解、配置案例详解</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br></pre></td><td class="code"><pre><span class="line">#定义Nginx运行的用户和用户组</span><br><span class="line">user www www;</span><br><span class="line"></span><br><span class="line">#nginx进程数，建议设置为等于CPU总核心数。</span><br><span class="line">worker_processes 8;</span><br><span class="line"></span><br><span class="line">#全局错误日志定义类型，[ debug | info | notice | warn | error | crit ]</span><br><span class="line">error_log /var/log/nginx/error.log info;</span><br><span class="line"></span><br><span class="line">#进程文件</span><br><span class="line">pid /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line">#一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。</span><br><span class="line">worker_rlimit_nofile 65535;</span><br><span class="line"></span><br><span class="line">#工作模式与连接数上限</span><br><span class="line">events</span><br><span class="line">&#123;</span><br><span class="line">#参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。</span><br><span class="line">use epoll;</span><br><span class="line">#单个进程最大连接数（最大连接数=连接数*进程数）</span><br><span class="line">worker_connections 65535;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#设定http服务器</span><br><span class="line">http</span><br><span class="line">&#123;</span><br><span class="line">include mime.types; #文件扩展名与文件类型映射表</span><br><span class="line">default_type application/octet-stream; #默认文件类型</span><br><span class="line">#charset utf-8; #默认编码</span><br><span class="line">server_names_hash_bucket_size 128; #服务器名字的hash表大小</span><br><span class="line">client_header_buffer_size 32k; #上传文件大小限制</span><br><span class="line">large_client_header_buffers 4 64k; #设定请求缓</span><br><span class="line">client_max_body_size 8m; #设定请求缓</span><br><span class="line">sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。</span><br><span class="line">autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。</span><br><span class="line">tcp_nopush on; #防止网络阻塞</span><br><span class="line">tcp_nodelay on; #防止网络阻塞</span><br><span class="line">keepalive_timeout 120; #长连接超时时间，单位是秒</span><br><span class="line"></span><br><span class="line">#FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。</span><br><span class="line">fastcgi_connect_timeout 300;</span><br><span class="line">fastcgi_send_timeout 300;</span><br><span class="line">fastcgi_read_timeout 300;</span><br><span class="line">fastcgi_buffer_size 64k;</span><br><span class="line">fastcgi_buffers 4 64k;</span><br><span class="line">fastcgi_busy_buffers_size 128k;</span><br><span class="line">fastcgi_temp_file_write_size 128k;</span><br><span class="line"></span><br><span class="line">#gzip模块设置</span><br><span class="line">gzip on; #开启gzip压缩输出</span><br><span class="line">gzip_min_length 1k; #最小压缩文件大小</span><br><span class="line">gzip_buffers 4 16k; #压缩缓冲区</span><br><span class="line">gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0）</span><br><span class="line">gzip_comp_level 2; #压缩等级</span><br><span class="line">gzip_types text/plain application/x-javascript text/css application/xml;</span><br><span class="line">#压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。</span><br><span class="line">gzip_vary on;</span><br><span class="line">#limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用</span><br><span class="line"></span><br><span class="line">          #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span><br><span class="line">          proxy_set_header Host $host;</span><br><span class="line">          proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">          client_max_body_size 10m;    #允许客户端请求的最大单文件字节数</span><br><span class="line">          client_body_buffer_size 128k;  #缓冲区代理缓冲用户端请求的最大字节数，</span><br><span class="line">          proxy_connect_timeout 90;  #nginx跟后端服务器连接超时时间(代理连接超时)</span><br><span class="line">          proxy_send_timeout 90;        #后端服务器数据回传时间(代理发送超时)</span><br><span class="line">          proxy_read_timeout 90;         #连接成功后，后端服务器响应时间(代理接收超时)</span><br><span class="line">          proxy_buffer_size 4k;             #设置代理服务器（nginx）保存用户头信息的缓冲区大小</span><br><span class="line">          proxy_buffers 4 32k;               #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置</span><br><span class="line">          proxy_busy_buffers_size 64k;    #高负荷下缓冲大小（proxy_buffers*2）</span><br><span class="line">          proxy_temp_file_write_size 64k;  #设定缓存文件夹大小，大于这个值，将从upstream服务器传</span><br><span class="line"></span><br><span class="line">upstream blog.ha97.com &#123;</span><br><span class="line">#upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。</span><br><span class="line">server 192.168.80.121:80 weight=3;</span><br><span class="line">server 192.168.80.122:80 weight=2;</span><br><span class="line">server 192.168.80.123:80 weight=3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#虚拟主机的配置</span><br><span class="line">server</span><br><span class="line">&#123;</span><br><span class="line">#监听端口</span><br><span class="line">listen 80;</span><br><span class="line">#域名可以有多个，用空格隔开</span><br><span class="line">server_name www.ha97.com ha97.com;</span><br><span class="line">index index.html index.htm index.php;</span><br><span class="line">root /data/www/ha97;</span><br><span class="line">location ~ .*\.(php|php5)?$</span><br><span class="line">&#123;</span><br><span class="line">fastcgi_pass 127.0.0.1:9000;</span><br><span class="line">fastcgi_index index.php;</span><br><span class="line">include fastcgi.conf;</span><br><span class="line">&#125;</span><br><span class="line">#图片缓存时间设置</span><br><span class="line">location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$</span><br><span class="line">&#123;</span><br><span class="line">expires 10d;</span><br><span class="line">&#125;</span><br><span class="line">#JS和CSS缓存时间设置</span><br><span class="line">location ~ .*\.(js|css)?$</span><br><span class="line">&#123;</span><br><span class="line">expires 1h;</span><br><span class="line">&#125;</span><br><span class="line">#日志格式设定</span><br><span class="line">log_format access &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">&apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">&apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;;</span><br><span class="line">#定义本虚拟主机的访问日志</span><br><span class="line">access_log /var/log/nginx/ha97access.log access;</span><br><span class="line"></span><br><span class="line">#对 &quot;/&quot; 启用反向代理</span><br><span class="line">location / &#123;</span><br><span class="line">proxy_pass http://127.0.0.1:88;</span><br><span class="line">proxy_redirect off;</span><br><span class="line">proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">#后端的Web服务器可以通过X-Forwarded-For获取用户真实IP</span><br><span class="line">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">#以下是一些反向代理的配置，可选。</span><br><span class="line">proxy_set_header Host $host;</span><br><span class="line">client_max_body_size 10m; #允许客户端请求的最大单文件字节数</span><br><span class="line">client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数，</span><br><span class="line">proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时)</span><br><span class="line">proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时)</span><br><span class="line">proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时)</span><br><span class="line">proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小</span><br><span class="line">proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置</span><br><span class="line">proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2）</span><br><span class="line">proxy_temp_file_write_size 64k;</span><br><span class="line">#设定缓存文件夹大小，大于这个值，将从upstream服务器传</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#设定查看Nginx状态的地址</span><br><span class="line">location /NginxStatus &#123;</span><br><span class="line">stub_status on;</span><br><span class="line">access_log on;</span><br><span class="line">auth_basic &quot;NginxStatus&quot;;</span><br><span class="line">auth_basic_user_file conf/htpasswd;</span><br><span class="line">#htpasswd文件的内容可以用apache提供的htpasswd工具来产生。</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#本地动静分离反向代理配置</span><br><span class="line">#所有jsp的页面均交由tomcat或resin处理</span><br><span class="line">location ~ .(jsp|jspx|do)?$ &#123;</span><br><span class="line">proxy_set_header Host $host;</span><br><span class="line">proxy_set_header X-Real-IP $remote_addr;</span><br><span class="line">proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;</span><br><span class="line">proxy_pass http://127.0.0.1:8080;</span><br><span class="line">&#125;</span><br><span class="line">#所有静态文件由nginx直接读取不经过tomcat或resin</span><br><span class="line">location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$</span><br><span class="line">&#123; expires 15d; &#125;</span><br><span class="line">location ~ .*.(js|css)?$</span><br><span class="line">&#123; expires 1h; &#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">更详细的模块参数请参考：http://wiki.nginx.org/Main</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kafka对topic leader 进行自动负载均衡</title>
      <link href="/kafka-auto-loadblance-leader/"/>
      <url>/kafka-auto-loadblance-leader/</url>
      <content type="html"><![CDATA[<p>在创建一个topic时，kafka尽量将partition均分在所有的brokers上，并且将replicas也j均分在不同的broker上。</p><p>每个partitiion的所有replicas叫做”assigned replicas”，”assigned replicas”中的第一个replicas叫”preferred replica”，刚创建的topic一般”preferred replica”是leader。leader replica负责所有的读写。</p><p>但随着时间推移，broker可能会停机，会导致leader迁移，导致机群的负载不均衡。我们期望对topic的leader进行重新负载均衡，让partition选择”preferred replica”做为leader。</p><p>查看topic详情</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper 127.0.0.1:2181 --describe  --topic logdata-es</span><br><span class="line"></span><br><span class="line">Topic:logdata-es        PartitionCount:6        ReplicationFactor:2     Configs:</span><br><span class="line">        Topic: logdata-es       Partition: 0    Leader: 2       Replicas: 3,2   Isr: 2,3</span><br><span class="line">        Topic: logdata-es       Partition: 1    Leader: 2       Replicas: 5,2   Isr: 2,5</span><br><span class="line">        Topic: logdata-es       Partition: 2    Leader: 1       Replicas: 4,1   Isr: 1,4</span><br><span class="line">        Topic: logdata-es       Partition: 3    Leader: 2       Replicas: 5,2   Isr: 2,5</span><br><span class="line">        Topic: logdata-es       Partition: 4    Leader: 1       Replicas: 1,3   Isr: 1,3</span><br><span class="line">        Topic: logdata-es       Partition: 5    Leader: 2       Replicas: 2,5   Isr: 2,5</span><br></pre></td></tr></table></figure><p>编辑相应topic的json文件  </p><p>vim logdata-es-autu.json </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line"> &quot;partitions&quot;:</span><br><span class="line">  [</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 0&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 1&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 2&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 3&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 4&#125;,</span><br><span class="line">    &#123;&quot;topic&quot;: &quot;logdata-es&quot;, &quot;partition&quot;: 5&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./kafka-preferred-replica-election.sh --zookeeper 127.0.0.1:2181 --path-to-json-file logdata-es-autu.json </span><br><span class="line"></span><br><span class="line">Successfully started preferred replica election for partitions Set([logdata-es,3], [logdata-es,2], [logdata-es,1], [logdata-es,5], [logdata-es,0], [logdata-es,4])</span><br></pre></td></tr></table></figure><p>之后在查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Topic:logdata-es        PartitionCount:6        ReplicationFactor:2     Configs:</span><br><span class="line">        Topic: logdata-es       Partition: 0    Leader: 3       Replicas: 3,2   Isr: 2,3</span><br><span class="line">        Topic: logdata-es       Partition: 1    Leader: 5       Replicas: 5,2   Isr: 2,5</span><br><span class="line">        Topic: logdata-es       Partition: 2    Leader: 4       Replicas: 4,1   Isr: 1,4</span><br><span class="line">        Topic: logdata-es       Partition: 3    Leader: 5       Replicas: 5,2   Isr: 2,5</span><br><span class="line">        Topic: logdata-es       Partition: 4    Leader: 1       Replicas: 1,3   Isr: 1,3</span><br><span class="line">        Topic: logdata-es       Partition: 5    Leader: 2       Replicas: 2,5   Isr: 2,5</span><br></pre></td></tr></table></figure><p>官方说明</p><p><a href="https://cwiki.apache.org/confluence/display/KAFKA/Replication%20tools" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/Replication%20tools</a></p><p>如果你感觉文章还可以的话，请帮点点下面的广告。谢谢！</p>]]></content>
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive中文乱码解决办法</title>
      <link href="/hive-chinese-garbled/"/>
      <url>/hive-chinese-garbled/</url>
      <content type="html"><![CDATA[<p>环境：  hive 1.2.1</p><p>hive 中文注释为乱码，</p><h4 id="首先我们修改一下hive源数据库里的编码"><a href="#首先我们修改一下hive源数据库里的编码" class="headerlink" title="首先我们修改一下hive源数据库里的编码"></a>首先我们修改一下hive源数据库里的编码</h4><p>连接到你环境的hive源数据中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show create COLUMNS_V2;</span><br></pre></td></tr></table></figure><p>可以看到 </p><p><img src="/images/hive-garbled-1.jpeg" alt="hive-garbled-1"></p><p>我们将COLUMNS_V2表中的COMMENT修改为utf-8</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE  `COLUMNS_V2` CHANGE  `COMMENT`  `COMMENT`  varchar(256) CHARACTER SET utf8 COLLATE utf8_general_ci NULL DEFAULT NULL;</span><br></pre></td></tr></table></figure><p>之后在看就可以了，可以显示中文啦。 </p><p><img src="/images/hive-garbled-2.jpeg" alt="hive-garbled-2"></p>]]></content>
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive计算合并小文件</title>
      <link href="/hive-merge-small-files/"/>
      <url>/hive-merge-small-files/</url>
      <content type="html"><![CDATA[<p>hive增加自动合并小文件配置以及在map阶段将多个小文件合并成一个计算。可以提高资源的利用率。</p><p>比如由于小文件原先需要启动10个map，现在只需要启动2个map。</p><p>hadoop  hive 环境： hadoop2.6+hive1.2.1    lzo压缩</p><p>hive-site.xml 配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.merge.mapfiles&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.merge.mapredfiles&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.merge.smallfiles.avgsize&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;69000000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.merge.size.per.task&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;256000000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;hive.merge.tezfiles&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.input.fileinputformat.split.maxsize&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;256000000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.input.fileinputformat.split.minsize&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.input.fileinputformat.split.minsize.per.node&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;128000000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">     &lt;name&gt;mapreduce.input.fileinputformat.split.minsize.per.rack&lt;/name&gt;</span><br><span class="line">     &lt;value&gt;128000000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>参数说明自己查吧！</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/AdminManual+Configuration</a></p><h3 id="测试的步骤"><a href="#测试的步骤" class="headerlink" title="测试的步骤"></a>测试的步骤</h3><p>启动压缩</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.output=true;</span><br><span class="line"></span><br><span class="line">set mapreduce.output.fileoutputformat.compress=true;</span><br></pre></td></tr></table></figure><p>一.减少map数，（当有大量小文件时，启动合并）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br><span class="line"></span><br><span class="line">set mapreduce.input.fileinputformat.split.maxsize=1073741824;</span><br><span class="line"></span><br><span class="line">set mapreduce.input.fileinputformat.split.minsize=1;</span><br><span class="line"></span><br><span class="line">set mapreduce.input.fileinputformat.split.minsize.per.node=536870912;</span><br><span class="line"></span><br><span class="line">set mapreduce.input.fileinputformat.split.minsize.per.rack=536870912;</span><br></pre></td></tr></table></figure><p>经过测试，这种设置可以在map阶段和并小文件，减少map的数量。</p><p>注意：在测试的时候，如果文件格式为Textfile，并且启用lzo压缩，不能生效。 rcfile以及orc可以生效，Textfile不启用lzo压缩也可以生效。如果是新集群的话，没有历史遗留的问题的话，建议hive都使用orc文件格式，以及启用lzo压缩。</p><p>二.MR作业结束后，判断生成文件的平均大小，如果小于阀值，就再启动一个job来合并文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set hive.merge.mapredfiles=true;</span><br><span class="line"></span><br><span class="line">set hive.merge.mapfiles=true;</span><br><span class="line"></span><br><span class="line">set hive.merge.smallfiles.avgsize=268435456;</span><br></pre></td></tr></table></figure><p>如果你感觉文章还可以的话，请帮点点下面的广告！谢谢！</p>]]></content>
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon-lb输出代理的访问日志</title>
      <link href="/marathon-lb-access-log-debug/"/>
      <url>/marathon-lb-access-log-debug/</url>
      <content type="html"><![CDATA[<h1 id="marathon-lb输出代理的访问日志"><a href="#marathon-lb输出代理的访问日志" class="headerlink" title="marathon-lb输出代理的访问日志"></a>marathon-lb输出代理的访问日志</h1><p>之前运行marathon-lb没有打印lb的代理日志，在容器中是能看到marathon-lb获取marathon信息的日志。如果想查看访问量、代理状态、代理的具体的URL等信息的话，还是没办法的。</p><p>当然marathon-lb是有接口可以看到一些访问量、访问状态统计的信息的。 可以用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://marathon-lb-ip:9090/haproxy?stats</span><br></pre></td></tr></table></figure><p>但是这个接口没有访问的具体信息，有问题很难排查。</p><p>还有什么接口， 你可以查看我之前写的文章</p><p><a href="https://sukbeta.github.io/marathon-lb-configure-nginx/">https://sukbeta.github.io/marathon-lb-configure-nginx/</a></p><p>所以，下面我们来说说这么收集marathon-lb的日志。</p><h4 id="marathon-lb的配置"><a href="#marathon-lb的配置" class="headerlink" title="marathon-lb的配置"></a>marathon-lb的配置</h4><p>进入容器修改配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it container-id /bin/bash</span><br></pre></td></tr></table></figure><p>这里没有vi 什么的，可以用sed修改 config.py</p><p>我的配置是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line">global</span><br><span class="line">  daemon</span><br><span class="line">  log /dev/log local1 debug</span><br><span class="line">  spread-checks 5</span><br><span class="line">  max-spread-checks 15000</span><br><span class="line">  maxconn 500000</span><br><span class="line">  .....</span><br><span class="line">  .....</span><br><span class="line">defaults</span><br><span class="line">  load-server-state-from-file global</span><br><span class="line">  log               global</span><br><span class="line">  mode  http</span><br><span class="line">  option        httplog</span><br><span class="line">  option        dontlognull</span><br><span class="line">  retries                   3</span><br><span class="line">  backlog               10000</span><br><span class="line">  maxconn               500000</span><br><span class="line">  timeout connect          10s</span><br><span class="line">  timeout client          300s</span><br><span class="line">  timeout server          300s</span><br><span class="line">  timeout tunnel        3600s</span><br><span class="line">  timeout http-keep-alive  1s</span><br><span class="line">  timeout http-request    15s</span><br><span class="line">  timeout queue           300s</span><br><span class="line">  timeout tarpit          60s</span><br><span class="line">......</span><br></pre></td></tr></table></figure><p>重新 commit 一下容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker commit -m &quot;logdebug&quot; 9c1c65028b12 marathon-lb-debuglog-v1:v1.11.1</span><br></pre></td></tr></table></figure><h4 id="启动容器配置"><a href="#启动容器配置" class="headerlink" title="启动容器配置"></a>启动容器配置</h4><p>首先，marathon-lb启动容器的时候需要挂在主机的/dev/log设备。lb是把日子好打到系统上的。</p><p>marathon-lb 的json 文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;id&quot;: &quot;/marathon-lb/marathon-lb-mesos-slave02&quot;,</span><br><span class="line">  &quot;cmd&quot;: null,</span><br><span class="line">  &quot;cpus&quot;: 1,</span><br><span class="line">  &quot;mem&quot;: 128,</span><br><span class="line">  &quot;disk&quot;: 0,</span><br><span class="line">  &quot;instances&quot;: 0,</span><br><span class="line">  &quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">      &quot;hostname&quot;,</span><br><span class="line">      &quot;CLUSTER&quot;,</span><br><span class="line">      &quot;mesos-slave02&quot;</span><br><span class="line">    ]</span><br><span class="line">  ],</span><br><span class="line">  &quot;container&quot;: &#123;</span><br><span class="line">    &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">    &quot;volumes&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;containerPath&quot;: &quot;/dev/log&quot;,</span><br><span class="line">        &quot;hostPath&quot;: &quot;/dev/log&quot;,</span><br><span class="line">        &quot;mode&quot;: &quot;RW&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    ],</span><br><span class="line">    &quot;docker&quot;: &#123;</span><br><span class="line">      &quot;image&quot;: &quot;marathon-lb-debuglog-v1:v1.11.1&quot;,</span><br><span class="line">      &quot;network&quot;: &quot;HOST&quot;,</span><br><span class="line">      &quot;portMappings&quot;: [],</span><br><span class="line">      &quot;privileged&quot;: true,</span><br><span class="line">      &quot;parameters&quot;: [],</span><br><span class="line">      &quot;forcePullImage&quot;: false</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;env&quot;: &#123;</span><br><span class="line">    &quot;TZ&quot;: &quot;Asia/Shanghai&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;args&quot;: [</span><br><span class="line">    &quot;sse&quot;,</span><br><span class="line">    &quot;-m&quot;,</span><br><span class="line">    &quot;http://192.168.1.10:8080&quot;,</span><br><span class="line">    &quot;--group&quot;,</span><br><span class="line">    &quot;marathon-group&quot;</span><br><span class="line">  ],</span><br><span class="line">  &quot;upgradeStrategy&quot;: &#123;</span><br><span class="line">    &quot;minimumHealthCapacity&quot;: 0,</span><br><span class="line">    &quot;maximumOverCapacity&quot;: 0</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这是就可以在系统中看到marthon的日志了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">journalctl -f</span><br><span class="line"></span><br><span class="line">Jan 10 16:01:29 mesos-slave02.host-mtime.com haproxy[33023]: 192.168.2.2:54544 [10/Jan/2019:08:01:29.742] marathon-lb/marathon-lb-mesos-slave01_host-mtime_com_192_168_1_101_31942 0/0/0/0/0 200 225 - - ---- 1/1/0/0/0 0/0 &quot;GET / HTTP/1.1&quot;</span><br></pre></td></tr></table></figure><h4 id="系统配置"><a href="#系统配置" class="headerlink" title="系统配置"></a>系统配置</h4><p>系统需要用rsyslog服务来讲haproxy的日志打到指定的文件中</p><p>配置 rsyslog的haproxy配置文件</p><p>vim /etc/rsyslog.d/haproxy.conf </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local1.*  /home/haproxy/haproxy.log</span><br></pre></td></tr></table></figure><ul><li>因为haproxy的log里配置的 local1 接口，所以这里接受 local1的所有日志</li></ul><p>vim /etc/rsyslog.conf </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">......</span><br><span class="line"># Provides UDP syslog reception</span><br><span class="line">$ModLoad imudp</span><br><span class="line">$UDPServerRun 514</span><br><span class="line"></span><br><span class="line"># Provides TCP syslog reception</span><br><span class="line">$ModLoad imtcp</span><br><span class="line">$InputTCPServerRun 514</span><br><span class="line">.....</span><br></pre></td></tr></table></figure><p>vim /etc/sysconfig/rsyslog </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SYSLOGD_OPTIONS=&quot;-r -m 0&quot;</span><br></pre></td></tr></table></figure><p>重启 rsyslog 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart rsyslog</span><br></pre></td></tr></table></figure><p>如果没有日志的话，重启一下marathon-lb的容器就可以啦。</p><p>如果感觉文航还可以的话，请帮忙点点下面的广告！ 谢谢！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
            <tag> marathon-lb </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hadoop Cannot obtain block length for LocatedBlock 解决</title>
      <link href="/hadoop-Cannot-obtain-block-length-for-LocatedBlock/"/>
      <url>/hadoop-Cannot-obtain-block-length-for-LocatedBlock/</url>
      <content type="html"><![CDATA[<p>  这几天发现HDFS上的个别文件出现读取异常，使用 hdfs dfs -get 下载文件的话也会报错 “get: Cannot obtain block length for LocatedBlock” 信息。</p><p>hdfs dfs -get 报错信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@-client00 linshi]$ hdfs dfs -get /data/logs/dt=2018-12-24/mx.1545619356868 .</span><br><span class="line">get: Cannot obtain block length for LocatedBlock&#123;BP-2011896023-10.10.10.100-1494585324698:blk_1133914343_60174136; getBlockSize()=359; corrupt=false; offset=0; locs=[DatanodeInfoWithStorage[10.10.10.104:50010,DS-f83c59e0-b769-433e-92d3-db175fcd6717,DISK], DatanodeInfoWithStorage[10.10.10.102:50010,DS-a6654129-9355-4413-bd0c-98175a2b501c,DISK], DatanodeInfoWithStorage[10.10.10.103:50010,DS-53787b49-64bd-48d8-ac16-f6af4a233808,DISK]]&#125;</span><br></pre></td></tr></table></figure><p>  出现这样问题的文件都是Flume产生的文件， 很可能是Flume在写文件的时候没有关闭写的操作，导致hadoop上的这个文件一直为 openforwrite 状态。或data node节点突然下线的情况也会出现。</p><ul><li>Flume客户端写入hdfs文件时的网络连接被不正常的关闭了</li><li>Flume客户端写入hdfs失败了，而且其replication副本也丢失了</li><li>HDFS文件租约未释放</li></ul><p>(我们在Flume的日志中也看到了相应异常日志)</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><h5 id="查找这样的文件"><a href="#查找这样的文件" class="headerlink" title="查找这样的文件"></a>查找这样的文件</h5><p>首先我们需要找出这样的文件。</p><p>出现这种情况可以用fsck检查一下hdfs的文件系统</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck /</span><br></pre></td></tr></table></figure><p>这时你会发现没什么异常文件出现。  下面我在检查一遍。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs fsck / –openforwrite</span><br><span class="line">或</span><br><span class="line">hdfs fsck /data/logs –openforwrite   # 检查具体目录</span><br></pre></td></tr></table></figure><p>这时就会出现有问题的文件了。</p><p>For Example:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">....................//data/logs/dt=2018-12-25/_mx_ticket_order_detail.1545895383766.tmp 578 bytes, 1 block(s), OPENFORWRITE: </span><br><span class="line">/data/logs/dt=2018-12-25/order_detail.1545895383766.tmp: MISSING 1 blocks of total size 578 B................................................................................</span><br><span class="line">....................................................................................................</span><br><span class="line">............../data/logs/dt=2018-12-26/_mx_ticket_order_detail.1545895204686.tmp 890 bytes, 1 block(s), OPENFORWRITE: </span><br><span class="line">/data/logs/dt=2018-12-26/order_detail.1545895204686.tmp: MISSING 1 blocks of total size 890 B......................................................................................</span><br><span class="line">................................................................................../data/logsdt=2018-12-27/_mx_ticket_order_detail.1545895289332.tmp 2367 bytes, 1 block(s), OPENFORWRITE: </span><br><span class="line">/data/logs/dt=2018-12-27/detail.1545895289332.tmp: MISSING 1 blocks of total size 2367 B..................</span><br><span class="line">...........................................................Status: CORRUPT</span><br></pre></td></tr></table></figure><h5 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h5><p>我用释放租约的方式解决的这个问题</p><p>释放租约命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs debug recoverLease -path -retries</span><br></pre></td></tr></table></figure><p>我的命令是：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs debug recoverLease /data/logs/dt=2018-12-24/mx.1545619356868</span><br></pre></td></tr></table></figure><p>之后再 get 这个文件就可以了， 问题解决！</p><p>如果你感觉文章还可以的话，请帮点点下面的广告。非常感谢！</p>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cgroup限制nodemanager、regionserver的资源</title>
      <link href="/cgroup-limit-nm-reg/"/>
      <url>/cgroup-limit-nm-reg/</url>
      <content type="html"><![CDATA[<h1 id="cgroup限制nodemanager、regionserver的资源"><a href="#cgroup限制nodemanager、regionserver的资源" class="headerlink" title="cgroup限制nodemanager、regionserver的资源"></a>cgroup限制nodemanager、regionserver的资源</h1><p>用系统自带的cgroup服务来限制nodemanager、regionserver的cpu使用率。  </p><h5 id="安装cgroup服务，"><a href="#安装cgroup服务，" class="headerlink" title="安装cgroup服务，"></a>安装cgroup服务，</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">##  centos 7.1 系统安装 cgroup</span><br><span class="line">chattr -i /etc/passwd /etc/shadow /etc/group /etc/gshadow</span><br><span class="line">yum install -y libcgroup libcgroup-tools</span><br><span class="line"></span><br><span class="line">## centos 6 系统安装 cgroup</span><br><span class="line">chattr -i /etc/passwd /etc/shadow /etc/group /etc/gshadow</span><br><span class="line">yum install -y libcgroup</span><br></pre></td></tr></table></figure><h5 id="编辑cgroup配置文件"><a href="#编辑cgroup配置文件" class="headerlink" title="编辑cgroup配置文件"></a>编辑cgroup配置文件</h5><p>cpu 32盒 最大使用 90% ，<br>yarn 32 <em> 0.9 </em> 100000 = 2880000 ，<br>hbase 600000  6盒cpu 600 * 1000</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/cgconfig.conf</span><br><span class="line">group yarn &#123;</span><br><span class="line">   perm &#123;</span><br><span class="line">    task &#123;</span><br><span class="line">        uid = hadoop;</span><br><span class="line">        gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">    admin &#123;</span><br><span class="line">       uid = hadoop;</span><br><span class="line">       gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   cpu &#123;</span><br><span class="line">          cpu.cfs_period_us= 100000;</span><br><span class="line">          cpu.cfs_quota_us= 2880000;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br><span class="line">group yarn/hbase &#123;</span><br><span class="line">   perm &#123;</span><br><span class="line">    task &#123;</span><br><span class="line">        uid = hadoop;</span><br><span class="line">        gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">    admin &#123;</span><br><span class="line">       uid = hadoop;</span><br><span class="line">       gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   cpu &#123;</span><br><span class="line">          cpu.cfs_period_us= 100000;</span><br><span class="line">          cpu.cfs_quota_us=  600000;</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="启动cgroup服务"><a href="#启动cgroup服务" class="headerlink" title="启动cgroup服务"></a>启动cgroup服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart cgconfig.service</span><br><span class="line">systemctl enable cgconfig.service</span><br></pre></td></tr></table></figure><h5 id="重新启动-nodemanager-和-regionserver-服务"><a href="#重新启动-nodemanager-和-regionserver-服务" class="headerlink" title="重新启动 nodemanager 和 regionserver 服务"></a>重新启动 nodemanager 和 regionserver 服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line"></span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br><span class="line">cgexec -g cpu:yarn yarn-daemon.sh start nodemanager</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">hbase-daemon.sh stop regionserver</span><br><span class="line">cgexec -g cpu:yarn/hbase hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure><h5 id="检查-服务"><a href="#检查-服务" class="headerlink" title="检查 服务"></a>检查 服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /sys/fs/cgroup/cpu/yarn</span><br><span class="line">cat tasks | grep &quot;nodemanager PID&quot;</span><br><span class="line"></span><br><span class="line">cd /sys/fs/cgroup/cpu/yarn/hbase</span><br><span class="line">cat tasks | grep &quot;regsionserver PID&quot;</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>hive metastore ha 配置</title>
      <link href="/hive-meta-ha/"/>
      <url>/hive-meta-ha/</url>
      <content type="html"><![CDATA[<p>hive metastore 配置多台，可以避免单节点故障导致整个集群的hive client不可用。同时hive client配置多个merastore地址，会自动选择可用节点。</p><h3 id="metastore单独配置"><a href="#metastore单独配置" class="headerlink" title="metastore单独配置"></a>metastore单独配置</h3><p>metastore 的配置单独拿出来，这样不容易让别人看到连接数据库的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@cd-client00 conf]# echo $HIVE_HOME</span><br><span class="line">/home/hadoop/apache-hadoop/hive</span><br><span class="line"></span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hive/hive-metestore/conf</span><br><span class="line">chmod 700 /home/hadoop/apache-hadoop/hive/hive-metestore/conf</span><br></pre></td></tr></table></figure><p>这个目录的权限你可以设置为 700 ，只有自己的帐号可以看到。 其他的hive client也不需要这个目录的配置。</p><p>vim /home/hadoop/apache-hadoop/hive/hive-metestore/conf/hive-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">          &lt;property&gt;</span><br><span class="line">            &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;jdbc:mysql://mysql-master.inc-shining.com:8806/hive_meta?zeroDateTimeBehavior=convertToNull&amp;amp;characterEncoding=UTF-8&lt;/value&gt;</span><br><span class="line">          &lt;/property&gt;</span><br><span class="line">          </span><br><span class="line">          &lt;property&gt;</span><br><span class="line">            &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hive_user&lt;/value&gt;</span><br><span class="line">            &lt;description&gt;Username to use against metastore database&lt;/description&gt;</span><br><span class="line">          &lt;/property&gt;</span><br><span class="line">          </span><br><span class="line">          &lt;property&gt;</span><br><span class="line">            &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;hive_password&lt;/value&gt;</span><br><span class="line">            &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">          &lt;/property&gt;</span><br><span class="line">        </span><br><span class="line">          &lt;property&gt;</span><br><span class="line">            &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">            &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">            &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;</span><br><span class="line">          &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h3 id="启动-hive-metastore"><a href="#启动-hive-metastore" class="headerlink" title="启动 hive metastore"></a>启动 hive metastore</h3><p>正常启动 metastore 的话会到 $HIVE_HOME/conf下找配置文件，我们这里已经将这个配置改目录了，所以我写了一个脚本启动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vim hive-meta-start.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">export HIVE_CONF_DIR=/home/hadoop/apache-hadoop/hive/hive-metastore/conf</span><br><span class="line">nohup  hive --service metastore &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.log 2&gt;&amp;1 &amp; </span><br><span class="line">echo $! &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.pid</span><br><span class="line">unset HIVE_CONF_DIR</span><br></pre></td></tr></table></figure><p>多台 hive metastoe 机器都是这么配置启动，即可。</p><h3 id="hive-client-配置"><a href="#hive-client-配置" class="headerlink" title="hive client 配置"></a>hive client 配置</h3><p>hive client的配置中就不需要存在连接mysql的信息配置。</p><p>vim $HIVE_HOME/conf/hive-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;thrift://meta-1.sukbeta.com:9083,thrift://meta-2.sukbeta.com:9083&lt;/value&gt;</span><br><span class="line">      &lt;description&gt;Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.&lt;/description&gt;</span><br><span class="line">   &lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这样就配置完成了，client端就可以直接用hive了，</p><ul><li>这样启动hive的时候，本地client端就无需实例化hive的metastore，启动速度会加快。</li></ul><p>如果你感觉文章还可以的话，请帮点点下面的广告！ 谢谢！</p>]]></content>
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kafka设置某个topic的数据过期时间</title>
      <link href="/kafka-set-topic-retention/"/>
      <url>/kafka-set-topic-retention/</url>
      <content type="html"><![CDATA[<h1 id="kafka-单独设置某个topic的数据过期时间"><a href="#kafka-单独设置某个topic的数据过期时间" class="headerlink" title="kafka 单独设置某个topic的数据过期时间"></a>kafka 单独设置某个topic的数据过期时间</h1><p>kafka 默认存放7天的临时数据，如果遇到磁盘空间小，存放数据量大，可以设置缩短这个时间。</p><h3 id="全局设置"><a href="#全局设置" class="headerlink" title="全局设置"></a>全局设置</h3><p>修改  server.properties</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">log.retention.hours=72</span><br><span class="line">log.cleanup.policy=delete</span><br></pre></td></tr></table></figure><h3 id="单独对某一个topic设置过期时间"><a href="#单独对某一个topic设置过期时间" class="headerlink" title="单独对某一个topic设置过期时间"></a>单独对某一个topic设置过期时间</h3><p>如果你这样设置完，可以磁盘空间还是不够，或只有某一个topic数据量过大。</p><p>想单独对这个topic的过期时间设置短点，</p><h5 id="可以这样设置："><a href="#可以这样设置：" class="headerlink" title="可以这样设置："></a>可以这样设置：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-configs.sh --zookeeper localhost:2181 --alter --entity-name wordcounttopic --entity-type topics --add-config retention.ms=86400000</span><br></pre></td></tr></table></figure><p>retention.ms=86400000  为一天，单位是毫秒。</p><h5 id="查看设置："><a href="#查看设置：" class="headerlink" title="查看设置："></a>查看设置：</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@kafka00 kafka]$   ./kafka-configs.sh --zookeeper localhost:2181 --describe --entity-name wordcounttopic --entity-type topics</span><br><span class="line"></span><br><span class="line">Configs for topics:wordcounttopic are retention.ms=86400000</span><br></pre></td></tr></table></figure><h5 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h5><p>如果没有立刻删除的话你可以设置下面参数。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper localhost:2181 --alter --topic wordcounttopic --config cleanup.policy=delete</span><br></pre></td></tr></table></figure><p>如果你感觉文章还可以的话，请帮点点下面的广告！ 谢谢！</p>]]></content>
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>一个简单好用的ssh远程执行命令的脚本</title>
      <link href="/ssh-remote-exec-command/"/>
      <url>/ssh-remote-exec-command/</url>
      <content type="html"><![CDATA[<p>运维经常需要到其他机器上执行命令，copy等操作。其实也有很多工具可以实现的，如jenkins、saltstack、ansible等等。这样还需要安装工具什么的。下面的小脚本不需要任何工具。先实现copy和执行命令吧。</p><p>好了，直接看脚本吧。</p><h4 id="脚本内容"><a href="#脚本内容" class="headerlink" title="脚本内容"></a>脚本内容</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">COMMAND=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line">SLAVES=$1</span><br><span class="line">shift</span><br><span class="line"></span><br><span class="line">#distribute files</span><br><span class="line">if [ $COMMAND == &quot;distribute&quot; ] || [ $COMMAND == &quot;copy&quot; ];then</span><br><span class="line">  SRC=$1</span><br><span class="line">  shift </span><br><span class="line"></span><br><span class="line">  DEST=$1</span><br><span class="line">  shift</span><br><span class="line"></span><br><span class="line">  if [ -f $SLAVES ];then</span><br><span class="line">  cat $SLAVES | while read slave</span><br><span class="line">  do</span><br><span class="line">    #已#开头的注释可略过</span><br><span class="line">    echo &quot;$slave&quot; | grep -q &quot;^#&quot;</span><br><span class="line">    if [ $? -eq 0 ] ; then</span><br><span class="line">        continue;</span><br><span class="line">    fi</span><br><span class="line">  </span><br><span class="line">    echo &quot;===================$slave=================&quot;</span><br><span class="line">    scp -r  -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no $SRC $slave:$DEST</span><br><span class="line">  done</span><br><span class="line">  exit 0</span><br><span class="line">else</span><br><span class="line">   echo &quot;===================$SLAVES=================&quot;</span><br><span class="line">    scp -r  -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no $SRC $SLAVES:$DEST</span><br><span class="line">  fi</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">#common</span><br><span class="line">if [ $COMMAND == &quot;common&quot; ] || [ $COMMAND == &quot;command&quot; ];then</span><br><span class="line">  if [ -f $SLAVES ];then</span><br><span class="line">  cat $SLAVES | while read slave</span><br><span class="line">  do</span><br><span class="line">    #已#开头的注释可略过</span><br><span class="line">    echo &quot;$slave&quot; | grep -q &quot;^#&quot;</span><br><span class="line">    if [ $? -eq 0 ] ; then</span><br><span class="line">        continue;</span><br><span class="line">    fi</span><br><span class="line"></span><br><span class="line">    ssh  -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no $slave &lt;&lt; EOF</span><br><span class="line">      echo &quot;==================$slave======================&quot;</span><br><span class="line">      eval $@</span><br><span class="line">      exit</span><br><span class="line">EOF</span><br><span class="line">  done</span><br><span class="line">  exit 0</span><br><span class="line">else</span><br><span class="line">    ssh  -oUserKnownHostsFile=/dev/null -oStrictHostKeyChecking=no $SLAVES &lt;&lt; EOF</span><br><span class="line">      echo &quot;==================$SLAVES======================&quot;</span><br><span class="line">      eval $@</span><br><span class="line">      exit</span><br><span class="line">EOF</span><br><span class="line">  fi</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><h4 id="脚本用法"><a href="#脚本用法" class="headerlink" title="脚本用法"></a>脚本用法</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># 复制文件到所有机器的/home/hadoop/apache-hadoop/目录下，all_node是列表</span><br><span class="line">./tools.sh copy all_node /home/hadoop/apache-hadoop/hadoop-2.8.0.tar.gz /home/hadoop/apache-hadoop/</span><br><span class="line"></span><br><span class="line"># 复制文件到shining-1.hostname机器的指定目录下</span><br><span class="line">./tools.sh copy shining-1.hostname /home/hadoop/mapred-site.xml /home/hadoop/apache-hadoop/hadoop/etc/hadoop/</span><br><span class="line"></span><br><span class="line"># 复制多个文件到所有机器的指定目录下</span><br><span class="line">./tools.sh copy all_node &quot;core-site.xml hdfs-site.xml yarn-site.xml&quot; /home/hadoop/apache-hadoop/hadoop/etc/hadoop/</span><br><span class="line"></span><br><span class="line"># 到所有机器下执行命令</span><br><span class="line">./tools.sh command all_node &apos;cd /home/hadoop/apache-hadoop/;ln -s hadoop-2.8.0 hadoop;&apos;</span><br><span class="line"></span><br><span class="line"># 到执行集群下执行命令</span><br><span class="line">./tools.sh command shining-1.hostname &quot;hostname&quot;</span><br><span class="line"></span><br><span class="line"># 执行追加命令</span><br><span class="line">./tools.sh command all_node &apos;echo &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt;&gt; /etc/profile ; echo &quot;export PATH=\$JAVA_HOME/bin:\$JAVA_HOME/jre/bin:\$PATH&quot; &gt;&gt; /etc/profile&apos;</span><br></pre></td></tr></table></figure><h5 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h5><ul><li>第一个参数是告诉脚本，是执行 copy 还是 command 命令</li><li>第二个参数是远程执行的主机列表， all_node 是一个文件，里面放主机列表，如果执行单台机器的，可以直接跟 shining-1.hostname 主机名。</li><li>第三个参数就是需要执行的具体命令。</li></ul><p>如果你感觉文章还可以的话，请帮点点下面的广告哦！ 谢谢</p>]]></content>
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>zookeeper清理日志</title>
      <link href="/zk-delete-log/"/>
      <url>/zk-delete-log/</url>
      <content type="html"><![CDATA[<h1 id="zookeeper-清理日志"><a href="#zookeeper-清理日志" class="headerlink" title="zookeeper 清理日志"></a>zookeeper 清理日志</h1><p>在使用zookeeper过程中，会有dataDir和dataLogDir两个目录，分别用于snapshot和事务日志的输出（默认情况下只有dataDir目录，snapshot和事务日志都保存在这个目录中，正常运行过程中，ZK会不断地把快照数据和事务日志输出到这两个目录，并且如果没有人为操作的话，ZK自己是不会清理这些文件的，需要管理员来清理。</p><h3 id="配置方法"><a href="#配置方法" class="headerlink" title="配置方法"></a>配置方法</h3><p>从3.4.0开始，zookeeper提供了自动清理snapshot和事务日志的功能，通过配置 autopurge.snapRetainCount 和 autopurge.purgeInterval 这两个参数能够实现定时清理了。这两个参数都是在zoo.cfg中配置的：</p><p>For Example： 一个zoo.cfg配置的例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">dataDir=/home/hadoop/apache-hadoop/zookeeper/var/data</span><br><span class="line">clientPort=2181</span><br><span class="line">dataLogDir=/home/hadoop/apache-hadoop/zookeeper/var/datalog</span><br><span class="line">maxClientCnxns=300</span><br><span class="line">server.1=namenode001-host.mjq-sukbeta.com:2888:3888</span><br><span class="line">server.2=namenode002-host.mjq-sukbeta.com:2888:3888</span><br><span class="line">server.3=datanode001-host.mjq-sukbeta.com:2888:3888</span><br><span class="line">server.4=datanode002-host.mjq-sukbeta.com:2888:3888</span><br><span class="line">server.5=datanode003-host.mjq-sukbeta.com:2888:3888</span><br><span class="line">autopurge.snapRetainCount=20</span><br><span class="line">autopurge.purgeInterval=48</span><br></pre></td></tr></table></figure><p>autopurge.purgeInterval  这个参数指定了清理频率，单位是小时，需要填写一个1或更大的整数，默认是0，表示不开启自己清理功能。  </p><p>autopurge.snapRetainCount 这个参数和上面的参数搭配使用，这个参数指定了需要保留的文件数目。默认是保留3个。</p><h3 id="脚本方法"><a href="#脚本方法" class="headerlink" title="脚本方法"></a>脚本方法</h3><p>写了一个脚本， 可以每天定时执行清理。</p><p>脚本内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line">           </span><br><span class="line">#snapshot file dir</span><br><span class="line">dataDir=/home/hadoop/apache-hadoop/zookeeper/var/data/version-2</span><br><span class="line">#tran log dir</span><br><span class="line">dataLogDir=/home/hadoop/apache-hadoop/zookeeper/var/datalog/version-2</span><br><span class="line">#zk log dir</span><br><span class="line">#Leave 30 files</span><br><span class="line">count=30</span><br><span class="line">count=$[$count+1]</span><br><span class="line">ls -t $dataLogDir/log.* | tail -n +$count | xargs rm -f</span><br><span class="line">ls -t $dataDir/snapshot.* | tail -n +$count | xargs rm -f</span><br></pre></td></tr></table></figure><ul><li>需要根据自己的情况修改脚本中的dataDir、dataLogDir路径。</li></ul><p>如果感觉文章还可以的话，帮点点下面的广告哦！非常感谢！</p>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Hue编译安装及Hadoop相关组建的配置</title>
      <link href="/hue-installed/"/>
      <url>/hue-installed/</url>
      <content type="html"><![CDATA[<h1 id="Hue安装部署"><a href="#Hue安装部署" class="headerlink" title="Hue安装部署"></a>Hue安装部署</h1><p>Hue是一个开源的Apache Hadoop UI系统，是基于Python Web框架Django实现的。Hue可以使开发者在浏览器端的Web控制台上与Hadoop集群进行交互来分析处理数据，例如操作HDFS上的数据，运行MapReduce Job等等。</p><p>本文介绍CentOS6.5安装hue3.11.0，及Hadoop相关组建的配置。</p><h4 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ant asciidoc cyrus-sasl-devel cyrus-sasl-gssapi gcc gcc-c++ krb5-devel libtidy libxml2-devel  libxslt-devel make mysql mysql-devel openldap-devel Python-devel sqlite-devel openssl-devel gmp-devel libffi-devel unzip</span><br><span class="line">yum install -y cyrus-sasl-plain </span><br><span class="line">yum install -y libssl-devel libffi-devel</span><br><span class="line">yum install -y python-simplejson python-setuptools rsync saslwrapper-devel pycrypto libyaml-devel libsasl2-dev libsasl2-modules-gssapi-mit libkrb5-dev libssl-devel</span><br></pre></td></tr></table></figure><h4 id="编译-hue"><a href="#编译-hue" class="headerlink" title="编译 hue"></a>编译 hue</h4><p>下载hue</p><p>下载hue-3.11.0.tgz<br>解压tar -zxvf hue-3.11.0.tgz</p><p>github 下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/cloudera/hue.git branch-3.11.0</span><br><span class="line"></span><br><span class="line">mv branch-3.11.0 hue-3.11.0</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">编译方式一：</span><br><span class="line">cd hue-3.11.0</span><br><span class="line">make apps</span><br><span class="line">编译完成后会在当前目录下生产build等目录，hue-3.11.0即可作为安装目录</span><br><span class="line"></span><br><span class="line">编译方式二：</span><br><span class="line">make install PREFIX=/usr/local</span><br><span class="line">会在/usr/local下生产hue目录，安装的时候就用此hue</span><br></pre></td></tr></table></figure><h4 id="配置hadoop的-HttpFS服务"><a href="#配置hadoop的-HttpFS服务" class="headerlink" title="配置hadoop的  HttpFS服务"></a>配置hadoop的  HttpFS服务</h4><p>如果hdfs启用了HA，则只能使用HttpFS服务，否则也可以使用Webhdfs</p><p>HttpFS服务配置：</p><p>core-site.xml文件添加：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.httpfs.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.httpfs.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>httpfs-site.xml中加入以下内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;httpfs.proxyuser.$username.hosts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;httpfs.proxyuser.$groupname.groups&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>这里都是hadoop  因为所有服务都是hadoop用户安装部署。</p><p>core-site.xml添加以下内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hadoop.proxyuser.hue.hosts&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;name&gt;hadoop.proxyuser.hue.groups&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;*&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>hdfs-site.xml添加这些语句</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h4 id="启动hdfs的HttpFS服务："><a href="#启动hdfs的HttpFS服务：" class="headerlink" title="启动hdfs的HttpFS服务："></a>启动hdfs的HttpFS服务：</h4><p>/home/hadoop/apache-hadoop/hadoop/sbin/httpfs.sh  start</p><p>测试：访问<a href="http://namenode_address:14000/webhdfs/v1" target="_blank" rel="noopener">http://namenode_address:14000/webhdfs/v1</a></p><h4 id="修改hue配置"><a href="#修改hue配置" class="headerlink" title="修改hue配置"></a>修改hue配置</h4><p>修改hue的/home/hadoop/hue-3.11.0/desktop/conf/hue.ini 配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"># Webserver listens on this address and port</span><br><span class="line">  http_host=192.168.110.160</span><br><span class="line">  http_port=8008</span><br><span class="line"></span><br><span class="line">  # Time zone name</span><br><span class="line">  ##time_zone=America/Los_Angeles</span><br><span class="line">  time_zone=Asia/Shanghai</span><br><span class="line">  </span><br><span class="line">  # Webserver runs as this user</span><br><span class="line">  server_user=hadoop</span><br><span class="line">  server_group=hadoop</span><br><span class="line"></span><br><span class="line">  # This should be the Hue admin and proxy user</span><br><span class="line">   default_user=hadoop</span><br><span class="line"></span><br><span class="line">  # This should be the hadoop cluster admin</span><br><span class="line">  default_hdfs_superuser=hadoop</span><br><span class="line">  </span><br><span class="line">  # Default encoding for site data</span><br><span class="line">  default_site_encoding=utf-8</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  # Note for MariaDB use the &apos;mysql&apos; engine.</span><br><span class="line">    engine=mysql</span><br><span class="line">    host=mysql-test.shining.com</span><br><span class="line">    port=3306</span><br><span class="line">    user=hadoop</span><br><span class="line">    password=123456</span><br><span class="line">    # Execute this script to produce the database password. This will be used when &apos;password&apos; is not set.</span><br><span class="line">    password_script=/path/script</span><br><span class="line">    name=db_hue</span><br><span class="line"></span><br><span class="line">[hadoop]</span><br><span class="line"> # Enter the filesystem uri</span><br><span class="line">      fs_defaultfs=hdfs://testhadoop:8020    ##core-site.xml中的fs.defaultFS的值</span><br><span class="line">  # Use WebHdfs/HttpFs as the communication mechanism.</span><br><span class="line">      # Domain should be the NameNode or HttpFs host.</span><br><span class="line">      # Default port is 14000 for HttpFs.</span><br><span class="line">      webhdfs_url=http://192.168.110.159:14000/webhdfs/v1   ##启用HttpFS</span><br><span class="line">  # Directory of the Hadoop configuration</span><br><span class="line">      hadoop_conf_dir=$HADOOP_HOME/etc/hadoop</span><br><span class="line">[[yarn_clusters]]</span><br><span class="line"># Enter the host on which you are running the ResourceManager</span><br><span class="line">      resourcemanager_host=192.168.110.159</span><br><span class="line"></span><br><span class="line">      # The port where the ResourceManager IPC listens on</span><br><span class="line">      resourcemanager_port=8032</span><br><span class="line"># URL of the ResourceManager API</span><br><span class="line">      resourcemanager_api_url=http://192.168.53.100:8088</span><br><span class="line"># URL of the HistoryServer API</span><br><span class="line">       history_server_api_url=http://192.168.53.101:19888</span><br><span class="line">   </span><br><span class="line">   </span><br><span class="line">    # [[[ha]]]</span><br><span class="line">      # Resource Manager logical name (required for HA)</span><br><span class="line">      ## logical_name=my-rm-name</span><br><span class="line"></span><br><span class="line">      # Un-comment to enable</span><br><span class="line">      ## submit_to=True</span><br><span class="line"></span><br><span class="line">      # URL of the ResourceManager API</span><br><span class="line">      ## resourcemanager_api_url=http://localhost:8088</span><br><span class="line">  </span><br><span class="line"> [[mapred_clusters]]</span><br><span class="line"></span><br><span class="line">    [[[default]]]</span><br><span class="line">      # Enter the host on which you are running the Hadoop JobTracker</span><br><span class="line">      jobtracker_host=192.168.110.160</span><br><span class="line">[beeswax]</span><br><span class="line"></span><br><span class="line">  # Host where HiveServer2 is running.</span><br><span class="line">  # If Kerberos security is enabled, use fully-qualified domain name (FQDN).</span><br><span class="line">  hive_server_host=192.168.110.160</span><br><span class="line"></span><br><span class="line">  # Port where HiveServer2 Thrift server runs on.</span><br><span class="line">   hive_server_port=10000</span><br><span class="line"></span><br><span class="line">  # Hive configuration directory, where hive-site.xml is located</span><br><span class="line">  hive_conf_dir=/home/hadoop/apache-hadoop/hive/conf</span><br><span class="line">  </span><br><span class="line">[spark]</span><br><span class="line">  # Host address of the Livy Server.</span><br><span class="line">  ## livy_server_host=localhost</span><br><span class="line"></span><br><span class="line">  # Port of the Livy Server.</span><br><span class="line">  ## livy_server_port=8998</span><br><span class="line"></span><br><span class="line">  # Configure Livy to start in local &apos;process&apos; mode, or &apos;yarn&apos; workers.</span><br><span class="line">  ## livy_server_session_kind=yarn</span><br><span class="line"></span><br><span class="line">  # Host of the Sql Server</span><br><span class="line">  ## sql_server_host=localhost</span><br><span class="line"></span><br><span class="line">  # Port of the Sql Server</span><br><span class="line">  ## sql_server_port=10000</span><br><span class="line">[hbase]</span><br><span class="line">  # Comma-separated list of HBase Thrift servers for clusters in the format of &apos;(name|host:port)&apos;.</span><br><span class="line">  # Use full hostname with security.</span><br><span class="line">  # If using Kerberos we assume GSSAPI SASL, not PLAIN.</span><br><span class="line">  #hbase_clusters=(Cluster|192.168.53.100:9090)</span><br><span class="line"></span><br><span class="line">  # HBase configuration directory, where hbase-site.xml is located.</span><br><span class="line">  ## hbase_conf_dir=/home/hadoop/apache-hadoop/hbase/conf</span><br><span class="line"></span><br><span class="line">  # Hard limit of rows or columns per row fetched before truncating.</span><br><span class="line">  ## truncate_limit = 500</span><br><span class="line"></span><br><span class="line">  # &apos;buffered&apos; is the default of the HBase Thrift Server and supports security.</span><br><span class="line">  # &apos;framed&apos; can be used to chunk up responses,</span><br><span class="line">  # which is useful when used in conjunction with the nonblocking server in Thrift.</span><br><span class="line">  ## thrift_transport=buffered</span><br></pre></td></tr></table></figure><h5 id="建hue数据库"><a href="#建hue数据库" class="headerlink" title="建hue数据库"></a>建hue数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">create database hue_123 DEFAULT CHARSET utf8 COLLATE utf8_general_ci;</span><br><span class="line">grant all PRIVILEGES on hue_123.* to &apos;hadoop&apos;@&apos;192.168.53.101&apos; IDENTIFIED BY &apos;123456&apos; with grant option ;</span><br><span class="line">grant all PRIVILEGES on hue_123.* to &apos;hadoop&apos;@&apos;%&apos; IDENTIFIED BY &apos;123456&apos; with grant option ;</span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h5 id="初始化hue（顺序执行）"><a href="#初始化hue（顺序执行）" class="headerlink" title="初始化hue（顺序执行）"></a>初始化hue（顺序执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apache-hadoop/hue/build/env/bin/hue syncdb </span><br><span class="line">（有提示设置用户名和密码）</span><br><span class="line">apache-hadoop/hue/build/env/bin/hue migrate</span><br></pre></td></tr></table></figure><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><p>start</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">apache-hadoop/hue/build/env/bin/supervisor &gt;/dev/null 2&gt;&amp;1 &amp;</span><br></pre></td></tr></table></figure><p>stop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ps -ef |grep hue</span><br><span class="line">kill -9 pid</span><br></pre></td></tr></table></figure><h5 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://192.168.110.160:8008</span><br></pre></td></tr></table></figure><p>如果您感觉文章还可以的话，请帮点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> hue </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
            <tag> hue </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>源码编译安装 apache hadoop</title>
      <link href="/install-apache-hadoop/"/>
      <url>/install-apache-hadoop/</url>
      <content type="html"><![CDATA[<h1 id="安装-apache-hadoop-2-7-2-集群"><a href="#安装-apache-hadoop-2-7-2-集群" class="headerlink" title="安装 apache hadoop 2.7.2 集群"></a>安装 apache hadoop 2.7.2 集群</h1><p>这是我安装编译hadoop得意个笔记手册，和大家分享一下。</p><p>环境：   </p><p>  系统 Centos 7<br>  java ： 1.8<br>  apache hadoop 版本 ： 2.7.2</p><h5 id="hadoop集群hosts列表"><a href="#hadoop集群hosts列表" class="headerlink" title="hadoop集群hosts列表"></a>hadoop集群hosts列表</h5><table><thead><tr><th>IP</th><th>hostname</th><th>运行服务</th></tr></thead><tbody><tr><td>192.168.77.158</td><td>namenode00.host-shining.com</td><td>namenode、zk、journalnode、standby-resourcemanager，hbase-master、spark-master</td></tr><tr><td>192.168.77.159</td><td>namenode01.host-shining.com</td><td>namenode、zk、journalnode、resourcemanager，hbase-master、jobhistory</td></tr><tr><td>192.168.77.161</td><td>datanode00.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.162</td><td>datanode01.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.163</td><td>datanode02.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.164</td><td>datanode03.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.165</td><td>datanode04.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.166</td><td>datanode05.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.167</td><td>datanode06.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.168</td><td>datanode07.host-shining.com</td><td>datanode、nodemanager</td></tr><tr><td>192.168.77.169</td><td>datanode08.host-shining.com</td><td>datanode、nodemanager</td></tr><tr><td>192.168.77.170</td><td>datanode09.host-shining.com</td><td>datanode、nodemanager</td></tr></tbody></table><blockquote><p>hosts文件并同步到每台机器上。  </p></blockquote><h5 id="hadoop-client-列表"><a href="#hadoop-client-列表" class="headerlink" title="hadoop client 列表"></a>hadoop client 列表</h5><table><thead><tr><th>IP</th><th>hostname</th></tr></thead><tbody><tr><td>192.168.77.160</td><td>client01.host-shining.com</td></tr></tbody></table><h5 id="安装软件包和lzop软件（每台机器都执行）"><a href="#安装软件包和lzop软件（每台机器都执行）" class="headerlink" title="安装软件包和lzop软件（每台机器都执行）"></a>安装软件包和lzop软件（每台机器都执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install  wget gcc gcc-c++ gcc-g77 autoconf automake zlib* fiex* libxml* ncurses-devel libmcrypt* libtool-ltdl-devel* make cmake bind-utils ntp ntpdate lrzsz rsync gzip unzip vim telnet openssl-devel nscd g++ sysstat ncurses-libs bzip2-devel git lsof expect  </span><br><span class="line"></span><br><span class="line">yum -y install *gcc* ncurses-devel openssl-devel cmake autoconfautomake libtool bzip2-devel g++ autoconf automake libtool cmake zlib1g-dev pkg-config  </span><br><span class="line"></span><br><span class="line">yum -y install  lzo-devel zlib-devel  gcc autoconf automakelibtool lzop subversion</span><br></pre></td></tr></table></figure><h5 id="安装jdk-1-8-（需要每台机器执行）"><a href="#安装jdk-1-8-（需要每台机器执行）" class="headerlink" title="安装jdk 1.8 （需要每台机器执行）"></a>安装jdk 1.8 （需要每台机器执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u45-linux-x64.tar.gz &amp;&amp; mkdir /usr/java/ &amp;&amp; mv  jdk1.8.0_45/ /usr/java/</span><br><span class="line">echo -e &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45\nexport PATH=\$JAVA_HOME/bin:\$JAVA_HOME/jre/bin:\$PATH&quot; &gt;&gt; /etc/profile</span><br></pre></td></tr></table></figure><h5 id="创建hadoop用户并添加无密码登入（每台集群都执行）"><a href="#创建hadoop用户并添加无密码登入（每台集群都执行）" class="headerlink" title="创建hadoop用户并添加无密码登入（每台集群都执行）"></a>创建hadoop用户并添加无密码登入（每台集群都执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd hadoop</span><br><span class="line">su - hadoop</span><br><span class="line">echo &quot;hadoop|xiaoaojianghu&quot; | chpasswd</span><br></pre></td></tr></table></figure><p>创建添加key的脚本，每台机器添加完用户之后执行  sh key_add.sh  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">vim key_add.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line"># 创建pub  key文件</span><br><span class="line">expect -c &apos;set timeout -1;</span><br><span class="line">spawn ssh-keygen -t rsa;</span><br><span class="line">expect &quot;Enter file in which to save the key&quot;;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">expect &quot;(empty for no passphrase):&quot;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">expect &quot;Enter same passphrase again:&quot;;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">interact&apos;</span><br><span class="line"># 在hosts文件里找到name和data的主机名</span><br><span class="line">for ip in `cat /etc/hosts | grep -v &quot;^#&quot; |grep -E &quot;name|data&quot;|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">do </span><br><span class="line">    echo $ip </span><br><span class="line">    # add hadoop pub key </span><br><span class="line">    expect -c &apos;set timeout -1;</span><br><span class="line">    spawn ssh-copy-id -i .ssh/id_rsa.pub &apos;$ip&apos;;</span><br><span class="line">    expect &quot;Are you sure you want to continue connecting (yes/no)?&quot;;</span><br><span class="line">    send &quot;yes\n&quot;;</span><br><span class="line">    expect &quot;password:&quot;;</span><br><span class="line">    send &quot;xiaoaojianghu\n&quot;;</span><br><span class="line">    interact&apos;</span><br><span class="line"></span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">for ip in `cat /etc/hosts | grep -E &quot;name|data&quot;|awk &apos;&#123;print $1&#125;&apos;`;do echo $ip ;ssh-copy-id -i ~/.ssh/id_rsa.pub $ip;done</span><br></pre></td></tr></table></figure><h5 id="datanode机器格式化硬盘-所有datanode节点执行"><a href="#datanode机器格式化硬盘-所有datanode节点执行" class="headerlink" title="datanode机器格式化硬盘  (所有datanode节点执行)"></a>datanode机器格式化硬盘  (所有datanode节点执行)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vim fdisk.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line">yum install -y expect parted</span><br><span class="line"></span><br><span class="line">#for letter in b c d e f g h i j k l m   #默认列表</span><br><span class="line">for letter in `fdisk -l | grep 4000 | awk &apos;&#123;print $2&#125;&apos; | cut -c 8 | sort`  # 找到4T的硬盘并格式化。</span><br><span class="line">do</span><br><span class="line">    expect -c &apos;set timeout -1;</span><br><span class="line">    spawn parted /dev/sd&apos;$letter&apos;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;mklabel gpt\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;unit GB\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;mkpart primary 0 -1\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;quit\n&quot;;</span><br><span class="line">    interact&apos;</span><br><span class="line"></span><br><span class="line">    nohup mkfs.xfs /dev/sd$&#123;letter&#125;1 &gt; sd$letter.out 2&gt;&amp;1 &amp;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h5 id="mount-挂在硬盘-所有datanode节点执行"><a href="#mount-挂在硬盘-所有datanode节点执行" class="headerlink" title="mount 挂在硬盘  (所有datanode节点执行)"></a>mount 挂在硬盘  (所有datanode节点执行)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">vim mount.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line"></span><br><span class="line">blkid | sort | grep -v sdm| awk -F &apos;&quot;&apos; &apos;&#123;print $2&#125;&apos; &gt; /tmp/uuid</span><br><span class="line">echo &quot;/data00</span><br><span class="line">/data01</span><br><span class="line">/data02</span><br><span class="line">/data03</span><br><span class="line">/data04</span><br><span class="line">/data05</span><br><span class="line">/data06</span><br><span class="line">/data07</span><br><span class="line">/data08</span><br><span class="line">/data09</span><br><span class="line">/data10</span><br><span class="line">/data11&quot; &gt; /tmp/dir</span><br><span class="line"></span><br><span class="line">for dir in `cat /tmp/dir`</span><br><span class="line">do</span><br><span class="line">    mkdir -p $dir</span><br><span class="line">done</span><br><span class="line"># 也可以用</span><br><span class="line"># mkdir /data&#123;00..11&#125;</span><br><span class="line"></span><br><span class="line">l=$(cat /tmp/uuid | wc -l)</span><br><span class="line">for ((i=1;i&lt;=$l;i++))</span><br><span class="line">do</span><br><span class="line">    u=$(sed -n &quot;$i&quot;p /tmp/uuid)</span><br><span class="line">    d=$(sed -n &quot;$i&quot;p /tmp/dir)</span><br><span class="line">    mount UUID=$u $d</span><br><span class="line">    cp /etc/fstab /etc/fstab_backup</span><br><span class="line">    echo -e &quot;UUID=$u\t$d\t\txfs\tdefaults,noatime,nodiratime\t0 0&quot; &gt;&gt; /etc/fstab   </span><br><span class="line">    # noatime,nodiratim   禁用文件访问时间</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h5 id="下载软件包"><a href="#下载软件包" class="headerlink" title="下载软件包"></a>下载软件包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/home/hadoop/apache-hadoop &amp;&amp; cd /home/hadoop/apache-hadoop</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-bin.tar.gz</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-src.tar.gz</span><br><span class="line">wget http://apache.fayea.com/hbase/stable/hbase-1.1.5-bin.tar.gz</span><br><span class="line">wget http://apache.fayea.com/hbase/stable/hbase-1.1.5-src.tar.gz</span><br><span class="line">wget http://apache.fayea.com/mahout/0.12.2/apache-mahout-distribution-0.12.2.tar.gz</span><br><span class="line">wget http://apache.fayea.com/zookeeper/stable/zookeeper-3.4.8.tar.gz</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/spark/spark-1.6.2/spark-1.6.2.tgz</span><br></pre></td></tr></table></figure><h5 id="安装zookeeper"><a href="#安装zookeeper" class="headerlink" title="安装zookeeper"></a>安装zookeeper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.8.tar.gz &amp;&amp; ln -s zookeeper-3.4.8 zookeeper</span><br><span class="line">cd /home/hadoop/apache-hadoop/zookeeper &amp;&amp;  mkdir -p var/&#123;data,datalog&#125;</span><br><span class="line">cd conf</span><br><span class="line">echo &quot;JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt; java.env</span><br><span class="line">echo &quot;export JAVA_OPTS=\&quot;-Xms1000m -Xmx1000m\&quot;&quot; &gt;&gt; java.env</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">sed -i &apos;/^dataDir=/c dataDir=/home/hadoop/apache-hadoop/zookeeper/var/data&apos; zoo.cfg</span><br><span class="line">echo &quot;dataLogDir=/home/hadoop/apache-hadoop/zookeeper/var/datalog&quot; &gt;&gt; zoo.cfg </span><br><span class="line">echo &quot;maxClientCnxns=300&quot; &gt;&gt; zoo.cfg </span><br><span class="line"></span><br><span class="line">在 zoo.cfg 里添加</span><br><span class="line">server.1=namenode00.host-shining.com:2888:3888</span><br><span class="line">server.2=namenode01.host-shining.com:2888:3888</span><br><span class="line">server.3=datanode00.host-shining.com:2888:3888</span><br><span class="line">server.4=datanode01.host-shining.com:2888:3888</span><br><span class="line">server.5=datanode02.host-shining.com:2888:3888</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /home/hadoop/apache-hadoop/zookeeper/var/data/myid </span><br><span class="line">每台机器安顺序排，这个文件是不一样的。   1、2、3、4、5</span><br></pre></td></tr></table></figure><p>用supervisor守护zookeeper   (supervisor需要重启)  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[program:zookeeper]</span><br><span class="line">command = /home/hadoop/apache-hadoop/zookeeper/bin/zkServer.sh start-foreground</span><br><span class="line">autostart = true</span><br><span class="line">autorestart = true</span><br><span class="line">startsecs = 3</span><br><span class="line">startretries = 3</span><br><span class="line">stopwaitsecs = 5</span><br><span class="line">user = hadoop</span><br><span class="line">redirect_stderr = true</span><br><span class="line">stdout_logfile = /home/shining/logs/supervisor/zookeeper.log</span><br><span class="line">stdout_logfile_maxbytes = 500MB</span><br><span class="line">stdout_logfile_backups = 5</span><br></pre></td></tr></table></figure><h4 id="安装hdfs"><a href="#安装hdfs" class="headerlink" title="安装hdfs"></a>安装hdfs</h4><h6 id="创建目录"><a href="#创建目录" class="headerlink" title="创建目录"></a>创建目录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/apache-hadoop</span><br><span class="line">tar -zxvf hadoop-2.7.2.tar.gz &amp;&amp; ln -s hadoop-2.7.2 hadoop</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/jn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/dn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/nn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs/dn_PORT</span><br><span class="line">chmod -R 755 /home/hadoop/apache-hadoop/hadoop/var/</span><br></pre></td></tr></table></figure><h5 id="开始修改配置文件"><a href="#开始修改配置文件" class="headerlink" title="开始修改配置文件"></a>开始修改配置文件</h5><h6 id="hdfs-site-xml-配置文件"><a href="#hdfs-site-xml-配置文件" class="headerlink" title="hdfs-site.xml 配置文件"></a>hdfs-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.shininghadoop&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.shininghadoop.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode00.host-shining.com:8020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.shininghadoop.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode01.host-shining.com:8020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.shininghadoop.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode00.host-shining.com:50070&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.shininghadoop.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode01.host-shining.com:50070&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;qjournal://namenode00.host-shining.com:8485;namenode01.host-shining.com:8485;datanode00.host-shining.com:8485;datanode01.host-shining.com:8485;datanode02.host-shining.com:8485/shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/dfs/jn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data00/data,/data01/data,/data02/data,/data03/data,/data04/data,/data05/data,/data06/data,/data07/data,/data08/data,/data09/data,/data10/data,/data11/data&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/dfs/nn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.shininghadoop&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;sshfence(hadoop)</span><br><span class="line">           shell(bin/true)&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60000&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;20971520&lt;/value&gt;</span><br><span class="line">    &lt;final&gt;true&lt;/final&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.block.size&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">    &lt;final&gt;true&lt;/final&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.support.append&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs/dn_PORT&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;dfs.ha.automatic-failover.enabled.appcluster&lt;/name&gt; </span><br><span class="line">  &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="core-site-xml-配置文件"><a href="#core-site-xml-配置文件" class="headerlink" title="core-site.xml 配置文件"></a>core-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;hdfs://shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Number of minutes between trash checkpoints.If zero, the trash feature is disabled.&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;namenode00.host-shining.com:2181,namenode01.host-shining.com:2181,datanode00.host-shining.com:2181,datanode01.host-shining.com:2181,datanode02.host-shining.com:2181&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.native.lib.available&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;022&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="hadoop-env-sh-配置文件-（去掉了注释部分）"><a href="#hadoop-env-sh-配置文件-（去掉了注释部分）" class="headerlink" title="hadoop-env.sh 配置文件 （去掉了注释部分）"></a>hadoop-env.sh 配置文件 （去掉了注释部分）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">JVM_OPTS=&quot;-server -verbose:gc</span><br><span class="line">  -XX:+PrintGCDateStamps</span><br><span class="line">  -XX:+PrintGCDetails</span><br><span class="line">  -XX:+UseGCLogFileRotation</span><br><span class="line">  -XX:NumberOfGCLogFiles=9</span><br><span class="line">  -XX:GCLogFileSize=20m&quot;</span><br><span class="line">export HADOOP_NAMENODE_OPTS=&quot;-Xmx40g -Xms10g -Xmn4g $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/nn_error_gc.log -Xloggc:$HADOOP_LOG_DIR/nn_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/nn_error.hprof&quot;</span><br><span class="line">export HADOOP_DATANODE_OPTS=&quot;-Xmx4g -Xms512m   $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/dn_error_gc.log -Xloggc:$HADOOP_LOG_DIR/dn_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/dn_error.hprof &quot;</span><br><span class="line">export HADOOP_JOB_HISTORYSERVER_OPTS=&quot;-Xmx4g -Xms2g   $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/log_error_gc.log -Xloggc:$HADOOP_LOG_DIR/log_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/log_error.hprof &quot;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-&quot;/etc/hadoop&quot;&#125;</span><br><span class="line">for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do</span><br><span class="line">  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then</span><br><span class="line">    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f</span><br><span class="line">  else</span><br><span class="line">    export HADOOP_CLASSPATH=$f</span><br><span class="line">  fi</span><br><span class="line">done</span><br><span class="line">export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.net.preferIPv4Stack=true&quot;</span><br><span class="line">export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;</span><br><span class="line">export HADOOP_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS&quot;</span><br><span class="line">export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_SECONDARYNAMENODE_OPTS&quot;</span><br><span class="line">export HADOOP_NFS3_OPTS=&quot;$HADOOP_NFS3_OPTS&quot;</span><br><span class="line">export HADOOP_PORTMAP_OPTS=&quot;-Xmx2048m $HADOOP_PORTMAP_OPTS&quot;</span><br><span class="line">export HADOOP_CLIENT_OPTS=&quot;-Xmx2048m $HADOOP_CLIENT_OPTS&quot;</span><br><span class="line">export HADOOP_SECURE_DN_USER=$&#123;HADOOP_SECURE_DN_USER&#125;</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=$&#123;HADOOP_LOG_DIR&#125;/$&#123;HADOOP_HDFS_USER&#125;</span><br><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br><span class="line">export HADOOP_IDENT_STRING=$USER</span><br><span class="line">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64:/usr/local/lib/</span><br></pre></td></tr></table></figure><h6 id="mapred-site-xml-配置文件"><a href="#mapred-site-xml-配置文件" class="headerlink" title="mapred-site.xml 配置文件"></a>mapred-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:10020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:19888&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/task&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-Xmx3400m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-Xmx3400m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.compress.map.output&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;LD_LIBRARY_PATH=/usr/lib64&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;9&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 默认值为一个数据块的大小--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="yarn-site-xml-配置文件"><a href="#yarn-site-xml-配置文件" class="headerlink" title="yarn-site.xml 配置文件"></a>yarn-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;shininghadoop-yarn&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;rm1,rm2&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com:2181,namenode01.host-shining.com:2181,datanode00.host-shining.com:2181,datanode01.host-shining.com:2181,datanode02.host-shining.com:2181&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- &lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;namenode01.host-shining.com&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt; </span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8031&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8032&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8030&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8033&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; --&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/local-dir&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;83968&lt;/value&gt;</span><br><span class="line">    &lt;discription&gt;每个节点可用内存,单位MB&lt;/discription&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;18&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;description&gt;Where to aggregate logs to.&lt;/description&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs/apps&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.application.classpath&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;$HADOOP_CONF_DIR,</span><br><span class="line">         $HADOOP_COMMON_HOME/share/hadoop/common/*,</span><br><span class="line">         $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,</span><br><span class="line">         $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,</span><br><span class="line">         $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/yarn/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/yarn/lib/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/mapreduce/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/mapreduce/lib/*</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/etc/hadoop/fair-scheduler.xml&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;http://namenode01.host-shining.com:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;16384&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;6&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1.8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="yarn-env-sh-配置文件"><a href="#yarn-env-sh-配置文件" class="headerlink" title="yarn-env.sh 配置文件"></a>yarn-env.sh 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_YARN_USER=$&#123;HADOOP_YARN_USER:-yarn&#125;</span><br><span class="line">export YARN_CONF_DIR=&quot;$&#123;YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf&#125;&quot;</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">export YARN_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">JVM_OPTS=&quot;-server -verbose:gc</span><br><span class="line">  -XX:+PrintGCDateStamps</span><br><span class="line">  -XX:+PrintGCDetails</span><br><span class="line">  -XX:+UseGCLogFileRotation</span><br><span class="line">  -XX:NumberOfGCLogFiles=9</span><br><span class="line">  -XX:GCLogFileSize=256m&quot;</span><br><span class="line">RESOURCEMANAGER_OPTS=&quot;-Xmx30g -Xms5g -Xmn2g $JVM_OPTS  -Xloggc:$YARN_LOG_DIR/rm_gc.log&quot;</span><br><span class="line">NODEMANAGER_OPTS=&quot;-Xmx2048m -Xms1024m -Xmn512m $JVM_OPTS  -Xloggc:$YARN_LOG_DIR/nm_gc.log&quot;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then</span><br><span class="line">  #echo &quot;run java in $JAVA_HOME&quot;</span><br><span class="line">  JAVA_HOME=$JAVA_HOME</span><br><span class="line">fi</span><br><span class="line">  </span><br><span class="line">if [ &quot;$JAVA_HOME&quot; = &quot;&quot; ]; then</span><br><span class="line">  echo &quot;Error: JAVA_HOME is not set.&quot;</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line">JAVA=$JAVA_HOME/bin/java</span><br><span class="line">JAVA_HEAP_MAX=-Xmx4096m </span><br><span class="line">if [ &quot;$YARN_HEAPSIZE&quot; != &quot;&quot; ]; then</span><br><span class="line">  JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_HEAPSIZE&quot;&quot;m&quot;</span><br><span class="line">fi</span><br><span class="line">IFS=</span><br><span class="line">if [ &quot;$YARN_LOG_DIR&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_LOG_DIR=&quot;$HADOOP_YARN_HOME/logs&quot;</span><br><span class="line">fi</span><br><span class="line">if [ &quot;$YARN_LOGFILE&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_LOGFILE=&apos;yarn.log&apos;</span><br><span class="line">fi</span><br><span class="line">if [ &quot;$YARN_POLICYFILE&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_POLICYFILE=&quot;hadoop-policy.xml&quot;</span><br><span class="line">fi</span><br><span class="line">unset IFS</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.root.logger=$&#123;YARN_ROOT_LOGGER:-INFO,console&#125;&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.root.logger=$&#123;YARN_ROOT_LOGGER:-INFO,console&#125;&quot;</span><br><span class="line">if [ &quot;x$JAVA_LIBRARY_PATH&quot; != &quot;x&quot; ]; then</span><br><span class="line">  YARN_OPTS=&quot;$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH&quot;</span><br><span class="line">fi  </span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE&quot;</span><br></pre></td></tr></table></figure><h6 id="slave-配置文件"><a href="#slave-配置文件" class="headerlink" title="slave 配置文件"></a>slave 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">datanode00.host-shining.com</span><br><span class="line">datanode01.host-shining.com</span><br><span class="line">datanode02.host-shining.com</span><br><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br><span class="line">datanode07.host-shining.com</span><br><span class="line">datanode08.host-shining.com</span><br><span class="line">datanode09.host-shining.com</span><br></pre></td></tr></table></figure><h5 id="apache-maven-安装"><a href="#apache-maven-安装" class="headerlink" title="apache maven 安装"></a>apache maven 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">tar -zxvf apache-maven-3.3.9-bin.tar.gz</span><br></pre></td></tr></table></figure><h5 id="设置环境变量-（每台机器都需要配置）"><a href="#设置环境变量-（每台机器都需要配置）" class="headerlink" title="设置环境变量 （每台机器都需要配置）"></a>设置环境变量 （每台机器都需要配置）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m -Xss1m&quot;</span><br><span class="line"></span><br><span class="line">export OOZIE_HOME=/home/hadoop/apache-hadoop/oozie</span><br><span class="line">export MAVEN_HOME=/home/hadoop/apache-maven-3.3.9</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export ZK_HOME=/home/hadoop/apache-hadoop/zookeeper</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HIVE_HOME=/home/hadoop/apache-hadoop/hive</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HDFS_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_HOME&#125;/lib/native </span><br><span class="line">export SCALA_HOME=/home/hadoop/apache-hadoop/scala</span><br><span class="line">export MAHOUT_HOME=/home/hadoop/apache-hadoop/mahout</span><br><span class="line">export MAHOUT_CONF_DIR=$MAHOUT_HOME/conf</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_HOME=/home/hadoop/apache-hadoop/spark</span><br><span class="line"></span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JAVA_HOME&#125;/jre/bin:$&#123;ZK_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;HBASE_HOME&#125;/bin:$MAHOUT_HOME/bin:$&#123;HIVE_HOME&#125;/bin:$&#123;SCALA_HOME&#125;/bin:$&#123;SPARK_HOME&#125;/bin:$&#123;MAVEN_HOME&#125;/bin:$&#123;OOZIE_HOME&#125;/bin:$PATH</span><br><span class="line">export classpath=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib:$MAHOUT_HOME/lib:$PIG_HOME/lib:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar:$HADOOP_CONF_DIR:$SPARK_HOME/lib:$HBASE_HOME/lib/native/Linux-amd64-64:/usr/local/lib:$HADOOP_HOME/lib/native</span><br><span class="line">export HBASE_LIBRARY_PATH=$&#123;HBASE_LIBRARY_PATH&#125;:$&#123;HBASE_HOME&#125;/lib/native/Linux-amd64-64:/usr/local/lib</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native/Linux-amd64-64:$HADOOP_HOME/lib/native:/usr/local/lib</span><br></pre></td></tr></table></figure><h5 id="protobuf-安装"><a href="#protobuf-安装" class="headerlink" title="protobuf 安装"></a>protobuf 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf protobuf-2.5.0.tar.gz   （每台机器安装）</span><br><span class="line">cd protobuf-2.5.0</span><br><span class="line">./configure          (root用户执行)</span><br><span class="line">make                 (root用户执行)</span><br><span class="line">make install         (root用户执行)</span><br></pre></td></tr></table></figure><h5 id="编译hdfs源码，-lib库-（编译以前需要安装maven，下载之后解压，设置环境变量即可-不需要没台机器都安装）"><a href="#编译hdfs源码，-lib库-（编译以前需要安装maven，下载之后解压，设置环境变量即可-不需要没台机器都安装）" class="headerlink" title="编译hdfs源码， lib库 （编译以前需要安装maven，下载之后解压，设置环境变量即可,不需要没台机器都安装）"></a>编译hdfs源码， lib库 （编译以前需要安装maven，下载之后解压，设置环境变量即可,不需要没台机器都安装）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget http://apache.fayea.com/hadoop/common/hadoop-2.7.2/hadoop-2.7.2-src.tar.gz</span><br><span class="line">tar -zxvf hadoop-2.7.2-src.tar.gz</span><br><span class="line">cd hadoop-2.7.2-src</span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line">cp -a hadoop-dist/target/hadoop-2.7.2/lib/native/* ~/apache-hadoop/hadoop/lib/native/</span><br></pre></td></tr></table></figure><h5 id="编译lzo压缩格式"><a href="#编译lzo压缩格式" class="headerlink" title="编译lzo压缩格式"></a>编译lzo压缩格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下载 lzo  https://codeload.github.com/twitter/hadoop-lzo/zip/master</span><br><span class="line">unzip hadoop-lzo-master.zip </span><br><span class="line">cd hadoop-lzo-master</span><br><span class="line">vim pom.xml     （修改hadoop版本）</span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.current.version&gt;2.7.2&lt;/hadoop.current.version&gt;</span><br><span class="line">    &lt;hadoop.old.version&gt;1.0.4&lt;/hadoop.old.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">mvn clean package -Dmaven.test.skip=true</span><br><span class="line">cp target/native/Linux-amd64-64/lib/*  ~/apache-hadoop/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.20-SNAPSHOT.jar  ~/apache-hadoop/hadoop/share/hadoop/common/lib/</span><br></pre></td></tr></table></figure><h4 id="安装hbase"><a href="#安装hbase" class="headerlink" title="安装hbase"></a>安装hbase</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf  hbase-1.2.2-bin.tar.gz &amp;&amp; ln -s hbase-1.2.2 hbase</span><br><span class="line">cd hbase/conf</span><br><span class="line">cp $HADOOP_HOME/etc/hadoop/core-site.xml  $HBASE_HOME/conf/</span><br><span class="line">cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml  $HBASE_HOME/conf/</span><br><span class="line">对于Hbase启用LZO</span><br><span class="line">cp -a $HADOOP_HOME/lib/native $HBASE_HOME/lib</span><br></pre></td></tr></table></figure><h5 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h5><h6 id="hbase-site-xml-配置文件"><a href="#hbase-site-xml-配置文件" class="headerlink" title="hbase.site.xml 配置文件"></a>hbase.site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">/**</span><br><span class="line"> *</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"> * or more contributor license agreements.  See the NOTICE file</span><br><span class="line"> * distributed with this work for additional information</span><br><span class="line"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"> * to you under the Apache License, Version 2.0 (the</span><br><span class="line"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"> * with the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://shininghadoop/hbase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.rest.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60050&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hbase/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com,namenode01.host-shining.com,datanode00.host-shining.com,datanode01.host-shining.com,datanode02.host-shining.com&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.master&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com,namenode01.host-shining.com&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Property from ZooKeeper&apos;sconfigzoo.cfg.The directory where the snapshot is stored.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Property from ZooKeeper&apos;sconfigzoo.cfg.Theport at which the clients will connect.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="hbase-env-sh-修改配置文件-（取消了注释的内容）"><a href="#hbase-env-sh-修改配置文件-（取消了注释的内容）" class="headerlink" title="hbase-env.sh 修改配置文件 （取消了注释的内容）"></a>hbase-env.sh 修改配置文件 （取消了注释的内容）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HBASE_HEAPSIZE=4096</span><br><span class="line">export HBASE_OPTS=&quot;-XX:+UseConcMarkSweepGC&quot;</span><br><span class="line">export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xms1024m -Xmx4096m&quot;</span><br><span class="line">export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xms1024m -Xmx4096m&quot;</span><br><span class="line">export HBASE_LOG_DIR=/home/hadoop/apache-hadoop/hbase/logs</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64</span><br><span class="line">export HBASE_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64</span><br><span class="line">export CLASSPATH=$CLASSPATH:$HBASE_LIBRARY_PATH</span><br></pre></td></tr></table></figure><h6 id="regionservers-配置文件修改"><a href="#regionservers-配置文件修改" class="headerlink" title="regionservers 配置文件修改"></a>regionservers 配置文件修改</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br></pre></td></tr></table></figure><h6 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hbase-env.sh 中   </span><br><span class="line">#export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot;</span><br><span class="line">#export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot;</span><br><span class="line">改为</span><br><span class="line">export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xms1024m -Xmx1024m&quot;</span><br><span class="line">export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xms1024m -Xmx1024m&quot; </span><br><span class="line"></span><br><span class="line">////  jdk 用1.8 的 PermSize MaxPermSize 参数没有，需要用 xmx</span><br></pre></td></tr></table></figure><h4 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h4><h6 id="数据库创建与授权"><a href="#数据库创建与授权" class="headerlink" title="数据库创建与授权"></a>数据库创建与授权</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">create database shininghadoop;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON shininghadoop.* TO &apos;shininghadoop&apos;@&quot;192.168.77.158&quot; IDENTIFIED BY &apos;shininghadoop&apos;WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON shininghadoop.* TO &apos;shininghadoop&apos;@&quot;namenode00.host-shining.com&quot; IDENTIFIED BY &apos;shininghadoop&apos;WITH GRANT OPTION;</span><br><span class="line">......   兜圈所有hadoop节点的IP地址</span><br><span class="line"></span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h6 id="导入hive元数据库"><a href="#导入hive元数据库" class="headerlink" title="导入hive元数据库"></a>导入hive元数据库</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.1.0-bin.tar.gz &amp;&amp; ln -s apache-hive-2.1.0-bin hive</span><br><span class="line">创建源数据</span><br><span class="line">cd $HIVE_HOME/scripts/metastore/upgrade/mysql/</span><br><span class="line">mysql -h数据库地址 -ushininghadoop -p</span><br><span class="line">use shininghadoop;</span><br><span class="line">source hive-schema-2.1.0.mysql.sql;</span><br></pre></td></tr></table></figure><h6 id="替换java-jdbc-jar"><a href="#替换java-jdbc-jar" class="headerlink" title="替换java jdbc jar"></a>替换java jdbc jar</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive 需要 java jar</span><br><span class="line">cd $HIVE_HOME/lib</span><br><span class="line">把 mysql-connector-java-5.1.35.jar 放在这里</span><br><span class="line">ln -s mysql-connector-java-5.1.35.jar mysql-connector-java.jar</span><br></pre></td></tr></table></figure><h6 id="hive-site-xml-配置文件-根据环境配置，线上用的是default文件"><a href="#hive-site-xml-配置文件-根据环境配置，线上用的是default文件" class="headerlink" title="hive-site.xml 配置文件  (根据环境配置，线上用的是default文件)"></a>hive-site.xml 配置文件  (根据环境配置，线上用的是default文件)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql://192.168.53.101:3306/testhadoop&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the URL of the MySQL database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;testhadoop&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;testhadoop&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; </span><br><span class="line">  &lt;value&gt;SchemaTable&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The Hive client authorization manager class name.</span><br><span class="line">  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.metastore.authorization.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;authorization manager class name to be used in the metastore for authorization.</span><br><span class="line">  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;hive client authenticator manager class name.</span><br><span class="line">  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.metastore.authenticator.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;authenticator manager class name to be used in the metastore for authentication.</span><br><span class="line">  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.group.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to some groups whenever a table gets created. </span><br><span class="line">   An example like &quot;groupX,groupY:select;groupZ:create&quot; will grant select privilege to groupX and groupY, </span><br><span class="line">   and grant create privilege to groupZ whenever a new table created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.role.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to some roles whenever a table gets created. </span><br><span class="line">   An example like &quot;roleX,roleY:select;roleZ:create&quot; will grant select privilege to roleX and roleY, </span><br><span class="line">   and grant create privilege to roleZ whenever a new table created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.owner.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;ALL&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to the owner whenever a table gets created. </span><br><span class="line">   An example like &quot;select,drop&quot; will grant select and drop privilege to the owner of the table&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.auto.convert.join&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h6 id="hive-env-sh"><a href="#hive-env-sh" class="headerlink" title="hive-env.sh"></a>hive-env.sh</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HEAPSIZE=2048</span><br><span class="line">export HIVE_CONF_DIR=/home/hadoop/apache-hadoop/hive/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/home/hadoop/apache-hadoop/hive/lib</span><br><span class="line">export HADOOP_PREFIX=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LIBEXEC_DIR=/home/hadoop/apache-hadoop/hadoop/libexec</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apache-hadoop/hadoop/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_HDFS_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_YARN_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apache-hadoop/hadoop</span><br></pre></td></tr></table></figure><h6 id="启动-hive-metastore和-server2"><a href="#启动-hive-metastore和-server2" class="headerlink" title="启动 hive metastore和 server2"></a>启动 hive metastore和 server2</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir $HIVE_HOME/hive-logs</span><br><span class="line">nohup.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">nohup  hive --service metastore &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.log 2&gt;&amp;1 &amp; </span><br><span class="line">echo $! &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.pid</span><br><span class="line">nohup hive  --service hiveserver2 &gt; /home/hadoop/apache-hadoop/hive/hive-logs/hiveserver2.log 2&gt;&amp;1 &amp; </span><br><span class="line">echo $! &gt; /home/hadoop/apache-hadoop/hive/hive-logs/hiveserver2.pid</span><br></pre></td></tr></table></figure><h5 id="hadoop启动"><a href="#hadoop启动" class="headerlink" title="hadoop启动"></a>hadoop启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 -R /home/hadoop/apache-hadoop</span><br><span class="line">hdfs zkfc -formatZK（格式化zookeeper）</span><br><span class="line">hadoop-daemon.sh start journalnode (启动journalnode)</span><br><span class="line">hdfs namenode -format（格式化namenode metadata）</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start namenode -bootstrapStandby( standy namenode)</span><br><span class="line">如果上面log没有问题</span><br><span class="line">stop-dfs.sh</span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh   (yarn的机器上执行)</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver  (namenode01 机器上执行)</span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><h5 id="spark-安装"><a href="#spark-安装" class="headerlink" title="spark 安装"></a>spark 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz    （每台机器安装）</span><br><span class="line">tar -zxvf scala-2.11.8.tgz &amp;&amp; ln -s scala-2.11.8 scala</span><br><span class="line"></span><br><span class="line">wget http://apache.fayea.com/mahout/0.12.2/apache-mahout-distribution-0.12.2.tar.gz</span><br><span class="line">tar -zxvf apache-mahout-distribution-0.12.2.tar.gz </span><br><span class="line">ln -s apache-mahout-distribution-0.12.2 mahout </span><br><span class="line">cp ~/apache-hadoop/hadoop/share/hadoop/common/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar ~/apache-hadoop/mahout/lib/</span><br><span class="line"></span><br><span class="line">wget http://mirrors.hust.edu.cn/apache/spark/spark-1.6.2/spark-1.6.2.tgz</span><br><span class="line">tar -zxvf spark-1.6.2.tgz &amp;&amp; ln -s spark-1.6.2.tgz spark-1.6.2</span><br><span class="line">cd spark/conf</span><br></pre></td></tr></table></figure><h6 id="spark-default-conf-配置文件"><a href="#spark-default-conf-配置文件" class="headerlink" title="spark-default.conf  配置文件"></a>spark-default.conf  配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://cd-namenode00.host-shining.com:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://shininghadoop/spark</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              5g</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure><h6 id="spark-env-sh-配置文件"><a href="#spark-env-sh-配置文件" class="headerlink" title="spark-env.sh 配置文件"></a>spark-env.sh 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_WORKER_MEMORY=5G</span><br><span class="line">export SPARK_MEM=$&#123;SPARK_MEM:-5g&#125;</span><br><span class="line">JAVA_OPTS=&quot;$OUR_JAVA_OPTS&quot;</span><br><span class="line">JAVA_OPTS=&quot;$JAVA_OPTS-Xms$SPARK_MEM -Xmx$SPARK_MEM&quot;</span><br><span class="line">JAVA_OPTS=&quot;$JAVA_OPTS-Djava.library.path=$SPARK_LIBRARY_PATH&quot;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HIVE_HOME=/home/hadoop/apache-hadoop/hive</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HDFS_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_HOME=/home/hadoop/apache-hadoop/spark</span><br><span class="line">export SCALA_HOME=/home/hadoop/apache-hadoop/scala</span><br><span class="line">export classpath=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib:$MAHOUT_HOME/lib:$PIG_HOME/lib:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar:$HADOOP_CONF_DIR:$SPARK_HOME/lib:$&#123;HADOOP_HOME&#125;/lib</span><br><span class="line">export HADOOP_CLASSPATH=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib/*:$MAHOUT_HOME/lib/*:$PIG_HOME/lib:$HIVE_HOME/lib/*:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/*:$HADOOP_CONF_DIR:$SPARK_HOME/lib/*:$&#123;HADOOP_HOME&#125;/lib/*:$HADOOP_CLASSPATH:</span><br><span class="line">export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$&#123;HADOOP_HOME&#125;/c++/Linux-$OS_ARCH-$OS_BIT/lib:/usr/local/lib:/usr/lib:$&#123;PBS_HOME&#125;/lib:/usr/lib64</span><br></pre></td></tr></table></figure><h6 id="slave-节点信息"><a href="#slave-节点信息" class="headerlink" title="slave 节点信息"></a>slave 节点信息</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br></pre></td></tr></table></figure><h6 id="spark-启动"><a href="#spark-启动" class="headerlink" title="spark 启动"></a>spark 启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/sbin</span><br><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h5 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h5><p>namenode地址    <a href="http://name00.host-shining.com:50070" target="_blank" rel="noopener">http://name00.host-shining.com:50070</a><br>spark 地址        <a href="http://name00.host-shining.com:8080" target="_blank" rel="noopener">http://name00.host-shining.com:8080</a><br>hbase地址        <a href="http://name00.host-shining.com:16010" target="_blank" rel="noopener">http://name00.host-shining.com:16010</a><br>yarn 地址        <a href="http://name01.host-shining.com:8088" target="_blank" rel="noopener">http://name01.host-shining.com:8088</a><br>jobhistory 地址    <a href="http://name01.host-shining.com:19888" target="_blank" rel="noopener">http://name01.host-shining.com:19888</a>  </p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>awk输出单引号或双引号</title>
      <link href="/awk-echo-SingleQuoteMark-DoubleQuotationMarks/"/>
      <url>/awk-echo-SingleQuoteMark-DoubleQuotationMarks/</url>
      <content type="html"><![CDATA[<h2 id="awk输出单引号，双引号"><a href="#awk输出单引号，双引号" class="headerlink" title="awk输出单引号，双引号"></a>awk输出单引号，双引号</h2><p>一个文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1</span><br><span class="line">1       1.1.1.1</span><br><span class="line">2       2.2.2.2</span><br></pre></td></tr></table></figure><h4 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h4><p>需要用单引号把第二列引起来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1 | awk &apos;&#123;print &quot;&apos;\&apos;&apos;&quot;$2&quot;&apos;\&apos;&apos;&quot;&#125;&apos;</span><br><span class="line">&apos;1.1.1.1&apos;</span><br><span class="line">&apos;2.2.2.2&apos;</span><br></pre></td></tr></table></figure><p>放大分解awk ，让大家看看  （这样大家应该能看清楚了）  </p><font color="#0099ff" size="5" face="黑体">awk ‘{print “ ‘ \ ‘ ‘ “ $2 “ ‘ \ ‘ ‘ “}’</font>  <h4 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h4><p>用双引号把第二列引起来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1 | awk &apos;&#123;print &quot;\&quot;&quot;$2&quot;\&quot;&quot;&#125;&apos;</span><br><span class="line">&quot;1.1.1.1&quot;</span><br><span class="line">&quot;2.2.2.2&quot;</span><br></pre></td></tr></table></figure><p>放大分解awk  </p><font color="#0099ff" size="5" face="黑体">awk ‘{print “ \ “ “ $2 “ \ “ “}’</font>  <p>希望对大家有帮助！  </p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> awk </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>nginx-1.10.1添加sticky模块实现基于cookie的负载均衡</title>
      <link href="/nginx-installed-sticky/"/>
      <url>/nginx-installed-sticky/</url>
      <content type="html"><![CDATA[<h1 id="nginx-1-10-1-添加sticky模块实现基于cookie的负载均衡"><a href="#nginx-1-10-1-添加sticky模块实现基于cookie的负载均衡" class="headerlink" title="nginx-1.10.1 添加sticky模块实现基于cookie的负载均衡"></a>nginx-1.10.1 添加sticky模块实现基于cookie的负载均衡</h1><p>在多台后台服务器的环境下，我们为了确保一个客户只和一台服务器通信，我们势必使用长连接。使用什么方式来实现这种连接呢，常见的有使用Nginx 自带的ip_hash来做，我想这绝对不是一个好的办法，如果前端是CDN，或者说一个局域网的客户同时访问服务器，导致出现服务器分配不均衡，以及不能 保证每次访问都粘滞在同一台服务器。如果基于cookie会是一种什么情形，想想看, 每台电脑都会有不同的cookie，在保持长连接的同时还保证了服务器的压力均衡，Nginx sticky值得推荐。  </p><p>如果浏览器不支持cookie，那么sticky不生效，毕竟整个模块是给予cookie实现的.</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>下载 sticky 模块<br>目前共有2个版本，一个是1.0，一个是1.1，1.0已经寿终正寝了.1.1增加了权重的参数.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://code.google.com/p/nginx-sticky-module/downloads/list</span><br><span class="line"></span><br><span class="line">或 直接下载</span><br><span class="line"></span><br><span class="line">wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/nginx-sticky-module/nginx-sticky-module-1.1.tar.gz</span><br></pre></td></tr></table></figure><h5 id="nginx-1-6-之下版本可以直接安装，1-6之上版本可以修改配置文件"><a href="#nginx-1-6-之下版本可以直接安装，1-6之上版本可以修改配置文件" class="headerlink" title="nginx-1.6 之下版本可以直接安装，1.6之上版本可以修改配置文件"></a>nginx-1.6 之下版本可以直接安装，1.6之上版本可以修改配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.6之上nginx添加stickty模块会报：</span><br><span class="line"></span><br><span class="line">ngx_http_sticky_module.c: In function ‘ngx_http_get_sticky_peer’:</span><br><span class="line">/ngx_http_sticky_module.c:333: 警告：赋值时将整数赋给指针，未作类型转换</span><br><span class="line">ake[1]: *** [objs/addon/nginx-sticky-module-1.1/ngx_http_sticky_module.o] 错误 1</span><br></pre></td></tr></table></figure><p>修改如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">修改ngx_http_sticky_misc.c 的281行</span><br><span class="line">原来</span><br><span class="line">digest-&gt;len = ngx_sock_ntop(in, digest-&gt;data, len, 1);</span><br><span class="line">修改为：</span><br><span class="line">digest-&gt;len = ngx_sock_ntop(in, sizeof(struct sockaddr_in), digest-&gt;data, len, 1);</span><br></pre></td></tr></table></figure><p>修改 ngx_http_sticky_module.c文件 （主要是1.9.x或之上版本会出现这问题）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、 第六行添加：</span><br><span class="line">    #include &lt;nginx.h&gt;</span><br></pre></td></tr></table></figure><p><img src="/images/nginx-sticky-1.jpeg" alt="image"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、第340行左右修改（iphp-&gt;rrp.current = iphp-&gt;selected_peer;）为：</span><br><span class="line">    #if defined(nginx_version) &amp;&amp; nginx_version &gt;= 1009000</span><br><span class="line">    iphp-&gt;rrp.current = peer;</span><br><span class="line">    #else</span><br><span class="line">    iphp-&gt;rrp.current = iphp-&gt;selected_peer;</span><br><span class="line">    #endif</span><br></pre></td></tr></table></figure><p><img src="/images/nginx-sticky-2.jpeg" alt="image">  </p><p>nginx 编译安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/home/shining/nginx-1.10.1 --with-http_ssl_module --with-http_gzip_static_module --with-http_stub_status_module --with-pcre --with-http_realip_module --with-http_addition_module --with-http_dav_module --with-stream --with-stream_ssl_module --pid-path=/var/run/nginx.pid --add-module=../nginx-sticky-module-1.1/  </span><br><span class="line">make  </span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>添加cookie负载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在upstream中添加sticky;</span><br><span class="line">http &#123;</span><br><span class="line">    upstream myproject&#123;</span><br><span class="line">        #添加sticky模块后加入此配置</span><br><span class="line">        sticky;</span><br><span class="line">        #被代理的服务</span><br><span class="line">        server 192.168.1.100:8081;</span><br><span class="line">        server 192.168.1.101:8080;</span><br><span class="line">    &#125;</span><br><span class="line">    #  其他配置都是一样的</span><br></pre></td></tr></table></figure><h2 id="在现有nginx上添加模块"><a href="#在现有nginx上添加模块" class="headerlink" title="在现有nginx上添加模块"></a>在现有nginx上添加模块</h2><p>nginx已经安装好， 只是添加模块的话，只需要重新编译，copy nginx 命令即可。  </p><p>sticky模块修改好之后，<br>先查看原来编译的命令：<br>/home/shining/nginx-1.10.1/sbin/nginx -V  </p><p>拿到编译的命令再重新编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/home/shining/nginx-1.10.1 --with-http_ssl_module --with-http_gzip_static_module --with-http_stub_status_module --with-pcre --with-http_realip_module --with-http_addition_module --with-http_dav_module --with-stream --with-stream_ssl_module --pid-path=/var/run/nginx.pid --add-module=../nginx-sticky-module-1.1/  </span><br><span class="line">make   ## 这之后就不有熬执行make install，否则会覆盖其他配置</span><br><span class="line"></span><br><span class="line"># 备份原来的nginx命令</span><br><span class="line">cp /home/shining/nginx-1.10.1/sbin/nginx /home/shining/nginx-1.10.1/sbin/nginx_backup</span><br><span class="line"># 替换新编译好的nginx命令</span><br><span class="line">cp ./objs/nginx /home/shining/nginx-1.10.1/sbin/</span><br></pre></td></tr></table></figure><p>查看编译：<br>/home/shining/nginx-1.10.1/sbin/nginx -t<br>生效<br>/home/shining/nginx-1.10.1/sbin/nginx -s stop<br>/home/shining/nginx-1.10.1/sbin/nginx  </p><h3 id="验证-sticky-模块是否生效"><a href="#验证-sticky-模块是否生效" class="headerlink" title="验证 sticky 模块是否生效"></a>验证 sticky 模块是否生效</h3><p>当配置完sticky策略之后，访问页面的时候再 Cookies 里看到 『route』信息  </p><p>如 : </p><p><img src="/images/nginx-sticky-3.jpeg" alt="images"></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon 约束 constraints 限制</title>
      <link href="/marathon-constraints-configure/"/>
      <url>/marathon-constraints-configure/</url>
      <content type="html"><![CDATA[<h2 id="marathon-约束-Constraints-限制"><a href="#marathon-约束-Constraints-限制" class="headerlink" title="marathon 约束 Constraints 限制"></a>marathon 约束 Constraints 限制</h2><p>Constraints控制在何处运行的应用程序，可以根据constraints属性，控制容器可以在哪个Agent节点上运行。</p><p>（往上看来好多文档写的都很类似，我就写一些不一样的吧。）</p><h3 id="mesos-agent-自定义-constraints-属性"><a href="#mesos-agent-自定义-constraints-属性" class="headerlink" title="mesos agent 自定义 constraints 属性"></a>mesos agent 自定义 constraints 属性</h3><p>rpm 安装的 mesos 可以在 /etc/mesos-slave/attributes 下定义这台agent。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;project:ppp1;IDC:BJ;oam:ops1&quot; &gt; /etc/mesos-slave/attributes</span><br></pre></td></tr></table></figure><p>重启mesos-slave服务即可，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mesos-slave</span><br></pre></td></tr></table></figure><p>这样这台Agent就有利3个属性。</p><p>分别是：</p><ul><li>project:ppp1</li><li>IDC:BJ</li><li>oam:ops1</li></ul><p>另外其他的Agent机器耶可以有相同的属性，这样就可以变成一组。（容器可以固定在相同属性的Agent机器上）</p><p>源码安装，值需要加上 –attributes 参数即可，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/mesos/mesos-slave/sbin/mesos-agent --master=zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --executor_registration_timeout=5mins --work_dir=/home/mesos/mesos --hostname=logstash00 --attributes=project:ppp1;IDC:BJ;oam:ops1</span><br></pre></td></tr></table></figure><h3 id="marathon-使用-constraints"><a href="#marathon-使用-constraints" class="headerlink" title="marathon 使用 constraints"></a>marathon 使用 constraints</h3><p>好了， 现在说说marathon 应该怎么使用 constraints</p><h4 id="容器固定在指定的Agent机器上"><a href="#容器固定在指定的Agent机器上" class="headerlink" title="容器固定在指定的Agent机器上"></a>容器固定在指定的Agent机器上</h4><p>可以通过 hostaname 来指定容器在哪台Agent上运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123;</span><br><span class="line">    &quot;id&quot;: &quot;sleep-cluster&quot;,</span><br><span class="line">    &quot;cmd&quot;: &quot;sleep 60&quot;,</span><br><span class="line">    &quot;instances&quot;: 3,</span><br><span class="line">    &quot;constraints&quot;: [[&quot;hostname&quot;, &quot;CLUSTER&quot;, &quot;host-name.test.com&quot;]]</span><br><span class="line">  &#125;&apos;</span><br></pre></td></tr></table></figure><h4 id="每个Agent上运行一个容器"><a href="#每个Agent上运行一个容器" class="headerlink" title="每个Agent上运行一个容器"></a>每个Agent上运行一个容器</h4><p>所有应用程序的任务中强制执行属性的唯一性。 确保每个主机上只运行一个应用程序任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;hostname&quot;,</span><br><span class="line">        &quot;UNIQUE&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><h4 id="利用自定义属性约束"><a href="#利用自定义属性约束" class="headerlink" title="利用自定义属性约束"></a>利用自定义属性约束</h4><p>根据自定义属性来约束在哪些Agent节点上运行容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;IDC&quot;,</span><br><span class="line">        &quot;LIKE&quot;,</span><br><span class="line">        &quot;BJ&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><p>这样就可以在有 IDC:BJ 的Agent节点上运行容器了，当然，耶可以写多个约束限制天剑的。</p><h5 id="不在哪个节点上"><a href="#不在哪个节点上" class="headerlink" title="不在哪个节点上"></a>不在哪个节点上</h5><p>指定条件，不在哪个节点运行容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;IDC&quot;,</span><br><span class="line">        &quot;UNLIKE&quot;,</span><br><span class="line">        &quot;BJ&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><h4 id="支持正则"><a href="#支持正则" class="headerlink" title="支持正则"></a>支持正则</h4><p>LIKE 接受一个正则表达式作为参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;project&quot;,</span><br><span class="line">        &quot;LIKE&quot;,</span><br><span class="line">        &quot;ppp[1-3]&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><p>mesos 可以自定义 constraints 这个就很灵活了，我们在初始化 Agent 节点的时候，就可以定义很多属性来。容器发布的时候我们就很容易约束了。</p><p>官方文档：<a href="https://mesosphere.github.io/marathon/docs/constraints.html" target="_blank" rel="noopener">https://mesosphere.github.io/marathon/docs/constraints.html</a></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker ulimit 配置</title>
      <link href="/docker-ulimit-configure/"/>
      <url>/docker-ulimit-configure/</url>
      <content type="html"><![CDATA[<h1 id="docker-ulimit-配置"><a href="#docker-ulimit-配置" class="headerlink" title="docker ulimit 配置"></a>docker ulimit 配置</h1><p>docker  设置 ulimit 方法</p><h3 id="一：通过docker-run-–ulimit-参数设置这个容器的-ulimit-值"><a href="#一：通过docker-run-–ulimit-参数设置这个容器的-ulimit-值" class="headerlink" title="一：通过docker run –ulimit 参数设置这个容器的 ulimit 值"></a>一：通过docker run –ulimit 参数设置这个容器的 ulimit 值</h3><p>如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --ulimit nofile=1024:1024 --rm debian sh -c &quot;ulimit -n&quot;</span><br></pre></td></tr></table></figure></p><p>官网说明：<br><a href="https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container—ulimit</a> </p><h3 id="二：-修改-docker-服务的-默认设置"><a href="#二：-修改-docker-服务的-默认设置" class="headerlink" title="二： 修改 docker 服务的 默认设置"></a>二： 修改 docker 服务的 默认设置</h3><p>vim /usr/lib/systemd/system/docker.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNPROC=1048576</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>systemctl daemon-reload<br>systemctl restart docker</p><p>文档： <a href="https://blog.csdn.net/signmem/article/details/51365006" target="_blank" rel="noopener">https://blog.csdn.net/signmem/article/details/51365006</a></p><ul><li>注释 设置为 infinity 时，值为 65536 ，如 LimitNOFILE=infinity，ulimit -n 值为 65536 </li></ul><h3 id="三-：-daemon-json-文件中配置"><a href="#三-：-daemon-json-文件中配置" class="headerlink" title="三 ： daemon.json 文件中配置"></a>三 ： daemon.json 文件中配置</h3><p>vim /etc/docker/daemon.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        &quot;default-ulimits&quot;: &#123;</span><br><span class="line">                &quot;nofile&quot;: &#123;</span><br><span class="line">                        &quot;Name&quot;: &quot;nofile&quot;,</span><br><span class="line">                        &quot;Hard&quot;: 64000,</span><br><span class="line">                        &quot;Soft&quot;: 64000</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>systemctl restart docker</p><p>文档 <a href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file</a></p><ul><li>注释： daemon.json  和 docker.service 两个都配置 ，daemon.json 中的配置生效。</li></ul><h4 id="For-Example"><a href="#For-Example" class="headerlink" title="For Example"></a>For Example</h4><ul><li>一个es memlock 的例子</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> ulimit </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Elasticsearch 解决 memory_lock 问题</title>
      <link href="/ES-Solve-memory-lock/"/>
      <url>/ES-Solve-memory-lock/</url>
      <content type="html"><![CDATA[<h1 id="Elasticsearch-解决-memory-lock-问题"><a href="#Elasticsearch-解决-memory-lock-问题" class="headerlink" title="Elasticsearch 解决 memory_lock 问题"></a>Elasticsearch 解决 memory_lock 问题</h1><p>我是docker运行的es，数据目录存放到本地磁盘上。最近做优化，想加上 memory_lock 参数，发现有问题。发现很多人有类似的问题。但都没说在docker上怎么解决。我整理一下解决办法，现在看来挺简单的。</p><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><p>docker 运行 es 6.5</p><p>run docker container</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>Error Message: Unable to lock JVM Memory: error=12, reason=Cannot allocate memory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-21T07:03:05,384][WARN ][o.e.b.JNANatives         ] [unknown] Unable to lock JVM Memory: error=12, reason=Cannot allocate memory</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] This can result in part of the JVM being swapped out.</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] These can be adjusted by modifying /etc/security/limits.conf, for example: </span><br><span class="line">        # allow user &apos;elasticsearch&apos; mlockall</span><br><span class="line">        elasticsearch soft memlock unlimited</span><br><span class="line">        elasticsearch hard memlock unlimited</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] If you are logged in interactively, you will have to re-login for the new limits to take effect.</span><br></pre></td></tr></table></figure><p>我们去系统里添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/secruity/limits.conf</span><br><span class="line"></span><br><span class="line">* soft  memlock  unlimited</span><br><span class="line">* hard memlock  unlimited</span><br><span class="line">elasticsearch soft memlock unlimited</span><br><span class="line">elasticsearch hard memlock unlimited</span><br></pre></td></tr></table></figure><p>之后再运行容器，问题依旧存在。<br>在网上找了好多这个问题的文章，有好多说法，我先说我解决的办法</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>只需要在docker container 运行时加上ulime的设置就可以了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>因为 docker 默认没有加载系统的 ulimit ，所以我们在系统上配置是没有用的，需要传进去。<br>这块可以查看 docker ulimit 的相关文档。</p><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><p>检查 memory_lock 是否生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://172.16.124.110:9200/_nodes?filter_path=**.mlockall</span><br></pre></td></tr></table></figure><p>返回内容为 true  及生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;nodes&quot;:&#123;&quot;pSYaXnT1SsStth5Dbww2yA&quot;:&#123;&quot;process&quot;:&#123;&quot;mlockall&quot;:true&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>如果你是二进制安装或源码，可以再系统中修改，  </p><p>添加 elasticsearch 帐号。 </p><p>limits.conf中添加<br>elasticsearch soft memlock unlimited<br>elasticsearch hard memlock unlimited  </p><p>重新登入，重新启动es。  </p><p>还有人说 在 ES_JAVA_OPS 中添加  -Djna.tmpdir=/目录<br>如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot;</span><br><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure><p>二进制安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemd:  </span><br><span class="line">/usr/lib/systemd/system/elasticsearch.service set: LimitMEMLOCK=infinity  </span><br><span class="line">SysV:  </span><br><span class="line">/etc/sysconfig/elasticsearch set: MAX_LOCKED_MEMORY=unlimited  </span><br><span class="line">Upstart:  </span><br><span class="line">/etc/default/elasticsearch set: MAX_LOCKED_MEMORY=unlimited  </span><br><span class="line">Then restart Elasticsearch.</span><br></pre></td></tr></table></figure><p>二进制和源码安装方式我没有验证过。来源于网络。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux 限制用户命令方法</title>
      <link href="/linux-Restrict-user-command-method/"/>
      <url>/linux-Restrict-user-command-method/</url>
      <content type="html"><![CDATA[<h2 id="linux-限制用户命令方法"><a href="#linux-限制用户命令方法" class="headerlink" title="linux 限制用户命令方法"></a>linux 限制用户命令方法</h2><p>linux 上想限制用户可以执行的命令，可以通过环境变量和安装lshell工具方式。</p><h4 id="环境变量的方式"><a href="#环境变量的方式" class="headerlink" title="环境变量的方式"></a>环境变量的方式</h4><p>脚本放在 /etc/profile.d/ 下，每个用户登入的时候都交脚在这里的环境变量。<br>脚本中判断当前用户，root 用户不受权限。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile.d/login.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">m=`whoami`</span><br><span class="line"></span><br><span class="line">if [[ &quot;$&#123;m&#125;&quot; != &quot;root&quot; ]];then</span><br><span class="line">    echo -e &quot;\e[01;33m* ** 你目前登录的账户是: \e[01;31m$LOGNAME\e[00m  ** \e[00m&quot;</span><br><span class="line">    echo -e &quot;\e[01;33m* ** 可以登入你申请的主机\e[00m&quot;</span><br><span class="line">    mkdir -p $HOME/bin</span><br><span class="line">    rm -f $HOME/bin/*</span><br><span class="line">    ln -s /bin/ls $HOME/bin</span><br><span class="line">    ln -s /bin/ping $HOME/bin</span><br><span class="line">    ln -s /usr/bin/ssh $HOME/bin/</span><br><span class="line">#    ln -s /usr/bin/ssh-copy-id $HOME/bin/</span><br><span class="line">    ln -s /usr/bin/ssh-keygen $HOME/bin/</span><br><span class="line">    ln -s /usr/bin/expect $HOME/bin/</span><br><span class="line">    ln -s /bin/grep $HOME/bin/</span><br><span class="line">    cat &lt;&lt; EOF &gt; $HOME/.newbash_profile</span><br><span class="line">HISTFILESIZE=500000000</span><br><span class="line">HISTSIZE=99999999</span><br><span class="line">HISTTIMEFORMAT=&quot;%Y/%m/%d_%H:%M:%S :&quot;</span><br><span class="line">PATH=$HOME/bin</span><br><span class="line">#export TMOUT=600</span><br><span class="line">export PATH</span><br><span class="line">EOF</span><br><span class="line">    chown $&#123;m&#125;:$&#123;m&#125; $HOME/.newbash_profile</span><br><span class="line">    exec bash --restricted --noprofile --rcfile $HOME/.newbash_profile</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>将允许用户执行的命令软连到用户家目录的bin下，这样用户只能执行特定的命令。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">exec bash --restricted --noprofile --rcfile $HOME/.newbash_profile </span><br><span class="line"></span><br><span class="line">这句话的意思是使用restricted模式，并且不加载系统默认的profile文件，而加载我们定义的profile文件$HOME/.newbash_profile</span><br><span class="line"></span><br><span class="line">这句话也可以添加到 $HOME/.ssh/authorized_keys，在前面加上语句：</span><br><span class="line">command=&quot;bash --restricted --noprofile --rcfile $HOME/.newbash_profile&quot; ssh-rsa ......</span><br></pre></td></tr></table></figure><h4 id="lshell-工具"><a href="#lshell-工具" class="headerlink" title="lshell 工具"></a>lshell 工具</h4><p>也可以安装 lshell 工具来实现对用户命令的限制， lshell不仅可以限制用户执行的命令，还可以限制用户对目录的限制等。   </p><p>lshell 的github地址：<a href="https://github.com/ghantoos/lshell" target="_blank" rel="noopener">https://github.com/ghantoos/lshell</a></p><p>安装  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lshell  # 需要 epel 源</span><br></pre></td></tr></table></figure><p>lshell 使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lshell --help</span><br><span class="line">Usage: lshell [OPTIONS]</span><br><span class="line"> --config &lt;file&gt; : Config file location (default /etc/lshell.conf) #指定配置文件</span><br><span class="line"> --log &lt;dir&gt; : Log files directory #指定日志目录 </span><br><span class="line"> -h, --help : Show this help message #显示帮助信息</span><br><span class="line"> --version : Show version #显示版本信息</span><br></pre></td></tr></table></figure><p>配置文件<br>配置文件分：  </p><ul><li>User configuration</li><li>Group configuration</li><li>Default configuration</li></ul><p>看一个官网的例子，很简单</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># CONFIGURATION START</span><br><span class="line">[global]</span><br><span class="line">logpath         : /var/log/lshell/</span><br><span class="line">loglevel        : 2</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">allowed         : [&apos;ls&apos;,&apos;pwd&apos;]</span><br><span class="line">forbidden       : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;] </span><br><span class="line">warning_counter : 2</span><br><span class="line">timer           : 0</span><br><span class="line">path            : [&apos;/etc&apos;, &apos;/usr&apos;]</span><br><span class="line">env_path        : &apos;:/sbin:/usr/foo&apos;</span><br><span class="line">scp             : 1 # or 0</span><br><span class="line">sftp            : 1 # or 0</span><br><span class="line">overssh         : [&apos;rsync&apos;,&apos;ls&apos;]</span><br><span class="line">aliases         : &#123;&apos;ls&apos;:&apos;ls --color=auto&apos;,&apos;ll&apos;:&apos;ls -l&apos;&#125;</span><br><span class="line"></span><br><span class="line">[grp:users]</span><br><span class="line">warning_counter : 5</span><br><span class="line">overssh         : - [&apos;ls&apos;]</span><br><span class="line"></span><br><span class="line">[foo]</span><br><span class="line">allowed         : &apos;all&apos; - [&apos;su&apos;]</span><br><span class="line">path            : [&apos;/var&apos;, &apos;/usr&apos;] - [&apos;/usr/local&apos;]</span><br><span class="line">home_path       : &apos;/home/users&apos;</span><br><span class="line"></span><br><span class="line">[bar]</span><br><span class="line">allowed         : + [&apos;ping&apos;] - [&apos;ls&apos;] </span><br><span class="line">path            : - [&apos;/usr/local&apos;]</span><br><span class="line">strict          : 1</span><br><span class="line">scpforce        : &apos;/home/bar/uploads/&apos;</span><br><span class="line"># CONFIGURATION END</span><br></pre></td></tr></table></figure><p>我简单使用的一个案例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# cat /etc/lshell.conf </span><br><span class="line"># lshell.py configuration file</span><br><span class="line">#</span><br><span class="line"># $Id: lshell.conf,v 1.27 2010/10/18 19:05:17 ghantoos Exp $</span><br><span class="line"></span><br><span class="line">[global]</span><br><span class="line">##  log directory (default /var/log/lshell/ )</span><br><span class="line">logpath         : /var/log/lshell/</span><br><span class="line">##  set log level to 0, 1, 2, 3 or 4  (0: no logs, 1: least verbose,</span><br><span class="line">##                                                 4: log all commands)</span><br><span class="line">loglevel        : 2</span><br><span class="line">##  configure log file name (default is %u i.e. username.log)</span><br><span class="line">#logfilename     : %y%m%d-%u</span><br><span class="line">#logfilename     : syslog</span><br><span class="line"></span><br><span class="line">##  in case you are using syslog, you can choose your logname</span><br><span class="line">#syslogname      : myapp</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">##  a list of the allowed commands or &apos;all&apos; to allow all commands in user&apos;s PATH</span><br><span class="line">allowed         : [&apos;ls&apos;,&apos;echo&apos;,&apos;cd&apos;,&apos;ll&apos;]</span><br><span class="line"></span><br><span class="line">##  a list of forbidden character or commands</span><br><span class="line">#forbidden       : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;,&apos;`&apos;,&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br><span class="line">forbidden       : [&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br><span class="line"></span><br><span class="line">##  a list of allowed command to use with sudo(8)</span><br><span class="line">#sudo_commands   : [&apos;ls&apos;, &apos;more&apos;]</span><br><span class="line"></span><br><span class="line">##  number of warnings when user enters a forbidden value before getting </span><br><span class="line">##  exited from lshell, set to -1 to disable.</span><br><span class="line">warning_counter : 2</span><br><span class="line"></span><br><span class="line">##  command aliases list (similar to bash’s alias directive)</span><br><span class="line">aliases         : &#123;&apos;ll&apos;:&apos;ls -l&apos;, &apos;vi&apos;:&apos;vim&apos;&#125;</span><br><span class="line"></span><br><span class="line">##  introduction text to print (when entering lshell)</span><br><span class="line">intro           : &quot;线上环境请谨慎执行命令\n执行help或者?\n列出可执行的命令\n执行lpath\n查看允许访问的路径&quot;</span><br><span class="line"></span><br><span class="line">##  configure your promt using %u or %h (default: username)</span><br><span class="line">prompt          : &quot;%u@%h&quot;</span><br><span class="line"></span><br><span class="line">##  a value in seconds for the session timer</span><br><span class="line">timer           : 0</span><br><span class="line"></span><br><span class="line">##  list of path to restrict the user &quot;geographicaly&quot;</span><br><span class="line">#path            : [&apos;/home/bla/&apos;,&apos;/etc&apos;]</span><br><span class="line"></span><br><span class="line">##  set the home folder of your user. If not specified the home_path is set to </span><br><span class="line">##  the $HOME environment variable</span><br><span class="line">#home_path       : &apos;/home/bla/&apos;</span><br><span class="line"></span><br><span class="line">##  update the environment variable $PATH of the user</span><br><span class="line">env_path        : &apos;:/usr/local/bin:/usr/sbin:/bin&apos;</span><br><span class="line"></span><br><span class="line">##  add environment variables</span><br><span class="line">#env_vars        : &#123;&apos;foo&apos;:1, &apos;bar&apos;:&apos;helloworld&apos;&#125;</span><br><span class="line"></span><br><span class="line">##  allow or forbid the use of scp (set to 1 or 0)</span><br><span class="line">#scp             : 1</span><br><span class="line"></span><br><span class="line">## forbid scp upload</span><br><span class="line">#scp_upload       : 0</span><br><span class="line"></span><br><span class="line">## forbid scp download</span><br><span class="line">#scp_download     : 0</span><br><span class="line"></span><br><span class="line">##  allow of forbid the use of sftp (set to 1 or 0)</span><br><span class="line">#sftp            : 1</span><br><span class="line"></span><br><span class="line">##  list of command allowed to execute over ssh (e.g. rsync, rdiff-backup, etc.)</span><br><span class="line">overssh         : [&apos;ls&apos;,&apos;sed&apos;,&apos;cp&apos;,&apos;mkdir&apos;,&apos;date&apos;,&apos;&gt;&apos;,&apos;;&apos;,&apos;&amp;&amp;&apos; ]</span><br><span class="line"></span><br><span class="line">##  logging strictness. If set to 1, any unknown command is considered as </span><br><span class="line">##  forbidden, and user&apos;s warning counter is decreased. If set to 0, command is</span><br><span class="line">##  considered as unknown, and user is only warned (i.e. *** unknown synthax)</span><br><span class="line">#strict          : 1</span><br><span class="line"></span><br><span class="line">##  force files sent through scp to a specific directory</span><br><span class="line">#scpforce        : &apos;/home/bla/uploads/&apos;</span><br><span class="line"></span><br><span class="line">##  history file maximum size </span><br><span class="line">history_size     : 9999</span><br><span class="line"></span><br><span class="line">##  set history file name (default is /home/%u/.lhistory)</span><br><span class="line">#history_file     : &quot;/home/%u/.lshell_history&quot;</span><br><span class="line"></span><br><span class="line">[rd]</span><br><span class="line">allowed : [ &apos;ls&apos;,&apos;cd&apos;,&apos;ll&apos;,&apos;ifconfig&apos;,&apos;less&apos;,&apos;echo&apos;,&apos;ip&apos;,&apos;&gt;&apos;,&apos;date&apos;,&apos;grep&apos;,&apos;cat&apos;,&apos;awk&apos;,&apos;|&apos;,&apos;telnet&apos;,&apos;ps&apos;,&apos;ping&apos;,&apos;netstat&apos;,&apos;more&apos;,&apos;jps&apos;,&apos;free&apos;,&apos;du&apos;,&apos;df&apos;,&apos;top&apos;,&apos;tail&apos;,&apos;sed&apos;,&apos;curl&apos;,&apos;date&apos;,&apos;iostat&apos;,&apos;iotop&apos;,&apos;pwd&apos;,&apos;diff&apos;,&apos;uptime&apos;,&apos;hostname&apos;,&apos;nslookup&apos; ]</span><br><span class="line">home_path : &apos;/home/rd&apos;    # 用户的家目录</span><br><span class="line">env_path : &apos;:/usr/local/bin:/usr/sbin:/sbin:/bin:/usr/local/sbin:/ust/bin&apos;</span><br><span class="line">path : [ &apos;/home/testdir&apos;,&apos;/home/rd&apos; ]  # 允许用户访问的目录</span><br><span class="line"></span><br><span class="line">#forbidden : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;,&apos;`&apos;,&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br></pre></td></tr></table></figure><p>修改用户shell为lshell</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">usermod rd -s /usr/bin/lshell</span><br><span class="line">或</span><br><span class="line">chsh -s /usr/bin/lshell rd</span><br><span class="line"></span><br><span class="line">新用户</span><br><span class="line">useradd rd -d /home/rd -s /usr/bin/lshell</span><br></pre></td></tr></table></figure><p>添加组 （方便记录日志）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usermod -aG lshell rd</span><br></pre></td></tr></table></figure><p>记录日志： (我没有使用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">groupadd --system lshell</span><br><span class="line">mkdir /var/log/lshell</span><br><span class="line">chown :lshell /var/log/lshell</span><br><span class="line">chmod 770 /var/log/lshell</span><br></pre></td></tr></table></figure><p>注释： lshell 没有重启， 随时修改配置文件，用户重新登入即可生效。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> command </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>parted 分区方法</title>
      <link href="/parted-partition/"/>
      <url>/parted-partition/</url>
      <content type="html"><![CDATA[<h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><p>parted 支持2TB以上的磁盘分区，并且允许调整分区的大小。</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><h4 id="MBR分区表：（MBR含义：主引导记录）"><a href="#MBR分区表：（MBR含义：主引导记录）" class="headerlink" title="MBR分区表：（MBR含义：主引导记录）"></a>MBR分区表：（MBR含义：主引导记录）</h4><ul><li>所支持的最大卷：2T （T; terabytes,1TB=1024GB）</li><li>对分区的设限：最多4个主分区或3个主分区加一个扩展分区。</li></ul><h4 id="GPT分区表：（GPT含义：GUID分区表）"><a href="#GPT分区表：（GPT含义：GUID分区表）" class="headerlink" title="GPT分区表：（GPT含义：GUID分区表）"></a>GPT分区表：（GPT含义：GUID分区表）</h4><ul><li>支持最大卷：18EB，（E：exabytes,1EB=1024TB）  </li><li>每个磁盘最多支持128个分区</li></ul><h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h2><h4 id="大于2T的整个磁盘分一个分区"><a href="#大于2T的整个磁盘分一个分区" class="headerlink" title="大于2T的整个磁盘分一个分区"></a>大于2T的整个磁盘分一个分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb</span><br><span class="line">可以输入p打印磁盘信息，查看分区的情况，找到起始和结束位置。</span><br><span class="line"></span><br><span class="line">mklabel gpt</span><br><span class="line">　　设置分区类型为gpt</span><br><span class="line"></span><br><span class="line">mkpart primary 0% 100%</span><br><span class="line">　　primary指分区类型为主分区，0是分区开始位置，100%是分区结束位置。相同的命令为：mkpart primary 0 -1 或者是：mkpart  primary 0  XXX 结束的空间</span><br><span class="line"></span><br><span class="line">print</span><br><span class="line">　　打印当前分区,查看分区设置是否正确</span><br><span class="line">　　</span><br><span class="line">quit</span><br><span class="line">　　退出</span><br><span class="line">　　</span><br><span class="line">mkfs.xfs /dev/sdb1</span><br><span class="line">格式化</span><br></pre></td></tr></table></figure><h4 id="设置lvm分区"><a href="#设置lvm分区" class="headerlink" title="设置lvm分区"></a>设置lvm分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb</span><br><span class="line">mklabel gpt</span><br><span class="line">mkpart primary xfs 0G 300G      # 300G的磁盘</span><br><span class="line">mkpart primary xfs 300G 500G    # 200G的磁盘</span><br><span class="line">mkpart primary xfs 500G -0G   # 500G到剩余所有空间的分区</span><br><span class="line">print</span><br><span class="line">set 1 lvm on   # 设置 第一个分区为 lvm 文件系统</span><br><span class="line">print</span><br><span class="line">rm 2       # 删除 2 分区  </span><br><span class="line">quit</span><br><span class="line"></span><br><span class="line">mkfs.xfs /dev/sdb2</span><br><span class="line">mkfs.xfs /dev/sdb3</span><br></pre></td></tr></table></figure><h4 id="批量分区"><a href="#批量分区" class="headerlink" title="批量分区"></a>批量分区</h4><p>自己用的一个批量分区脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br><span class="line">yum install -y expect parted</span><br><span class="line"></span><br><span class="line">#for letter in b c d e f g h i j k l m</span><br><span class="line">for letter in `fdisk -l | grep 4000 | awk &apos;&#123;print $2&#125;&apos; | cut -c 8 | sort`</span><br><span class="line">do</span><br><span class="line">expect -c &apos;set timeout -1;</span><br><span class="line">spawn parted /dev/sd&apos;$letter&apos;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;mklabel gpt\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;unit GB\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;mkpart primary 0 -1\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;quit\n&quot;;</span><br><span class="line">interact&apos;</span><br><span class="line"></span><br><span class="line">nohup mkfs.xfs /dev/sd$&#123;letter&#125;1 &gt; sd$letter.out 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="非交互式模式"><a href="#非交互式模式" class="headerlink" title="非交互式模式"></a>非交互式模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># parted /dev/sdb mklabel gpt </span><br><span class="line"># parted /dev/sdb mkpart primary 0 300G</span><br><span class="line"># parted /dev/sdb mkpart primary 300G 1000G </span><br><span class="line"># parted /dev/sdb mkpart logical 1000G -0G</span><br><span class="line"># parted /dev/sdb p</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> parted </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>源码安装mesos</title>
      <link href="/source-install-mesos/"/>
      <url>/source-install-mesos/</url>
      <content type="html"><![CDATA[<h1 id="源码安装mesos"><a href="#源码安装mesos" class="headerlink" title="源码安装mesos"></a>源码安装mesos</h1><p>源码安装mesos</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.apache.org/dist/mesos/1.3.0/mesos-1.3.0.tar.gz</span><br><span class="line"></span><br><span class="line">需要 mvn 环境 export MAVEN_HOME</span><br><span class="line">yum groupinstall -y &quot;Development Tools&quot;</span><br><span class="line">yum install apr* patch libcurl libcurl-devel path python-devel java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel subversion subversion-devel systemtap systemtap-client zlib-devel</span><br><span class="line"></span><br><span class="line">tar -zxvf mesos-1.3.0.tar.gz</span><br><span class="line">cd mesos-1.3.0</span><br><span class="line">./configure --prefix=/home/mesos/mesos-slave</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>官网安装说明 ：   <a href="http://mesos.apache.org/documentation/latest/building/" target="_blank" rel="noopener">http://mesos.apache.org/documentation/latest/building/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Install a few utility tools</span><br><span class="line">$ sudo yum install -y tar wget git</span><br><span class="line"></span><br><span class="line"># Fetch the Apache Maven repo file.</span><br><span class="line">$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo</span><br><span class="line"></span><br><span class="line"># Install the EPEL repo so that we can pull in &apos;libserf-1&apos; as part of our</span><br><span class="line"># subversion install below.</span><br><span class="line">$ sudo yum install -y epel-release</span><br><span class="line"></span><br><span class="line"># &apos;Mesos &gt; 0.21.0&apos; requires &apos;subversion &gt; 1.8&apos; devel package,</span><br><span class="line"># which is not available in the default repositories.</span><br><span class="line"># Create a WANdisco SVN repo file to install the correct version:</span><br><span class="line">$ sudo bash -c &apos;cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF</span><br><span class="line">[WANdiscoSVN]</span><br><span class="line">name=WANdisco SVN Repo 1.9</span><br><span class="line">enabled=1</span><br><span class="line">baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/\$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco</span><br><span class="line">EOF&apos;</span><br><span class="line"></span><br><span class="line"># Parts of Mesos require systemd in order to operate. However, Mesos</span><br><span class="line"># only supports versions of systemd that contain the &apos;Delegate&apos; flag.</span><br><span class="line"># This flag was first introduced in &apos;systemd version 218&apos;, which is</span><br><span class="line"># lower than the default version installed by centos. Luckily, centos</span><br><span class="line"># 7.1 has a patched &apos;systemd &lt; 218&apos; that contains the &apos;Delegate&apos; flag.</span><br><span class="line"># Explicity update systemd to this patched version.</span><br><span class="line">$ sudo yum update systemd</span><br><span class="line"></span><br><span class="line"># Install essential development tools.</span><br><span class="line">$ sudo yum groupinstall -y &quot;Development Tools&quot;</span><br><span class="line"></span><br><span class="line"># Install other Mesos dependencies.</span><br><span class="line">$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.8.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel</span><br><span class="line"></span><br><span class="line"># Change working directory.</span><br><span class="line">$ cd mesos</span><br><span class="line"></span><br><span class="line"># Bootstrap (Only required if building from git repository).</span><br><span class="line">$ ./bootstrap</span><br><span class="line"></span><br><span class="line"># Configure and build.</span><br><span class="line">$ mkdir build</span><br><span class="line">$ cd build</span><br><span class="line">$ ../configure</span><br><span class="line">$ make</span><br><span class="line"></span><br><span class="line"># Run test suite.</span><br><span class="line">$ make check</span><br><span class="line"></span><br><span class="line"># Install (Optional).</span><br><span class="line">$ make install</span><br></pre></td></tr></table></figure><p>用super启动mesos-agent</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash00 conf]# cat mesos-agent.ini </span><br><span class="line">[program:mesos-agent]</span><br><span class="line">command = /home/mesos/mesos-slave/sbin/mesos-agent --master=zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --executor_registration_timeout=5mins --work_dir=/home/mesos/mesos --hostname=logstash00 --attributes=nginx:ok;service:nginx;test:ok;app:nginx;db:ok;server:nginx</span><br><span class="line">autostart = true</span><br><span class="line">autorestart = true</span><br><span class="line">startsecs = 3</span><br><span class="line">startretries = 3</span><br><span class="line">stopwaitsecs = 5</span><br><span class="line">user = root</span><br><span class="line">redirect_stderr = true</span><br><span class="line">stdout_logfile = /home/mesos/logs/supervisor/mesos-agent.log</span><br><span class="line">stdout_logfile_maxbytes = 500MB</span><br><span class="line">stdout_logfile_backups = 3</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>logstash 配置文件写法</title>
      <link href="/logstash-configure-file/"/>
      <url>/logstash-configure-file/</url>
      <content type="html"><![CDATA[<h1 id="logstash-配置文件"><a href="#logstash-配置文件" class="headerlink" title="logstash 配置文件"></a>logstash 配置文件</h1><h5 id="开启http接口，并把收集到的日志放入ES中。"><a href="#开启http接口，并把收集到的日志放入ES中。" class="headerlink" title="开启http接口，并把收集到的日志放入ES中。"></a>开启http接口，并把收集到的日志放入ES中。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    http &#123;</span><br><span class="line">         host =&gt; &quot;0.0.0.0&quot;</span><br><span class="line">         port =&gt; 7881   # 开启端口</span><br><span class="line">         codec =&gt; json  # 格式化 json</span><br><span class="line">          add_field =&gt; &#123;   # 添加字段，在接受到的每条日志中添加 marathon：base-marathon 一个字段</span><br><span class="line">            &quot;marathon&quot; =&gt; &quot;base-marathon&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">##  判断日志中包含 oam_type 的key 放到相应的ES索引中。192.168.5</span><br><span class="line">output &#123;</span><br><span class="line">     if [oam_type] == &quot;hadoop&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-hadoop-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else if [oam_type] == &quot;kafka&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-kafka-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else if [oam_type] == &quot;es&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-es-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-marathon-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="从kafka中取出nginx日志，放入到HDFS上"><a href="#从kafka中取出nginx日志，放入到HDFS上" class="headerlink" title="从kafka中取出nginx日志，放入到HDFS上"></a>从kafka中取出nginx日志，放入到HDFS上</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;10.10.110.122:2181,10.10.110.123:2181,10.10.110.124:2181/kafka&quot;</span><br><span class="line">        group_id =&gt; &quot;logstash-kafka-hdfs&quot;</span><br><span class="line">        topic_id =&gt; &quot;prd_nginx_access&quot;</span><br><span class="line">        codec =&gt; plain</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 格式化，将十二个月转换成数字。</span><br><span class="line">filter &#123;</span><br><span class="line">        grok &#123;</span><br><span class="line">                match =&gt; &#123;</span><br><span class="line">                        &quot;message&quot; =&gt;&quot;^(?&lt;hostname&gt;.+?)\s(?&lt;modulname&gt;.+?)\s(?&lt;remote_addr&gt;.+?)\s\-\s(?&lt;remote_user&gt;.+?)\s\[(?&lt;Day&gt;.+?)/(?&lt;Month&gt;.+?)/(?&lt;Year&gt;.+?):(?&lt;Hour&gt;.+?):&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if [Month] == &quot;Jan&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;01&quot;]</span><br><span class="line">                        &#125; </span><br><span class="line">        &#125; else if [Month] == &quot;Feb&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;02&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Mar&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;03&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Apr&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;04&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;May&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;05&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Jun&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;06&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Jul&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;07&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Aug&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;08&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Sep&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;09&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Oct&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;10&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Nov&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;11&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Dec&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;12&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        if [modulname] &#123;</span><br><span class="line">           webhdfs &#123;</span><br><span class="line">                workers =&gt; 1</span><br><span class="line">                host =&gt; &quot;namenode-master.host.com&quot;</span><br><span class="line">                port =&gt; 14000</span><br><span class="line">                user =&gt; &quot;hadoop&quot;</span><br><span class="line">                path =&gt; &quot;/Data/Logs/domain=%&#123;modulname&#125;/dt=%&#123;Year&#125;%&#123;Month&#125;%&#123;Day&#125;/hour=%&#123;Hour&#125;/%&#123;modulname&#125;_%&#123;Year&#125;%&#123;Month&#125;%&#123;Day&#125;%&#123;Hour&#125;.log&quot;</span><br><span class="line">                flush_size =&gt; 5000</span><br><span class="line">                compression =&gt; &quot;gzip&quot;</span><br><span class="line">                idle_flush_time =&gt; 6</span><br><span class="line">                retry_interval =&gt; 3</span><br><span class="line">                retry_times =&gt; 3</span><br><span class="line">                codec =&gt; line &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">           file &#123;</span><br><span class="line">                path =&gt; &quot;/home/logs/supervisor/logstash_prd_kafka_hdfs_error.log&quot;</span><br><span class="line">                codec =&gt;  line &#123; format =&gt; &quot;custom format: %&#123;message&#125;&quot; &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="收集nginx日志放到kafka中"><a href="#收集nginx日志放到kafka中" class="headerlink" title="收集nginx日志放到kafka中"></a>收集nginx日志放到kafka中</h5><p> 日志格式为文本, logstash 放到kafka中会变成一个大的json串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/nginx/logs/accesslog/**/*.log&quot;]</span><br><span class="line">        exclude =&gt; [&quot;/home/nginx/logs/accesslog/11.test.com/*.log&quot;,&quot;/home/nginx/logs/accesslog/2.test.com/*.log&quot;,&quot;/home/nginx/logs/accesslog/3.test.com/*.log&quot;]</span><br><span class="line">        sincedb_path =&gt; &quot;/home/optools/logstash/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        discover_interval =&gt; 10</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 5</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        bootstrap_servers =&gt; &quot;kafka00:9092,kafka01:9092,kafka02:9092&quot;</span><br><span class="line">        topic_id =&gt; &quot;prd_nginx_access&quot;</span><br><span class="line">        compression_type =&gt; &quot;gzip&quot;</span><br><span class="line">        codec =&gt; plain &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="处理json日志"><a href="#处理json日志" class="headerlink" title="处理json日志"></a>处理json日志</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/logs/v4-weblog/*.log&quot;]</span><br><span class="line">        sincedb_path =&gt; &quot;/home/logstash/conf/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        codec =&gt; &quot;json&quot;  # 往后端传是json，如果后端要文本，codec =&gt; &quot;plain&quot;</span><br><span class="line">        discover_interval =&gt; 10</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 5</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        kafka &#123;</span><br><span class="line">        topic_id =&gt; &quot;weblogv4_mx_wandafilm&quot;</span><br><span class="line">        bootstrap_servers =&gt; &quot;192.168.5.30:9092,192.168.5.38:9092,192.168.5.48:9092&quot;</span><br><span class="line">        codec =&gt; plain &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    </span><br><span class="line">    #stdout&#123;</span><br><span class="line">    #    codec =&gt; rubydebug</span><br><span class="line">    #&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.18:2181,192.168.5.28:2181,192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;huawei_hard_monitor&quot;</span><br><span class="line">        topic_id =&gt; &quot;huawei_hard_monitor&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        grok &#123;</span><br><span class="line">                match =&gt; &#123;</span><br><span class="line">                        &quot;SNMPv2-SMI::enterprises.2011.23.2.1&quot; =&gt; &quot;^Location:(?&lt;Location&gt;.*?); Time:(?&lt;Time&gt;.*?); Sensor:(?&lt;Sensor&gt;.*?); Severity:(?&lt;Severity&gt;.*?); Code:(?&lt;Code&gt;.*?); Description:(?&lt;Description&gt;.*?)$&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mutate &#123;</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-MIB::sysUpTime.0&quot;, &quot;SNMPv2-MIB--sysUpTime-0&quot;]</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-MIB::snmpTrapOID.0&quot;, &quot;SNMPv2-MIB--snmpTrapOID-0&quot;]</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-SMI::enterprises.2011.23.2.1&quot;, &quot;SNMPv2-SMI--enterprises_2011_23_2_1&quot;]</span><br><span class="line">         &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">           elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-huawei_hard_monitor-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 50000</span><br><span class="line">                idle_flush_time =&gt; 30</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">            &#125;</span><br><span class="line">#        stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="nginx-日志收集"><a href="#nginx-日志收集" class="headerlink" title="nginx 日志收集"></a>nginx 日志收集</h5><p>中文转码，\x 转为Xx \\x 转为 XXx<br>添加字段，nginx access 和 error 日志放在不同索引中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181,192.168.5.18:2181,192.168.5.28:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;logstash-docker-nginx&quot;</span><br><span class="line">        topic_id =&gt; &quot;test_for_docker&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 4  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        ruby &#123;</span><br><span class="line">                code =&gt; &quot;</span><br><span class="line">                  event[&apos;log&apos;] = event[&apos;log&apos;].gsub(&apos;\x&apos;,&apos;Xx&apos;)</span><br><span class="line">                  event[&apos;log&apos;] = event[&apos;log&apos;].gsub(&apos;\\x&apos;,&apos;XXx&apos;)</span><br><span class="line">                  &quot;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        if &quot;http_cookie&quot; in [log] &#123;</span><br><span class="line">                mutate &#123; add_tag =&gt; &quot;nginx-access&quot; &#125;</span><br><span class="line">                json &#123;</span><br><span class="line">                         source =&gt; &quot;log&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                mutate &#123;</span><br><span class="line">                        convert =&gt; [</span><br><span class="line">                        &quot;status&quot;, &quot;integer&quot;,</span><br><span class="line">                        &quot;body_bytes_sent&quot; , &quot;integer&quot;,</span><br><span class="line">                        &quot;upstream_response_time&quot;, &quot;float&quot;,</span><br><span class="line">                        &quot;request_time&quot;, &quot;float&quot;</span><br><span class="line">                        ]</span><br><span class="line">                        remove_field =&gt; &quot;log&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                geoip &#123;</span><br><span class="line">                        source =&gt; &quot;ip&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                date &#123;</span><br><span class="line">                        match =&gt; [&quot;time_local&quot;, &quot;ISO8601&quot;]</span><br><span class="line">                        locale =&gt;&quot;en&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">                 mutate &#123; add_tag =&gt; &quot;nginx-error&quot; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        if &quot;nginx-access&quot;  in [tags] &#123;</span><br><span class="line">           elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-nginxaccess-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 10240</span><br><span class="line">                idle_flush_time =&gt; 30</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">                &#125;</span><br><span class="line">         &#125; else if &quot;nginx-error&quot; in [tags] &#123;</span><br><span class="line">             elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-nginxerror-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 100</span><br><span class="line">                idle_flush_time =&gt; 5</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">#               stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="转换时间"><a href="#转换时间" class="headerlink" title="转换时间"></a>转换时间</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.18:2181,192.168.5.28:2181,192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;es-hdfs&quot;</span><br><span class="line">        topic_id =&gt; &quot;logdata-es&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        date &#123;</span><br><span class="line">                match =&gt; [ &quot;time&quot; , &quot;yyyy-MM-dd HH:mm:ss&quot; ]</span><br><span class="line">                locale =&gt; &quot;zh&quot;</span><br><span class="line">                timezone =&gt; &quot;-00:00:00&quot;</span><br><span class="line">                target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">     elasticsearch &#123;</span><br><span class="line">        hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">        index =&gt; &quot;logstash-%&#123;app&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">        #document_type =&gt; &quot;%&#123;type&#125;&quot;</span><br><span class="line">        flush_size =&gt; 3840</span><br><span class="line">        idle_flush_time =&gt; 10</span><br><span class="line">        template_overwrite =&gt; true</span><br><span class="line">    &#125;</span><br><span class="line">#  stdout &#123; codec =&gt; rubydebug &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="一个nginx-json-日志收集的例子"><a href="#一个nginx-json-日志收集的例子" class="headerlink" title="一个nginx json 日志收集的例子"></a>一个nginx json 日志收集的例子</h4><p>nginx 配置日志格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">log_format main   &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos;</span><br><span class="line">                    &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos;</span><br><span class="line">                    &apos;&quot;remote_user&quot;:&quot;$remote_user&quot;,&apos;</span><br><span class="line">                    &apos;&quot;ip&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                    &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos;</span><br><span class="line">                    &apos;&quot;domain&quot;:&quot;$server_name&quot;,&apos;</span><br><span class="line">                    &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request&quot;:&quot;$request_uri&quot;,&apos;</span><br><span class="line">                    &apos;&quot;requesturl&quot;:&quot;$request&quot;,&apos;</span><br><span class="line">                    &apos;&quot;args&quot;:&quot;$args&quot;,&apos;</span><br><span class="line">                    &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                    &apos;&quot;status&quot;: $status,&apos;</span><br><span class="line">                    &apos;&quot;responsetime&quot;:$request_time,&apos;</span><br><span class="line">                    &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                    &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_cookie&quot;:&quot;$http_cookie&quot;,&apos;</span><br><span class="line">                    &apos;&quot;https&quot;:&quot;$https&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_body&quot;:&quot;$request_body&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_x_clientid&quot;:&quot;$http_x_clientid&quot;&apos;</span><br><span class="line">                    &apos;&#125;&apos;;</span><br></pre></td></tr></table></figure><p>logstash 手机nginx日志，并处理转码问题。（需要2.4版本之上，才能支持这个正则）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/nginx/logs/accesslog/**/*.log&quot;]</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        sincedb_path =&gt; &quot;/home/logstash/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        discover_interval =&gt; 30</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 10</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">ruby &#123;</span><br><span class="line">                code =&gt; &quot;if event.get(&apos;message&apos;).include?(&apos;\x&apos;) then</span><br><span class="line">    event.set(&apos;message&apos;, event.get(&apos;message&apos;).gsub(/\\x([0-9A-F]&#123;2&#125;)/) &#123;</span><br><span class="line">        case $1</span><br><span class="line">            when &apos;22&apos;</span><br><span class="line">                &apos;\\&quot;&apos;</span><br><span class="line">            when &apos;0D&apos;</span><br><span class="line">                &apos;\\r&apos;</span><br><span class="line">            when &apos;0A&apos;</span><br><span class="line">                &apos;\\n&apos;</span><br><span class="line">            when &apos;27&apos;</span><br><span class="line">                &apos;\\\&apos;&apos;</span><br><span class="line">            when &apos;5C&apos;</span><br><span class="line">                &apos;\\\\&apos;</span><br><span class="line">            else</span><br><span class="line">                $1.hex.chr</span><br><span class="line">        end</span><br><span class="line">    &#125;)</span><br><span class="line">end&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        json &#123;</span><br><span class="line">                source =&gt; &quot;message&quot;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">mutate &#123;</span><br><span class="line">        remove_field =&gt;[&quot;message&quot;]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        bootstrap_servers =&gt; &quot;192.168.0.2:9092,192.168.0.3:9092,192.168.0.4:9092&quot;</span><br><span class="line">        topic_id =&gt; &quot;mtopic_name&quot;</span><br><span class="line">        compression_type =&gt; &quot;gzip&quot;</span><br><span class="line">    &#125;</span><br><span class="line">#stdout &#123; codec =&gt; rubydebug &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> logstash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logstash </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon事件日志收集</title>
      <link href="/marathon-EventLog-to-ES/"/>
      <url>/marathon-EventLog-to-ES/</url>
      <content type="html"><![CDATA[<h2 id="marathon-事件日志收集到ES中"><a href="#marathon-事件日志收集到ES中" class="headerlink" title="marathon 事件日志收集到ES中"></a>marathon 事件日志收集到ES中</h2><p>  marathon 有配置可以主动把 marathon 的事件日志发送到 http 接口上。这里的事件包括，发布容器的json内容，docker容器死掉、启动，运行、kill，健康检查等日志。也可以说 marathon 所有的动作日志这上都会有。所有我要把这个日志收集起来，并可以做监控，历史查询等等。<br>  先说一下我的方案：<br>  启动一个logstash实例，并配置http接口，把收集到的日志存到ES索引中。<br>  marathon 配置 logstash 的 http 接口地址，把日志发送到这里即可。<br>  后期可以在ES中查询你想要的事件日志，并报警，或用kibana出图等。  </p><h3 id="logstash-配置"><a href="#logstash-配置" class="headerlink" title="logstash 配置"></a>logstash 配置</h3><p>#####logstash 配置文件<br>vim /etc/logstash.conf  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    http &#123;</span><br><span class="line">         host =&gt; &quot;0.0.0.0&quot;</span><br><span class="line">         port =&gt; 7882</span><br><span class="line">         codec =&gt; json</span><br><span class="line">          add_field =&gt; &#123;</span><br><span class="line">            &quot;marathon&quot; =&gt; &quot;base-marathon&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">     elasticsearch &#123;</span><br><span class="line">          hosts =&gt; [&quot;10.214.193.27:9250&quot;,&quot;10.214.193.28:9250&quot;,&quot;10.214.193.29:9250&quot;]</span><br><span class="line">          index =&gt; &quot;logstash-marathon-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">          flush_size =&gt; 10000</span><br><span class="line">          idle_flush_time =&gt; 60</span><br><span class="line">          template_overwrite =&gt; true</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 add_field 添加一个字段  “marathon”:”base-marathon”<br>这样 ES的一个索引所可以放多个marathon的日志，但是需要启动多个logstash，每个配置不同，因为marathon的日志是一样的，区别不出来是哪个集群的日志。  </p><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><p>可以用supervisor或docker方式启动。就不在这里说了。</p><h3 id="marathon-配置"><a href="#marathon-配置" class="headerlink" title="marathon 配置"></a>marathon 配置</h3><p>我用的是 yum 安装的marathon，版本是 1.4.9 , 配置是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/marathon/conf/</span><br><span class="line"></span><br><span class="line">echo &quot;http_callback&quot; &gt; /etc/marathon/conf/event_subscriber</span><br><span class="line">echo &quot;http://logstashIP:7882&quot; &gt; /etc/marathon/conf/http_endpoints</span><br></pre></td></tr></table></figure><p>二进制配置的参数是一样的。  </p><p>重启服务加载配置  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart marathon</span><br></pre></td></tr></table></figure><h4 id="ES-上查询日志"><a href="#ES-上查询日志" class="headerlink" title="ES 上查询日志"></a>ES 上查询日志</h4><p>这样配置完成之后，等marathon有动作的话，ES上就会有数据了，   </p><p>如：<br><img src="/images/marathon-es-data.jpeg" alt="marathon-es-data"></p><p>这只是一个事件，还还有好多时间呢！</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Unix / Linux ssh-add Command Examples to Add SSH Key to Agent (转)</title>
      <link href="/Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent/"/>
      <url>/Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent/</url>
      <content type="html"><![CDATA[<h3 id="Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent"><a href="#Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent" class="headerlink" title="Unix / Linux ssh-add Command Examples to Add SSH Key to Agent"></a>Unix / Linux ssh-add Command Examples to Add SSH Key to Agent</h3><p>ssh-add is a helper program for ssh-agent.<br>ssh-add adds RSA or DSA identity files to the ssh agent. For ssh-add to work properly, the agent should be running, and have the SSH_AUTH_SOCK environment variable set.  </p><ol><li>Fix “Could not Open” Error (and Add Default RSA/DSA identities)<br>By default, when you try to execute the ssh-add command, you might get “Could not open a connection to your authentication agent.” error message as shown below.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">Could not open a connection to your authentication agent.</span><br><span class="line">The reason is ssh-agent is not running.  </span><br><span class="line">But, if you start the ssh-agent as shown below, you’ll still get the same error.  </span><br><span class="line"></span><br><span class="line">$ ssh-agent</span><br><span class="line">SSH_AUTH_SOCK=/tmp/ssh-cYYsc14689/agent.14689; export SSH_AUTH_SOCK;</span><br><span class="line">SSH_AGENT_PID=14690; export SSH_AGENT_PID;</span><br><span class="line">echo Agent pid 14690;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">Could not open a connection to your authentication agent.</span><br><span class="line">In order to fix the issue, you should start the ssh-agent as shown below.  </span><br><span class="line"></span><br><span class="line">$ exec ssh-agent bash</span><br><span class="line">Now, when you execute the ssh-add, it will add the ~/.ssh/id_rsa, ~/.ssh/id_dsa and ~/.ssh/identity files to ssh-agent, and will not throw any error message.</span><br><span class="line"></span><br><span class="line">$ ssh-add</span><br><span class="line">Identity added: /home/ramesh/.ssh/id_rsa (/home/ramesh/.ssh/id_rsa)</span><br><span class="line">Identity added: /home/ramesh/.ssh/id_dsa (/home/ramesh/.ssh/id_dsa)</span><br></pre></td></tr></table></figure><ol start="2"><li>Display the entries loaded in ssh-agent<br>Use either -l or -L as shown below to display all the RSA and DSA entries that are currently loaded into the ssh-agent.<br>The following examples shows that there are two entries currently loaded to the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">2048 34:36:63:c2:7d:a5:13:e4 /home/ramesh/.ssh/id_rsa (RSA)</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br><span class="line"></span><br><span class="line">$ ssh-add -L</span><br><span class="line">ssh-rsa A2EAAAABIwAAAQEAtVRcaEnxOef0n5WLr9DV1JsLpx4E+P2Zf/N9JBLBbVKDD1BZf</span><br><span class="line">eRmLK8hZZKf0iva8+q1VNyxQB5oTfKGr79ll7KDRwfIgErw== /home/ramesh/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">ssh-dsa 8WDTpyJiLUNlIXSfCRe7nOjeMlgyn8vM3cWsosO0x4eMDYEMvefzhev0RAtbhyBvs</span><br><span class="line">WLLCwkaVzCZdZvsDa2cl7zKRd+3zLSfBQRa1wpMjJaeJbCg== /home/ramesh/.ssh/id_dsa</span><br></pre></td></tr></table></figure><ol start="3"><li>Delete all entries from ssh-agent<br>Use option -D as shown below to remove all the ssh entries from the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -D</span><br><span class="line">All identities removed.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">The agent has no identities.</span><br></pre></td></tr></table></figure><ol start="4"><li>Delete specific entries from ssh-agent<br>Using -d option, you can specify exactly what entries you like to delete.<br>The following example will remove only the default RSA entry from the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">2048 34:36:63:c2:7d:a5:13:e4 /home/ramesh/.ssh/id_rsa (RSA)</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br><span class="line"></span><br><span class="line">$ ssh-add -d /home/ramesh/.ssh/id_rsa</span><br><span class="line">Identity removed: /home/ramesh/.ssh/id_rsa (/home/ramesh/.ssh/id_rsa.pub)</span><br><span class="line"></span><br><span class="line">$ ssh-add -l</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br></pre></td></tr></table></figure><ol start="5"><li>Lock (or) Unlock the SSH Agent<br>You can lock the ssh agent as shown below using -x option. Once you lock the agent, you cannot add, delete, or list entries in the ssh agent without a password.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -x</span><br><span class="line">Enter lock password:</span><br><span class="line">Again:</span><br><span class="line">Agent locked.</span><br><span class="line">After locking, if you try to add, you’ll se SSH_AGENT_FAILURE message as shown below.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">SSH_AGENT_FAILURE</span><br><span class="line">SSH_AGENT_FAILURE</span><br><span class="line">Could not add identity: /home/ramesh/.ssh/id_rsa</span><br><span class="line">To unlock an agent, use -X option as shown below. Make sure you enter the same password that you gave while locking the agent. If you give a wrong password, you’ll set “Failed to unlock agent.” message.</span><br><span class="line"></span><br><span class="line">$ ssh-add -X</span><br><span class="line">Enter lock password:</span><br><span class="line">Agent unlocked.</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cgroup 服务限制进程资源</title>
      <link href="/cgroup-limit-process-resources/"/>
      <url>/cgroup-limit-process-resources/</url>
      <content type="html"><![CDATA[<h2 id="用-cgroup-服务限制进程资源"><a href="#用-cgroup-服务限制进程资源" class="headerlink" title="用 cgroup 服务限制进程资源"></a>用 cgroup 服务限制进程资源</h2><h3 id="CGroup-功能及组成"><a href="#CGroup-功能及组成" class="headerlink" title="CGroup 功能及组成"></a>CGroup 功能及组成</h3><p>  CGroup 是将任意进程进行分组化管理的 Linux 内核功能。CGroup 本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O 或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。这些具体的资源管理功能称为 CGroup 子系统或控制器。CGroup 子系统有控制内存的 Memory 控制器、控制进程调度的 CPU 控制器等。运行中的内核可以使用的 Cgroup 子系统由/proc/cgroup 来确认。  </p><p>  CGroup 提供了一个 CGroup 虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用 CGroup，必须挂载 CGroup 文件系统。这时通过挂载选项指定使用哪个子系统。</p><h4 id="安装cgroup服务"><a href="#安装cgroup服务" class="headerlink" title="安装cgroup服务"></a>安装cgroup服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">centos 6</span><br><span class="line">yum install -y libcgroup</span><br><span class="line"></span><br><span class="line">centos 7 </span><br><span class="line">yum install -y libcgroup libcgroup-tools</span><br></pre></td></tr></table></figure><h4 id="配置cgroup配置"><a href="#配置cgroup配置" class="headerlink" title="配置cgroup配置"></a>配置cgroup配置</h4><p>  这里需要是限制cpu<br>  先创建一个组，把需要限制的进程，启动的时候放到这组下。  </p><p>vim /etc/cgconfig.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mount &#123;</span><br><span class="line">#       cpuset  = /cgroup/cpuset;</span><br><span class="line">        cpu     = /cgroup/cpu;</span><br><span class="line">#       cpuacct = /cgroup/cpuacct;</span><br><span class="line">#       memory  = /cgroup/memory;</span><br><span class="line">#       devices = /cgroup/devices;</span><br><span class="line">#       freezer = /cgroup/freezer;</span><br><span class="line">#       net_cls = /cgroup/net_cls;</span><br><span class="line">#       blkio   = /cgroup/blkio;</span><br><span class="line">&#125;</span><br><span class="line">group yarn &#123;       # yarn 为组名</span><br><span class="line">   perm &#123;</span><br><span class="line">    task &#123;</span><br><span class="line">        uid = hadoop;     # 权限设置，为hadoop</span><br><span class="line">        gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">    admin &#123;</span><br><span class="line">       uid = hadoop;</span><br><span class="line">       gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   cpu &#123;   # 可以用 cpu.cfs_period_us 和 cpu.cfs_quota_us 来限制该组中的所有进程在单位时间里可以使用的 cpu 时间。</span><br><span class="line">          cpu.cfs_period_us= 100000;  # 就是时间周期，默认为 100000，即百毫秒  值的范围： 1000-100000 </span><br><span class="line">          cpu.cfs_quota_us= 2160000;  # cpu.cfs_quota_us 就是在这期间内可使用的 cpu 时间，默认 -1，即无限制</span><br><span class="line">   &#125;   # 现在这个设置代表，这个组可以用的cpu为21.6盒，2160000/100000 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动cgroup服务"><a href="#启动cgroup服务" class="headerlink" title="启动cgroup服务"></a>启动cgroup服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service cgroup restart</span><br><span class="line">chkconfig cgroup on</span><br></pre></td></tr></table></figure><h4 id="启动-yarn-服务"><a href="#启动-yarn-服务" class="headerlink" title="启动 yarn 服务"></a>启动 yarn 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br><span class="line">cgexec -g cpu:yarn yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><p>注释： 用cgexe启动的服务，他的子进程也会在这个cgroup组下。总体cpu加和不会超过组的设置。</p><p>查看进程在哪个组下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@db-datanode09 ~]$ ps -eo pid,cgroup,cmd</span><br><span class="line">153514 cpu:/                               /sbin/udevd -d</span><br><span class="line">153515 cpu:/                               /sbin/udevd -d</span><br><span class="line">154089 cpu:/yarn                           /usr/java/jdk1.8.0_45/bin/java -Dproc_nodemanager -Xmx4096m -Dhadoop.log.dir=/home/hadoop/apache-hadoop/hadoop/logs</span><br><span class="line"></span><br><span class="line"># 注释： cpu:/  代表在cgroup根配置下，cpu:/yarn 代表在根的yarn的配置下</span><br></pre></td></tr></table></figure><p>检查服务  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /sys/fs/cgroup/cpu/yarn</span><br><span class="line">cat tasks | grep &quot;nodemanager PID&quot;</span><br></pre></td></tr></table></figure><p>这是在 /cgroup/cpu/ 目录下就会出现 yarn 目录， 权限是hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@db-datanode09 cpu]# cd /cgroup/cpu/yarn</span><br><span class="line">[root@db-datanode09 yarn]# ls -l</span><br><span class="line">total 0</span><br><span class="line">--w--w---- 1 hadoop hadoop 0 Aug  1 14:19 cgroup.event_control</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cgroup.procs</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.cfs_period_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.cfs_quota_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.rt_period_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.rt_runtime_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.shares</span><br><span class="line">-r--r--r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.stat</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 notify_on_release</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:22 tasks</span><br></pre></td></tr></table></figure><hr><h2 id="cgroup服务几种模式介绍"><a href="#cgroup服务几种模式介绍" class="headerlink" title="cgroup服务几种模式介绍"></a>cgroup服务几种模式介绍</h2><h4 id="cgroup-配置文件说明"><a href="#cgroup-配置文件说明" class="headerlink" title="cgroup 配置文件说明"></a>cgroup 配置文件说明</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mount &#123;</span><br><span class="line">       cpuset  = /cgroup/cpuset;为cgroup中的任务分配独立的cpu</span><br><span class="line">       cpu     = /cgroup/cpu;使用调度程序对cpu的使用控制</span><br><span class="line">       cpuacct = /cgroup/cpuacct;自动生成cgroup中的cpu使用的报告</span><br><span class="line">       memory  = /cgroup/memory;管理任务的内存</span><br><span class="line">       devices = /cgroup/devices;允许或拒绝cgroup中的任务访问设备</span><br><span class="line">       freezer = /cgroup/freezer;挂起或者恢复任务</span><br><span class="line">       net_cls = /cgroup/net_cls;控制网络流量</span><br><span class="line">       blkio   = /cgroup/blkio;为块设备输入输出设置，比如物理设备(磁盘，usb等)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cpu限制"><a href="#cpu限制" class="headerlink" title="cpu限制"></a>cpu限制</h4><p>cgroup中对cpu资源控制的方式大约有三种：  </p><p>1.通过cpu子系统中的cpu quote方式</p><p>2.通过cpu子系统中的cpu share方式</p><p>3.通过cpuset子系统中的cpuset 将任务绑定到相应的cpu核上</p><p>cpuset的方式是限定任务可以在哪些cpu上运行；cpu share的方式，是在控制群组中设置权重，通过权重和任务等来分配能够使用cpu的资源；</p><h6 id="通过cpu-quote方式来限制"><a href="#通过cpu-quote方式来限制" class="headerlink" title="通过cpu quote方式来限制"></a>通过cpu quote方式来限制</h6><p>启动cgroup服务后，可以在/cgroup/cpu目录下看到如下文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">total 0</span><br><span class="line">--w--w--w- 1 root   root   0 Jul 26 11:44 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.cfs_period_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.cfs_quota_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.rt_period_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.rt_runtime_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.shares</span><br><span class="line">-r--r--r-- 1 root   root   0 Jul 26 11:44 cpu.stat</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 notify_on_release</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 release_agent</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 tasks</span><br></pre></td></tr></table></figure><p>这里做一下说明：</p><p>cpu.cfs_period_us： 单位是微秒，最大值是1s，最小值是1毫秒(ms),取值范围为1000-1000000</p><p>cpu.cfs_quota_us 单位是微秒，意思是在 cpu.cfs_period_us的时间内，用户可以占用的时间。对于单核来说，最大等于 cpu.cfs_period_us的值，对于多核来说，可以理解为最多可使用的cpu核数</p><p>cpu.stat:</p><p>nr_periods 时间间隔， 指经过了多少个cpu.cfs_period_us的时间间隔 nr_throttled 被限制运行的次数 throttled_time 总共被限制的时间，微秒</p><p>在多核的系统中， cpu.cfs_quota_us/cpu.cfs_period_us 的值就是可以使用的最大的cpu的核数</p><p>tasks 将需要控制的任务的id写入到tasks文件中，就可以控制资源了</p><h6 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h6><p>cpu限制也有分组、分层的概念，<br>如： /cgroup/cpu  这是cpu的根级，默认不限制cpu使用量，<br>  /cgroup/cpu/yarn 这是cpu下的一层，也是一个组，他的cpu使用量不能大于上一层设置。<br>  还可以在/cgroup/cpu/yarn 下创建其他层。</p><h6 id="进程添加到控制组"><a href="#进程添加到控制组" class="headerlink" title="进程添加到控制组"></a>进程添加到控制组</h6><ol><li>单一pid添加到某个控制组<br> echo pid &gt; /cgroup/cpu/yarn/tasks</li><li>cgrule服务<br>用法：  </li></ol><p>user hierarchies control_group<br>user:command hierarchies control_group<br>当在user 使用前缀时代表是一个组群而不是单独用户例如@admins 是admins组群中的所有用户<br>cgrule配置文件在/etc/cgrule.conf,配置好启动服务后就可以根据规则自动将任务附加到控制群组了。</p><p>如：<br>vim /etc/cgrule.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Example:</span><br><span class="line">#&lt;user&gt;         &lt;controllers&gt;   &lt;destination&gt;</span><br><span class="line">#@student       cpu,memory      usergroup/student/</span><br><span class="line">#peter          cpu             test1/</span><br><span class="line">#%              memory          test2/</span><br><span class="line">rd           cpu             yarn     # rd 用户所有进程的cpu限制都在yarn这个组里</span><br><span class="line">@hadoopcpuyarn  # hadoop 组里所有用户的进程cpu限制都在yarn这个组里</span><br><span class="line">mtime:scpcpuyarn  # mtime的scp命令的cpu限制在yarn这个组里</span><br></pre></td></tr></table></figure><p>启动服务：  </p><p>/etc/init.d/cgred restart  </p><ol start="3"><li>cgexec 命令启动服务<br>用法：<br>cgexec -g subsystems:path_to_cgroup command arguments<br>如：<br>cgexec -g cpu:yarn yarn-daemon.sh start nodemanager</li></ol><h3 id="redhat-cgroup"><a href="#redhat-cgroup" class="headerlink" title="redhat cgroup"></a>redhat cgroup</h3><p>关于其他资源 如 memory、network等限制，可以参考 radhat cgroup的介绍<br>地址：<br>centos 6<br> <a href="https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups" target="_blank" rel="noopener">https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups</a></p><p>centos 7<br><a href="https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/7/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups" target="_blank" rel="noopener">https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/7/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups</a></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kafka修改分区、副本数、副本迁移</title>
      <link href="/kafka-Modify-Partitions-and-ReplicationFactor/"/>
      <url>/kafka-Modify-Partitions-and-ReplicationFactor/</url>
      <content type="html"><![CDATA[<h2 id="kafka修改分区和副本数"><a href="#kafka修改分区和副本数" class="headerlink" title="kafka修改分区和副本数"></a>kafka修改分区和副本数</h2><h3 id="查看现在副本分配情况"><a href="#查看现在副本分配情况" class="headerlink" title="查看现在副本分配情况"></a>查看现在副本分配情况</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --describe  --topic test1</span><br><span class="line"></span><br><span class="line">Topic:test1       PartitionCount:3        ReplicationFactor:2     Configs:</span><br><span class="line">        Topic: test1      Partition: 0    Leader: 2       Replicas: 2,4   Isr: 2,4</span><br><span class="line">        Topic: test1      Partition: 1    Leader: 3       Replicas: 3,5   Isr: 3,5</span><br><span class="line">        Topic: test1      Partition: 2    Leader: 4       Replicas: 4,1   Isr: 4,1</span><br></pre></td></tr></table></figure><h3 id="topic-分区扩容"><a href="#topic-分区扩容" class="headerlink" title="topic 分区扩容"></a>topic 分区扩容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper 127.0.0.1:2181 -alter --partitions 4 --topic test1</span><br></pre></td></tr></table></figure><h3 id="修改副本数量、副本迁移"><a href="#修改副本数量、副本迁移" class="headerlink" title="修改副本数量、副本迁移"></a>修改副本数量、副本迁移</h3><h4 id="这个文件自己创建-格式按照下面的格式就可以了"><a href="#这个文件自己创建-格式按照下面的格式就可以了" class="headerlink" title="这个文件自己创建 格式按照下面的格式就可以了"></a>这个文件自己创建 格式按照下面的格式就可以了</h4><p>根据topic的分区情况自行修改 partitions-topic.json 文件配置  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        &quot;partitions&quot;:</span><br><span class="line">                [</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 0,</span><br><span class="line">                        &quot;replicas&quot;: [1,2]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 1,</span><br><span class="line">                        &quot;replicas&quot;: [0,3]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 2,</span><br><span class="line">                        &quot;replicas&quot;: [4,5]</span><br><span class="line">                &#125;</span><br><span class="line">                ],</span><br><span class="line">        &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="执行副本搬迁"><a href="#执行副本搬迁" class="headerlink" title="执行副本搬迁"></a>执行副本搬迁</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file partitions-topic.json --execute</span><br></pre></td></tr></table></figure><h4 id="查看迁移情况："><a href="#查看迁移情况：" class="headerlink" title="查看迁移情况："></a>查看迁移情况：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file partitions-topic.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,0] is still in progress</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,1] completed successfully</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,2] is still in progress</span><br></pre></td></tr></table></figure><h4 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h4><p>kafka-reassign-partitions.sh工具来重新分布分区。该工具有三种使用模式：  </p><ol><li>generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）</li><li>execute模式，根据指定的reassign plan重新分配Partition</li><li>verify模式，验证重新分配Partition是否成功</li></ol><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker搭建macvlan网络</title>
      <link href="/docker-build-MacVlanNetwork/"/>
      <url>/docker-build-MacVlanNetwork/</url>
      <content type="html"><![CDATA[<h2 id="docker-搭建macvlan-网络"><a href="#docker-搭建macvlan-网络" class="headerlink" title="docker 搭建macvlan 网络"></a>docker 搭建macvlan 网络</h2><p>  简单说，macvlan就是在宿主的网卡设置多个vlan信息，根据走的网卡不同，并带有不行的vlan标记。  </p><h3 id="交换机需要支持"><a href="#交换机需要支持" class="headerlink" title="交换机需要支持"></a>交换机需要支持</h3><p>macvlan需要交换机上有几个设置：  </p><ul><li>连接宿主的交换机接口需要改为 Trunk 模式。（这样才能多vlan通过这个口通讯）</li><li>交换机上添加macvlan设置的相应vlan信息。</li><li>三层交换机上设置各个vlan的网关地址。并实现vlan间互联。</li></ul><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>环境介绍  </p><table><thead><tr><th>宿主机IP</th><th>宿主机vlan</th><th>macvlan IP</th><th>vlan 号</th></tr></thead><tbody><tr><td>192.168.53.11</td><td>233</td><td>172.20.30.x</td><td>30</td></tr><tr><td>192.168.53.12</td><td>233</td><td>172.20.19.x</td><td>19</td></tr></tbody></table><h4 id="实时生效安装"><a href="#实时生效安装" class="headerlink" title="实时生效安装"></a>实时生效安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release</span><br><span class="line">yum install -y vconfig</span><br><span class="line">加载模块哦</span><br><span class="line">modprobe 8021q</span><br><span class="line">lsmod |grep -i 8021q</span><br><span class="line">网卡开启混合模式</span><br><span class="line">ip link set em1 promisc on</span><br><span class="line">使用vconfig命令配置vlan </span><br><span class="line">vconfig add em1 233 </span><br><span class="line">vconfig add em1 30   # 另外一台设置  vconfig add em1 19</span><br><span class="line">在em1接口上配置两个VLAN </span><br><span class="line">vconfig set_flag em1.233 1 1 </span><br><span class="line">vconfig set_flag em1.30 1 1   # 另外一台 vconfig set_flag em1.19 1 1</span><br><span class="line"></span><br><span class="line">ifconfig em1 0.0.0.0 </span><br><span class="line">ifconfig em1.233 192.168.53.11 netmask 255.255.255.0 up </span><br><span class="line">ifconfig em1.30 172.20.30.2 netmask 255.255.255.0 up</span><br></pre></td></tr></table></figure><p>这样一个临时配置就可以了， 配置docker的网络就可以，docker配置网络的命令后面一起发吧，  </p><p>上面属于临时配置，机器重启配置就没有了，不适合生产。</p><h4 id="永久配置"><a href="#永久配置" class="headerlink" title="永久配置"></a>永久配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">um install -y epel-release</span><br><span class="line">yum install -y vconfig</span><br></pre></td></tr></table></figure><p>添加模块<br>vim /etc/rc.d/rc.local 添加  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sbin/modprobe 8021q</span><br></pre></td></tr></table></figure><p>网卡开启混合模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;PROMISC=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-em1</span><br></pre></td></tr></table></figure><p>修改王凯配置文件<br>vim /etc/sysconfig/network-scripts/ifcfg-em1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1</span><br><span class="line">NAME=em1</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">BONDING_MASTER=yes</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">PEERDNS=yes</span><br><span class="line">PROMISC=yes</span><br></pre></td></tr></table></figure><p>生成 macvlan 网卡</p><p>vim /etc/sysconfig/network-scripts/ifcfg-em1.233</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1.233</span><br><span class="line">NAME=em1.233</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.53.11</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.53.1</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">VLAN=yes</span><br><span class="line">NM_CONTROLLED=no</span><br></pre></td></tr></table></figure><p>vim /etc/sysconfig/network-scripts/ifcfg-em1.30</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1.30</span><br><span class="line">NAME=em1.30</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">VLAN=yes</span><br><span class="line">NM_CONTROLLED=no</span><br></pre></td></tr></table></figure><p>另外一台 其他配置都一样， ifcfg-em1.30 网卡信息修改为 ifcfg-em1.19 即可。  </p><p>之后重启网卡，如果配置没有问题，网络是可以连接的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br></pre></td></tr></table></figure><p>以后新添加vlan的时候，也可以先做好配置文件。直接ifup即可。<br>ifup /etc/sysconfig/network-scripts/ifcfg-em1.19</p><p>网络信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@wd-slave01 ~]# ifconfig</span><br><span class="line">em1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 108408  bytes 17234693 (16.4 MiB)</span><br><span class="line">        RX errors 0  dropped 11508  overruns 0  frame 0</span><br><span class="line">        TX packets 24225  bytes 4849942 (4.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line">        </span><br><span class="line">docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 0.0.0.0</span><br><span class="line">        inet6 fe80::42:87ff:fecd:c222  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 02:42:87:cd:c2:22  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 458940  bytes 71009715 (67.7 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 198525  bytes 55224280 (52.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">em1.233: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.53.12  netmask 255.255.255.0  broadcast 192.168.53.255</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 108408  bytes 17234693 (16.4 MiB)</span><br><span class="line">        RX errors 0  dropped 11508  overruns 0  frame 0</span><br><span class="line">        TX packets 24225  bytes 4849942 (4.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">em1.30: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 2133458  bytes 245138875 (233.7 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 1343034  bytes 151915911 (144.8 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h4 id="docker-配置网络"><a href="#docker-配置网络" class="headerlink" title="docker 配置网络"></a>docker 配置网络</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=172.20.30.0 --gateway=172.20.30.1 -o parent=em1.30 mac_net1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker network ls   查看网络情况    </span><br><span class="line">docker network inspect 074ebc238447  查看网络详细信息及ip地址分配清凉</span><br></pre></td></tr></table></figure><p>启动容器   指定IP 指定网络  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name test1 --ip=172.55.55.10 --network mac_net1 nginx-nettools:1.13  </span><br><span class="line">或动态分配</span><br><span class="line">docker run -d --name test2  --network mac_net1 nginx-nettools:1.13</span><br></pre></td></tr></table></figure><p>限制分配ip地址池</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=172.20.30.0/24 --gateway=172.20.30.1 --ip-range=172.20.30.48/30 -o parent=em1.20 mac_net30</span><br><span class="line">这样只能分配4个ip地址</span><br><span class="line">172.20.30.128/25 也就是 128-255 可得 128个ip地址</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> macvlan </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux 删除乱码文件</title>
      <link href="/linux-Delete-garbled-files/"/>
      <url>/linux-Delete-garbled-files/</url>
      <content type="html"><![CDATA[<h2 id="linux-利用-inum-删除乱码文件"><a href="#linux-利用-inum-删除乱码文件" class="headerlink" title="linux 利用 inum 删除乱码文件"></a>linux 利用 inum 删除乱码文件</h2><p>  当系统中产生一些乱码文件的时候，rm直接是删除不掉的。如 “-，&amp;”等一些特殊字符。<br>  这时候我们可以利用linux 的inum 号来找到这个文件，并删除。</p><p>  例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# ll -i</span><br><span class="line">总用量 4</span><br><span class="line">   213388 -rw-r--r--. 1 root root    0 6月   4 07:40 -c</span><br><span class="line">134938544 drwxr-xr-x. 2 root root   23 12月 18 05:12 123</span><br><span class="line">   213391 -rw-r--r--. 1 root root    0 6月   4 07:40 --poolmetadata</span><br><span class="line">   213390 -rw-r--r--. 1 root root    0 6月   4 07:40 --thinpool</span><br><span class="line">   213387 -rw-r--r--. 1 root root    0 6月   4 07:40 --zero</span><br></pre></td></tr></table></figure><p>利用inum 号删除文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除文件或文件夹</span><br><span class="line">find ./ -inum 213388 -print -exec rm &#123;&#125; -rf \;</span><br><span class="line">删除文件</span><br><span class="line">find ./ -inum 213388 -delete;</span><br></pre></td></tr></table></figure><p>也可以重命名乱码文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find ./ -inum 213388 -exec mv &#123;&#125; newfile \;</span><br></pre></td></tr></table></figure><p>文件名字就改为了 newfile</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> rm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>logstash out file to HDFS</title>
      <link href="/logstash-out-file-to-HDFS/"/>
      <url>/logstash-out-file-to-HDFS/</url>
      <content type="html"><![CDATA[<h2 id="logstash-out-file-to-HDFS"><a href="#logstash-out-file-to-HDFS" class="headerlink" title="logstash out file to HDFS"></a>logstash out file to HDFS</h2><p>  logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。<br>  logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。<br>  即 <a href="http://namenode00:50070/webhdfs/v1/" target="_blank" rel="noopener">http://namenode00:50070/webhdfs/v1/</a>  接口  </p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>  可以在官网找到相应的版本， 我们用的是2.3.1，下载地址：  </p><pre><code>https://www.elastic.co/downloads/past-releases  </code></pre><p>  webhdfs插件地址  </p><pre><code>github地址：  git clone  https://github.com/heqin5136/logstash-output-webhdfs-discontinued.git官网地址及使用说明：  https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html</code></pre><p>插件安装方式：</p><pre><code>logstash 安装在 /home/mtime/logstash-2.3.1git clone  https://github.com/heqin5136/logstash-output-webhdfs-discontinued.gitcd logstash-output-webhdfs-discontinued/home/mtime/logstash-2.3.1/bin/plugin install logstash-output-webhdfs-discontinued</code></pre><p>检查hdfs的webhds接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">   curl -i  &quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;op=LISTSTATUS&quot;   </span><br><span class="line">   </span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Cache-Control: no-cache</span><br><span class="line">Expires: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Date: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Expires: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Date: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Content-Type: application/json</span><br><span class="line">Set-Cookie: hadoop.auth=&quot;u=hadoop&amp;p=hadoop&amp;t=simple&amp;e=1499957619679&amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Server: Jetty(6.1.26)</span><br></pre></td></tr></table></figure><p>注释： active namenode 返回是200 ，standby namenode 返回是403.</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>添加 logstash 一个配置文件</p><p>vim /home/mtime/logstash-2.3.1/conf/hdfs.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    zk_connect =&gt; &quot;192.168.51.191:2181,192.168.51.192:2181,192.168.51.193:2181&quot;   ## kafka zk 地址 </span><br><span class="line">    group_id =&gt; &apos;hdfs&apos;   # 消费者组</span><br><span class="line">    topic_id =&gt; &apos;tracks&apos;  # topic 名字</span><br><span class="line">    consumer_threads =&gt; 1  </span><br><span class="line">    codec =&gt; &apos;json&apos;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;   ##  为解决 插入hdfs时间相差8小时， </span><br><span class="line">        date &#123;  </span><br><span class="line">                match =&gt; [ &quot;time&quot; , &quot;yyyy-MM-dd HH:mm:ss&quot; ]</span><br><span class="line">                locale =&gt; &quot;zh&quot;</span><br><span class="line">                timezone =&gt; &quot;-00:00:00&quot;</span><br><span class="line">                target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    webhdfs &#123;</span><br><span class="line">           workers =&gt; 1</span><br><span class="line">           host =&gt; &quot;namenode&quot;</span><br><span class="line">           standby_host =&gt; &quot;standbynamenode&quot;</span><br><span class="line">           port =&gt; 14000</span><br><span class="line">           user =&gt; &quot;loguser&quot;</span><br><span class="line">           path =&gt; &quot;/Service-Data/%&#123;+YYYY&#125;-%&#123;+MM&#125;-%&#123;+dd&#125;/%&#123;app&#125;/logstash-%&#123;+HH&#125;.log&quot;</span><br><span class="line">           flush_size =&gt; 10000</span><br><span class="line">           idle_flush_time =&gt; 10</span><br><span class="line">           compression =&gt; &quot;gzip&quot;</span><br><span class="line">           retry_interval =&gt; 3</span><br><span class="line">           codec =&gt; &apos;json&apos;   # 解决 写入hdfs文件是json格式，否则内容为 %&#123;message&#125;</span><br><span class="line">       &#125;</span><br><span class="line">  # stdout &#123; codec =&gt; rubydebug &#125; # 打开日志</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于hdfs部分配置，可以在 <a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html" target="_blank" rel="noopener">plugins-outputs-webhdfs</a>  官网找到。</p><h3 id="启动-logstart"><a href="#启动-logstart" class="headerlink" title="启动 logstart"></a>启动 logstart</h3><pre><code>cd /home/mtime/logstash-2.3.1/bin/./logstash -f ../conf/hdfs.conf    # 为前台启动 </code></pre><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><ul><li>新版logstash已经支持webhdfs插件，可以直接安装啦。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./logstash-plugin install logstash-output-webhdfs</span><br></pre></td></tr></table></figure><ul><li>报错处理</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items &#123;:outgoing_count=&gt;160, :exception=&gt;&quot;WebHDFS::IOError&quot;,</span><br></pre></td></tr></table></figure><p>我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。<br>官方提供的例子中用的就是50070端口，一直没有尝试14000端口。</p><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">because this file lease is currently owned by DFSClient</span><br></pre></td></tr></table></figure><p>hadoop 租约问题，后期正常就没有了。<br> 执行recoverLease来释放文件的锁</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log</span><br></pre></td></tr></table></figure><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:message=&gt;&quot;webhdfs write caused an exception: &#123;\&quot;RemoteException\&quot;:&#123;\&quot;message\&quot;:\&quot;Failed to APPEND_FILE</span><br></pre></td></tr></table></figure><p>当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。<br>我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。<br>这个应该也可以通过有话写入hdfs参数来解决。</p><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Max write retries reached. Exception: initialize: name or service not known &#123;:level=&gt;:error&#125;</span><br></pre></td></tr></table></figure><p>losgstash 需要能解析所有 hadoop 集群所有节点的主机名。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> logstash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logstash </tag>
            
            <tag> hdfs </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos容器映射端口限制</title>
      <link href="/mesos-container-port-limit/"/>
      <url>/mesos-container-port-limit/</url>
      <content type="html"><![CDATA[<h2 id="mesos-容器映射端口限制"><a href="#mesos-容器映射端口限制" class="headerlink" title="mesos 容器映射端口限制"></a>mesos 容器映射端口限制</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>  mesos 在启动容器的时候，理念是容器内的端都映射到宿主的随机端口。<br>  在容器的时代，其实这样的理念是很好，当容器多的时候，固定端口肯定是有一定的局限性的。可以通过注册中心、mesos-dns、marathon-lb等服务来找到你要的服务地址和端口。<br>  但是有时候有一些服务需要一些固定端口。比如cadvisor、还有我们自己写的容器，可能会映射一些其他端口。  </p><h3 id="默认端口限制"><a href="#默认端口限制" class="headerlink" title="默认端口限制"></a>默认端口限制</h3><p>  默认mesos的端口也是可以指定的，只是范围比较小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">31000 - 32000</span><br></pre></td></tr></table></figure><p>  marahotn 的json 文件中，你可以写。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;portMappings&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;containerPort&quot;: 80,</span><br><span class="line">    &quot;hostPort&quot;: 31000,  # 一般设置 0 为随机端口，</span><br><span class="line">    &quot;servicePort&quot;: 0,</span><br><span class="line">    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>docker 启动时候就是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                            NAMES</span><br><span class="line">70314cd31714        nginx-nettools:1.13   &quot;nginx -g &apos;daemon ...&quot;   24 minutes ago      Up 24 minutes       443/tcp, 0.0.0.0:31000-&gt;80/tcp   mesos-07a768f1-f635-4517-9b60-4e86bfef658e</span><br></pre></td></tr></table></figure><h3 id="配置mesos"><a href="#配置mesos" class="headerlink" title="配置mesos"></a>配置mesos</h3><p>yum 安装的meoss 添加配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;ports(*):[1024-65534]&quot; &gt; /etc/mesos-slave/resources</span><br></pre></td></tr></table></figure><p>重启 mesos-slave 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart mesos-slave</span><br></pre></td></tr></table></figure><p>二进制安装的mesos 在启动命令中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--resources=ports(*):[1024-65534]</span><br></pre></td></tr></table></figure><p>这样你的端口就是在 1024 - 65524 中间随意指定了。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>marathon json文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;portMappings&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;containerPort&quot;: 80,</span><br><span class="line">    &quot;hostPort&quot;: 8080,</span><br><span class="line">    &quot;servicePort&quot;: 0,</span><br><span class="line">    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>docker 启动时候是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                           NAMES</span><br><span class="line">1235513ee658        nginx-nettools:1.13   &quot;nginx -g &apos;daemon ...&quot;   6 minutes ago       Up 6 minutes        443/tcp, 0.0.0.0:8080-&gt;80/tcp   mesos-655d4923-0d1f-4130-8d61-aab824df3f25-S13.9e0c2cfb-3d07-467f-ac47-08e492703263</span><br></pre></td></tr></table></figure><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>如果mesos上运行过容器，在你修改配置文件之后重启会有问题。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你可以通过</span><br><span class="line">journalctl -xe</span><br><span class="line">或</span><br><span class="line">查看mesos的log日志 找到问题</span><br></pre></td></tr></table></figure><p>解决方法： 日志中会有提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">To remedy this do as follows:</span><br><span class="line">Step 1: rm -f /home/mtime/mesos/meta/slaves/latest</span><br><span class="line">        This ensures agent doesn&apos;t recover old live executors.</span><br><span class="line">  ep 2: Restart the agent.</span><br></pre></td></tr></table></figure><p>rm -f /home/mtime/mesos/meta/slaves/latest<br>删除之后在重启即可。</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
            <tag> port </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon私有仓库用户名和密码方式</title>
      <link href="/marathon-DockerRegistry-user/"/>
      <url>/marathon-DockerRegistry-user/</url>
      <content type="html"><![CDATA[<h1 id="marathon-使用仓库用户名和密码方式"><a href="#marathon-使用仓库用户名和密码方式" class="headerlink" title="marathon 使用仓库用户名和密码方式"></a>marathon 使用仓库用户名和密码方式</h1><h2 id="首先需要本地手动登入镜像仓库。"><a href="#首先需要本地手动登入镜像仓库。" class="headerlink" title="首先需要本地手动登入镜像仓库。"></a>首先需要本地手动登入镜像仓库。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># docker login registry.inc-test.com</span><br><span class="line">   Username: admin </span><br><span class="line">   Password: Default@123</span><br></pre></td></tr></table></figure><p>登入成功之后会在当前用户的家目录创建一个隐藏目录 ~/.docker ，打包这么目录，放在一个目录下， 并让marathon启动容器的时候引用这个文件即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cd ~</span><br><span class="line"># tar czf docker.tar.gz .docker</span><br><span class="line"></span><br><span class="line"># cp docker.tar.gz /etc/</span><br></pre></td></tr></table></figure><h2 id="marathon-json-启动容器引用验证文件"><a href="#marathon-json-启动容器引用验证文件" class="headerlink" title="marathon json 启动容器引用验证文件"></a>marathon json 启动容器引用验证文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;uris&quot;: [</span><br><span class="line">   &quot;file:///etc/docker.tar.gz&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>注释：  </p><ul><li>这样需要每台mesos slave机器都需要放置这个文件，实际操作很不灵活，</li><li>而且用户切换也不好做，每台机器需要放不不用户的验证文件。</li><li>如果用户密码修改，还需要批量修改每台slave机器上的验证文件。</li></ul><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><ul><li>把这个文件放在http页面上，只要网络通就可以访问，不需要每台机器都配置验证文件，修改也比较访问。</li></ul><p>把docker.tar.gz文件放在http页面中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/docker.tar.gz 10.10.130.201:/var/www/html/download/docker_img/harbor-admin.tar.gz</span><br><span class="line"></span><br><span class="line"># 一个用户手动生成一个文件，如需要切换用户的时候指定不同文件即可。</span><br></pre></td></tr></table></figure><h2 id="例如："><a href="#例如：" class="headerlink" title="例如："></a>例如：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;nginx&quot;,</span><br><span class="line">    &quot;cpus&quot;: 0.2,</span><br><span class="line">    &quot;mem&quot;: 128,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">    &quot;constraints&quot;: [</span><br><span class="line">        [</span><br><span class="line">            &quot;hostname&quot;,</span><br><span class="line">            &quot;CLUSTER&quot;,</span><br><span class="line">            &quot;es02.host-test.com&quot;</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    &quot;uris&quot;: [</span><br><span class="line">        &quot;http://10.10.130.201/download/docker_img/harbor-admin.tar.gz&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;registry.inc-test.com/web-lb/nginx:1.13&quot;,</span><br><span class="line">            &quot;network&quot;: &quot;BRIDGE&quot;,</span><br><span class="line">            &quot;portMappings&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 80,</span><br><span class="line">                    &quot;hostPort&quot;: 31009,</span><br><span class="line">                    &quot;servicePort&quot;: 0,</span><br><span class="line">                    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>marathon 官网说明 <a href="https://mesosphere.github.io/marathon/docs/native-docker-private-registry.html" target="_blank" rel="noopener">https://mesosphere.github.io/marathon/docs/native-docker-private-registry.html</a></p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos解决sandbox日志切分问题</title>
      <link href="/mesos-sandbox-split/"/>
      <url>/mesos-sandbox-split/</url>
      <content type="html"><![CDATA[<h1 id="mesos-解决sandbox日志切分问题"><a href="#mesos-解决sandbox日志切分问题" class="headerlink" title="mesos 解决sandbox日志切分问题"></a>mesos 解决sandbox日志切分问题</h1><p>mesos运行的docker容器，容器打印到前台console的日志会记录到mesos的work目录中容器沙箱中stdout和stderr文件中，容器不重启，日志会一直变大，这样会到只宿主空间变大。  </p><p>另外这份日志还会日志到系统的/var/log/messages 文件中。  </p><p>首先关于 mesos-slave 的 work-dir 中设置的目录，里面存放的docker容器的沙箱目录，会有 stderr\stdout等文件，其中这两个文件是记录容器console的日志，会一直保留，直到容器销毁，这样日志文件会持续增大。</p><p>为解决这个问题问题。mesos 没有明确的配置。 <a href="http://mesos.apache.org/documentation/latest/logging/" target="_blank" rel="noopener">http://mesos.apache.org/documentation/latest/logging/</a> 文章中有提到沙箱大小的设置，但是没有测试成功。</p><p>我的解决办法：利用系统的 logrotate 模块做日志的切分和删除。</p><p>如：添加配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/logrotate.d/mesos  &lt;&lt; EOF</span><br><span class="line">/home/mtime/mesos/slaves/*/frameworks/*/executors/*/runs/latest/stderr</span><br><span class="line">/home/mtime/mesos/slaves/*/frameworks/*/executors/*/runs/latest/stdout </span><br><span class="line">&#123;</span><br><span class="line">        daily</span><br><span class="line">        missingok</span><br><span class="line">        copytruncate</span><br><span class="line">        notifempty</span><br><span class="line">        size 102400</span><br><span class="line">        dateext</span><br><span class="line">        rotate 7</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>这样每天都会切分 大于 100Mb的日志了， 并保留7天。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/sbin/logrotate -d -v -f /etc/logrotate.conf</span><br><span class="line"></span><br><span class="line">-d  测试配置文件，不是真正执行。</span><br></pre></td></tr></table></figure><p>crontab  中 已经添加，logrotate 会每天执行的。/etc/cron.daily/logrotate </p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon-lb配置及nginx负载</title>
      <link href="/marathon-lb-configure-nginx/"/>
      <url>/marathon-lb-configure-nginx/</url>
      <content type="html"><![CDATA[<h1 id="marathon-lb配置"><a href="#marathon-lb配置" class="headerlink" title="marathon-lb配置"></a>marathon-lb配置</h1><h2 id="marathon-lb-get-images"><a href="#marathon-lb-get-images" class="headerlink" title="marathon-lb get images"></a>marathon-lb get images</h2><p>Marathon-lb既是一个服务发现工具，也是负载均衡工具，它集成了haproxy，自动获取各个app的信息，为每一组app生成haproxy配置，通过servicePort或者web虚拟主机提供服务。</p><p>要使用marathonn-lb，每组app必须设置HAPROXY_GROUP标签。</p><p>Marathon-lb运行时绑定在各组app定义的服务端口（servicePort，如果app不定义servicePort，marathon会随机分配端口号）上，可以通过marathon-lb所在节点的相关服务端口访问各组app。</p><p>例如：marathon-lb部署在slave5，test-app 部署在slave1，test-app 的servicePort是10004，那么可以在slave5的 10004端口访问到test-app提供的服务。</p><p>由于servicePort 非80、443端口（80、443端口已被marathon-lb中的 haproxy独占），对于web服务来说不太方便，可以使用 haproxy虚拟主机解决这个问题：</p><p>在提供web服务的app配置里增加HAPROXY_{n}_VHOST（WEB虚拟主机）标签，marathon-lb会自动把这组app的WEB集群服务发布在marathon-lb所在节点的80和443端口上，用户设置DNS后通过虚拟主机名来访问。</p><h3 id="官方下载镜像"><a href="#官方下载镜像" class="headerlink" title="官方下载镜像"></a>官方下载镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">images url :</span><br><span class="line">https://store.docker.com/community/images/mesosphere/marathon-lb</span><br><span class="line"></span><br><span class="line">docker pull mesosphere/marathon-lb</span><br><span class="line"></span><br><span class="line">github url:</span><br><span class="line">https://github.com/mesosphere/marathon-lb</span><br></pre></td></tr></table></figure><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --privileged -e PORTS=9090 --net=host docker.io/mesosphere/marathon-lb sse -m http://marathon1_ip:8080 -m http://marathon2_ip:8080 -m http://master3_ip:8080  --group external</span><br></pre></td></tr></table></figure><p>marathon</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vim marathon-lb.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;marathon-lb-testv1&quot;,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">    &quot;constraints&quot;: [</span><br><span class="line">        [</span><br><span class="line">            &quot;hostname&quot;,</span><br><span class="line">            &quot;CLUSTER&quot;,</span><br><span class="line">            &quot;host-hostname.com&quot;</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;docker.io/mesosphere/marathon-lb:latest&quot;,</span><br><span class="line">            &quot;privileged&quot;: true,</span><br><span class="line">            &quot;network&quot;: &quot;HOST&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;args&quot;: [</span><br><span class="line">        &quot;sse&quot;,</span><br><span class="line">        &quot;-m&quot;,</span><br><span class="line">        &quot;http://10.10.131.78:8080&quot;,</span><br><span class="line">        &quot;--group&quot;,</span><br><span class="line">        &quot;external&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">curl -X POST http://10.10.131.78:8080/v2/apps -d @marathon-lb.json -H &quot;Content-type: application/json&quot;</span><br></pre></td></tr></table></figure><h2 id="marathon-lb-API"><a href="#marathon-lb-API" class="headerlink" title="marathon-lb API"></a>marathon-lb API</h2><table><thead><tr><th>Endpoint</th><th>Description</th></tr></thead><tbody><tr><td>:9090/haproxy?stats</td><td>HAProxy stats endpoint. This produces an HTML page which can be viewed in your browser, providing various statistics about the current HAProxy instance.</td></tr><tr><td>:9090/haproxy?stats;csv</td><td>This is a CSV version of the stats above, which can be consumed by other tools. For example, it’s used in the zdd.py script.</td></tr><tr><td>:9090/_haproxy_health_check</td><td>HAProxy health check endpoint. Returns 200 OK if HAProxy is healthy.</td></tr><tr><td>:9090/_haproxy_getconfig</td><td>Returns the HAProxy config file as it was when HAProxy was started. Implemented in getconfig.lua.</td></tr><tr><td>:9090/_haproxy_getvhostmap</td><td>Returns the HAProxy vhost to backend map. This endpoint returns HAProxy map file only when the –haproxy-map flag is enabled, it returns an empty string otherwise. Implemented in getmaps.lua.</td></tr><tr><td>:9090/_haproxy_getappmap</td><td>Returns the HAProxy app ID to backend map. Like _haproxy_getvhostmap, this requires the –haproxy-map flag to be enabled and returns an empty string otherwise. Also implemented in getmaps.lua.</td></tr><tr><td>:9090/_haproxy_getpids</td><td>Returns the PIDs for all HAProxy instances within the current process namespace. This literally returns $(pidof haproxy). Implemented in getpids.lua. This is also used by the zdd.py script to determine if connections have finished draining during a deploy.</td></tr><tr><td>:9090/_mlb_signal/hup*</td><td>Sends a SIGHUP signal to the marathon-lb process, causing it to fetch the running apps from Marathon and reload the HAProxy config as though an event was received from Marathon.</td></tr><tr><td>:9090/_mlb_signal/usr1*</td><td>Sends a SIGUSR1 signal to the marathon-lb process, causing it to restart HAProxy with the existing config, without checking Marathon for changes.</td></tr></tbody></table><ul><li>API from marathon-lb <a href="https://github.com/mesosphere/marathon-lb" target="_blank" rel="noopener">!github</a></li><li>marathon-lb 文档详解 <a href="https://github.com/mesosphere/marathon-lb/blob/master/Longhelp.md#templates" target="_blank" rel="noopener">!https://github.com/mesosphere/marathon-lb/blob/master/Longhelp.md#templates</a></li></ul><p>如常用： <a href="http://marathon-lb-ip:9090/haproxy?stats" target="_blank" rel="noopener">http://marathon-lb-ip:9090/haproxy?stats</a></p><h2 id="nginx-start"><a href="#nginx-start" class="headerlink" title="nginx start"></a>nginx start</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># vim nginx.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;nginx-test&quot;,</span><br><span class="line">    &quot;cpus&quot;: 0.2,</span><br><span class="line">    &quot;mem&quot;: 128,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">  &quot;labels&quot;: &#123;</span><br><span class="line">     &quot;HAPROXY_GROUP&quot;:&quot;external&quot;</span><br><span class="line">     &quot;HAPROXY_0_VHOST&quot;:&quot;nginx.test.com&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">    &quot;uris&quot;: [</span><br><span class="line">        &quot;http://10.10.130.201/download/docker_img/db-harbor-admin.tar.gz&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;healthChecks&quot;: [&#123; &quot;path&quot;: &quot;/&quot; &#125;],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;nginx:1.13&quot;,</span><br><span class="line">            &quot;network&quot;: &quot;BRIDGE&quot;,</span><br><span class="line">            &quot;portMappings&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 80,</span><br><span class="line">                    &quot;hostPort&quot;: 0,</span><br><span class="line">                    &quot;servicePort&quot;: 10000,</span><br><span class="line">                    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># curl -X POST http://10.10.131.78:8080/v2/apps -d @nginx.json -H &quot;Content-type: application/json&quot;</span><br></pre></td></tr></table></figure><h2 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h2><ol><li>一定要加上HAPROXY_GROUP标签，它填写的是marathon-lb创建时定义的组名 </li><li>HAPROXY_0_VHOST是标签名，对于web服务可以加上VHOST标签，让marathon-lb设置WEB虚拟主机；</li><li>containerPort为80,是指容器内的端口。</li><li>hostPort是当前主机映射到contenterPort的端口，如果hostPort为0的话,则说明是随机的。</li><li>serverPort是marathon-lb需要配置的haproxy代理暴露的端口,这里设置为10000，说明访问marathon-lb机器的10000端口就可为访问这个应用容器的80端口。</li></ol><h2 id="访问marathon-lb"><a href="#访问marathon-lb" class="headerlink" title="访问marathon-lb"></a>访问marathon-lb</h2><p>ip 访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://marathon-lb_ip:10000/</span><br></pre></td></tr></table></figure><ul><li>访问marathon-lb部署的宿主机ip地址和serverPort的端口。</li></ul><p>域名访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">需要添加dns解析，根据 &quot;HAPROXY_0_VHOST&quot;:&quot;nginx.test.com&quot; 设置的配置。</span><br><span class="line">如：</span><br><span class="line">vim /etc/hosts  添加</span><br><span class="line">10.10.131.151nginx.test.com</span><br><span class="line"></span><br><span class="line">这里 10.10.131.151 是 marathon-lb 的ip地址</span><br><span class="line"></span><br><span class="line">curl nginx.test.com  即可</span><br></pre></td></tr></table></figure><h2 id="marathon-lb-代理80端口"><a href="#marathon-lb-代理80端口" class="headerlink" title="marathon-lb 代理80端口"></a>marathon-lb 代理80端口</h2><p>默认marathon-lb 80和443端口是被占用的，所以nginx在发布的时候“serverPort”是不能设置为80和443端口的。  </p><p>为了解决这个问题，需要更改源码，重新生成镜像。  </p><p>首先现在 marathon-lb源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># git clone https://github.com/mesosphere/marathon-lb.git</span><br><span class="line"># cd marathon-lb</span><br><span class="line"></span><br><span class="line">在这个目录下找到所有80、443端口信息。改为其他端口</span><br><span class="line"></span><br><span class="line"># grep 80 . -R</span><br><span class="line">找到相应文件，80 替换为7080</span><br><span class="line">:%s/80/7080/g</span><br><span class="line"></span><br><span class="line">找到相应文件，443 替换为7443</span><br><span class="line">:%s/443/7443/g</span><br></pre></td></tr></table></figure><p>重新生成镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t marathon-lb-7080 .</span><br><span class="line"></span><br><span class="line">成功之后 docker images 就会多出 marathon-lb-7080 镜像</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
            <tag> marathon-lb </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos-dns搭建</title>
      <link href="/mesos-dns-installed/"/>
      <url>/mesos-dns-installed/</url>
      <content type="html"><![CDATA[<h2 id="mesos-dns-搭建"><a href="#mesos-dns-搭建" class="headerlink" title="mesos-dns 搭建"></a>mesos-dns 搭建</h2><p>  Mesos-DNS用来支持Mesos集群上的服务发现，使运行在Mesos上的应用和服务可以通过域名服务器来发现彼此。你只要知道一个Mesos数据中心上运行的应用的名字，就可以通过Mesos-DNS查询到该应用的IP和端口号。  </p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>  官方下载mesos-dns镜像没有提供mesos-dns的HTTP接口出来，所以先用二进制搭建，在自己build镜像。  </p><p>  mesos-dns文件下载：<a href="https://github.com/mesosphere/mesos-dns/releases" target="_blank" rel="noopener">!https://github.com/mesosphere/mesos-dns/releases</a></p><p>  下载 mesos-dns-v0.6.0-linux-amd64 一个二进制文件。</p><p>  准备配置文件：config.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;zk&quot;: &quot;zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos&quot;,</span><br><span class="line">  &quot;masters&quot;: [&quot;10.0.0.52:5050&quot;, &quot;10.0.0.53:5050&quot;, &quot;10.0.0.54:5050&quot;],</span><br><span class="line">  &quot;refreshSeconds&quot;: 10,</span><br><span class="line">  &quot;ttl&quot;: 0,</span><br><span class="line">  &quot;domain&quot;: &quot;mesos&quot;,</span><br><span class="line">  &quot;port&quot;: 53,</span><br><span class="line">  &quot;resolvers&quot;: [&quot;10.10.130.5&quot;],</span><br><span class="line">  &quot;timeout&quot;: 5, </span><br><span class="line">  &quot;httpon&quot;: true,</span><br><span class="line">  &quot;dnson&quot;: true,</span><br><span class="line">  &quot;httpport&quot;: 8123,</span><br><span class="line">  &quot;externalon&quot;: true,</span><br><span class="line">  &quot;listener&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">  &quot;SOAMname&quot;: &quot;docker-test.com&quot;,</span><br><span class="line">  &quot;SOARname&quot;: &quot;root.docker-test.com&quot;,</span><br><span class="line">  &quot;SOARefresh&quot;: 10,</span><br><span class="line">  &quot;SOARetry&quot;:   3,</span><br><span class="line">  &quot;SOAExpire&quot;:  86400,</span><br><span class="line">  &quot;SOAMinttl&quot;: 10,</span><br><span class="line">  &quot;IPSources&quot;: [&quot;netinfo&quot;, &quot;mesos&quot;, &quot;host&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动 mesos-dns</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv mesos-dns-v0.6.0-linux-amd64 mesos-dns</span><br><span class="line">chmod +x mesos-dns</span><br><span class="line">./mesos-dns -config=config.json -v=2</span><br></pre></td></tr></table></figure><p>mesos-dns 会启动 53 和 8123 两个端口， 53 为dns端口，8123 为http api端口。  </p><h5 id="HTTP-API-接口"><a href="#HTTP-API-接口" class="headerlink" title="HTTP API 接口"></a>HTTP API 接口</h5><table><thead><tr><th>URL</th><th>说明 </th></tr></thead><tbody><tr><td> <a href="http://10.0.0.49:8123/v1/version" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/version</a></td><td>mesos-dns版本信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/config" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/config</a></td><td>mesos-dns配置信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/hosts/{host}" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/hosts/{host}</a></td><td>该host的IP地址信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/services/{service}" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/services/{service}</a></td><td>该service的host、IP、端口信息</td></tr></tbody></table><p> 例子：</p><pre><code>http://10.0.0.49:8123/v1/hosts/nginxqq-nginx.marathon.slave.mesos</code></pre><p>  分析：marathon.slave.mesos 是固定的，mesos是condig.json中domain定义的，在往前是从节点，marathon是框架，nginx是组，nginxqq是appid </p><pre><code>http://10.0.0.49:8123/v1/services/_nginxqq-nginx._tcp.marathon.slave.mesos  </code></pre><p>  分析： _nginxqq-nginx._tcp.marathon.slave.mesos ， nginxqq容器的ID名，nginx为组名，_tcp.marathon.slave.mesos 为固定的。</p><h5 id="dig-获取mesos-dns信息"><a href="#dig-获取mesos-dns信息" class="headerlink" title="dig 获取mesos-dns信息"></a>dig 获取mesos-dns信息</h5><p>查找app所在节点的IP</p><pre><code>dig nginxqq-nginx.marathon.slave.mesos +short</code></pre><p>查找app服务端口号</p><pre><code>dig SRV _nginxqq-nginx._tcp.marathon.slave.mesos +short </code></pre><ul><li>其中 过得到的主机名 mesos-dns 是可以解析的，就是app所在的物理机。</li></ul><h4 id="docker-images"><a href="#docker-images" class="headerlink" title="docker images"></a>docker images</h4><p>创建 docker file 目录，放入所用的文件</p><pre><code>mkdir dockerfile-mesos-dnscd dockerfile-mesos-dnscp ~/mesos-dns .cp ~/config.json .</code></pre><p>编辑 Dockerfile 文件  </p><p>vim Dockerfile</p><pre><code>FROM centos:6WORKDIR /root/ADD mesos-dns /root/ADD config.json /root/EXPOSE 53 8123CMD [&quot;/root/mesos-dns&quot;, &quot;-config=/root/config.json&quot;, &quot;-v=2&quot;]</code></pre><p>生成镜像</p><pre><code>docker build -t stg-mesos-dns:0.6.0 .</code></pre><p>运行镜像</p><pre><code>docker run  -d --name=stg-mesos-dns --net=host stg-mesos-dns:0.6.0</code></pre><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
