<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>源码编译安装 apache hadoop</title>
      <link href="/2018/12/03/install-apache-hadoop/"/>
      <url>/2018/12/03/install-apache-hadoop/</url>
      <content type="html"><![CDATA[<h1 id="安装-apache-hadoop-2-7-2-集群"><a href="#安装-apache-hadoop-2-7-2-集群" class="headerlink" title="安装 apache hadoop 2.7.2 集群"></a>安装 apache hadoop 2.7.2 集群</h1><p>这是我安装编译hadoop得意个笔记手册，和大家分享一下。</p><p>环境：   </p><p>  系统 Centos 7<br>  java ： 1.8<br>  apache hadoop 版本 ： 2.7.2</p><h5 id="hadoop集群hosts列表"><a href="#hadoop集群hosts列表" class="headerlink" title="hadoop集群hosts列表"></a>hadoop集群hosts列表</h5><table><thead><tr><th>IP</th><th>hostname</th><th>运行服务</th></tr></thead><tbody><tr><td>192.168.77.158</td><td>namenode00.host-shining.com</td><td>namenode、zk、journalnode、standby-resourcemanager，hbase-master、spark-master</td></tr><tr><td>192.168.77.159</td><td>namenode01.host-shining.com</td><td>namenode、zk、journalnode、resourcemanager，hbase-master、jobhistory</td></tr><tr><td>192.168.77.161</td><td>datanode00.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.162</td><td>datanode01.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.163</td><td>datanode02.host-shining.com</td><td>datanode、nodemanager、zk、journalnode</td></tr><tr><td>192.168.77.164</td><td>datanode03.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.165</td><td>datanode04.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.166</td><td>datanode05.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.167</td><td>datanode06.host-shining.com</td><td>datanode、nodemanager、regionserver、spark-work</td></tr><tr><td>192.168.77.168</td><td>datanode07.host-shining.com</td><td>datanode、nodemanager</td></tr><tr><td>192.168.77.169</td><td>datanode08.host-shining.com</td><td>datanode、nodemanager</td></tr><tr><td>192.168.77.170</td><td>datanode09.host-shining.com</td><td>datanode、nodemanager</td></tr></tbody></table><blockquote><p>hosts文件并同步到每台机器上。  </p></blockquote><h5 id="hadoop-client-列表"><a href="#hadoop-client-列表" class="headerlink" title="hadoop client 列表"></a>hadoop client 列表</h5><table><thead><tr><th>IP</th><th>hostname</th></tr></thead><tbody><tr><td>192.168.77.160</td><td>client01.host-shining.com</td></tr></tbody></table><h5 id="安装软件包和lzop软件（每台机器都执行）"><a href="#安装软件包和lzop软件（每台机器都执行）" class="headerlink" title="安装软件包和lzop软件（每台机器都执行）"></a>安装软件包和lzop软件（每台机器都执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install  wget gcc gcc-c++ gcc-g77 autoconf automake zlib* fiex* libxml* ncurses-devel libmcrypt* libtool-ltdl-devel* make cmake bind-utils ntp ntpdate lrzsz rsync gzip unzip vim telnet openssl-devel nscd g++ sysstat ncurses-libs bzip2-devel git lsof expect  </span><br><span class="line"></span><br><span class="line">yum install –y *gcc* ncurses-devel openssl-devel cmake autoconfautomake libtool bzip2-devel g++ autoconf automake libtool cmake zlib1g-dev pkg-config  </span><br><span class="line"></span><br><span class="line">yum -y install  lzo-devel zlib-devel  gcc autoconf automakelibtool lzop subversion</span><br></pre></td></tr></table></figure><h5 id="安装jdk-1-8-（需要每台机器执行）"><a href="#安装jdk-1-8-（需要每台机器执行）" class="headerlink" title="安装jdk 1.8 （需要每台机器执行）"></a>安装jdk 1.8 （需要每台机器执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u45-linux-x64.tar.gz &amp;&amp; mkdir /usr/java/ &amp;&amp; mv  jdk1.8.0_45/ /usr/java/</span><br><span class="line">echo -e &quot;export JAVA_HOME=/usr/java/jdk1.8.0_45\nexport PATH=\$JAVA_HOME/bin:\$JAVA_HOME/jre/bin:\$PATH&quot; &gt;&gt; /etc/profile</span><br></pre></td></tr></table></figure><h5 id="创建hadoop用户并添加无密码登入（每台集群都执行）"><a href="#创建hadoop用户并添加无密码登入（每台集群都执行）" class="headerlink" title="创建hadoop用户并添加无密码登入（每台集群都执行）"></a>创建hadoop用户并添加无密码登入（每台集群都执行）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">useradd hadoop</span><br><span class="line">su - hadoop</span><br><span class="line">echo &quot;hadoop|xiaoaojianghu&quot; | chpasswd</span><br></pre></td></tr></table></figure><p>创建添加key的脚本，每台机器添加完用户之后执行  sh key_add.sh  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">vim key_add.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line"># 创建pub  key文件</span><br><span class="line">expect -c &apos;set timeout -1;</span><br><span class="line">spawn ssh-keygen -t rsa;</span><br><span class="line">expect &quot;Enter file in which to save the key&quot;;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">expect &quot;(empty for no passphrase):&quot;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">expect &quot;Enter same passphrase again:&quot;;</span><br><span class="line">send &quot;\n&quot;;</span><br><span class="line">interact&apos;</span><br><span class="line"># 在hosts文件里找到name和data的主机名</span><br><span class="line">for ip in `cat /etc/hosts | grep -v &quot;^#&quot; |grep -E &quot;name|data&quot;|awk &apos;&#123;print $2&#125;&apos;`</span><br><span class="line">do </span><br><span class="line">    echo $ip </span><br><span class="line">    # add hadoop pub key </span><br><span class="line">    expect -c &apos;set timeout -1;</span><br><span class="line">    spawn ssh-copy-id -i .ssh/id_rsa.pub &apos;$ip&apos;;</span><br><span class="line">    expect &quot;Are you sure you want to continue connecting (yes/no)?&quot;;</span><br><span class="line">    send &quot;yes\n&quot;;</span><br><span class="line">    expect &quot;password:&quot;;</span><br><span class="line">    send &quot;xiaoaojianghu\n&quot;;</span><br><span class="line">    interact&apos;</span><br><span class="line"></span><br><span class="line">done</span><br><span class="line"> </span><br><span class="line">for ip in `cat /etc/hosts | grep -E &quot;name|data&quot;|awk &apos;&#123;print $1&#125;&apos;`;do echo $ip ;ssh-copy-id -i ~/.ssh/id_rsa.pub $ip;done</span><br></pre></td></tr></table></figure><h5 id="datanode机器格式化硬盘-所有datanode节点执行"><a href="#datanode机器格式化硬盘-所有datanode节点执行" class="headerlink" title="datanode机器格式化硬盘  (所有datanode节点执行)"></a>datanode机器格式化硬盘  (所有datanode节点执行)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">vim fdisk.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line">yum install -y expect parted</span><br><span class="line"></span><br><span class="line">#for letter in b c d e f g h i j k l m   #默认列表</span><br><span class="line">for letter in `fdisk -l | grep 4000 | awk &apos;&#123;print $2&#125;&apos; | cut -c 8 | sort`  # 找到4T的硬盘并格式化。</span><br><span class="line">do</span><br><span class="line">    expect -c &apos;set timeout -1;</span><br><span class="line">    spawn parted /dev/sd&apos;$letter&apos;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;mklabel gpt\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;unit GB\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;mkpart primary 0 -1\n&quot;;</span><br><span class="line">    expect &quot;(parted)&quot;;</span><br><span class="line">    send &quot;quit\n&quot;;</span><br><span class="line">    interact&apos;</span><br><span class="line"></span><br><span class="line">    nohup mkfs.ext4 /dev/sd$&#123;letter&#125;1 &gt; sd$letter.out 2&gt;&amp;1 &amp;</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h5 id="mount-挂在硬盘-所有datanode节点执行"><a href="#mount-挂在硬盘-所有datanode节点执行" class="headerlink" title="mount 挂在硬盘  (所有datanode节点执行)"></a>mount 挂在硬盘  (所有datanode节点执行)</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">vim mount.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">source /etc/profile</span><br><span class="line"></span><br><span class="line">blkid | sort | grep -v sdm| awk -F &apos;&quot;&apos; &apos;&#123;print $2&#125;&apos; &gt; /tmp/uuid</span><br><span class="line">echo &quot;/data00</span><br><span class="line">/data01</span><br><span class="line">/data02</span><br><span class="line">/data03</span><br><span class="line">/data04</span><br><span class="line">/data05</span><br><span class="line">/data06</span><br><span class="line">/data07</span><br><span class="line">/data08</span><br><span class="line">/data09</span><br><span class="line">/data10</span><br><span class="line">/data11&quot; &gt; /tmp/dir</span><br><span class="line"></span><br><span class="line">for dir in `cat /tmp/dir`</span><br><span class="line">do</span><br><span class="line">    mkdir -p $dir</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">l=$(cat /tmp/uuid | wc -l)</span><br><span class="line">for ((i=1;i&lt;=$l;i++))</span><br><span class="line">do</span><br><span class="line">    u=$(sed -n &quot;$i&quot;p /tmp/uuid)</span><br><span class="line">    d=$(sed -n &quot;$i&quot;p /tmp/dir)</span><br><span class="line">    mount UUID=$u $d</span><br><span class="line">    cp /etc/fstab /etc/fstab_backup</span><br><span class="line">    echo -e &quot;UUID=$u\t$d\t\text4\tdefaults,noatime,nodiratime\t0 0&quot; &gt;&gt; /etc/fstab   </span><br><span class="line">    # noatime,nodiratim   禁用文件访问时间</span><br><span class="line">done</span><br></pre></td></tr></table></figure><h5 id="下载软件包"><a href="#下载软件包" class="headerlink" title="下载软件包"></a>下载软件包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/home/hadoop/apache-hadoop &amp;&amp; cd /home/hadoop/apache-hadoop</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.7.2/hadoop-2.7.2.tar.gz</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-bin.tar.gz</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hive/stable-2/apache-hive-2.1.0-src.tar.gz</span><br><span class="line">wget http://apache.fayea.com/hbase/stable/hbase-1.1.5-bin.tar.gz</span><br><span class="line">wget http://apache.fayea.com/hbase/stable/hbase-1.1.5-src.tar.gz</span><br><span class="line">wget http://apache.fayea.com/mahout/0.12.2/apache-mahout-distribution-0.12.2.tar.gz</span><br><span class="line">wget http://apache.fayea.com/zookeeper/stable/zookeeper-3.4.8.tar.gz</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/spark/spark-1.6.2/spark-1.6.2.tgz</span><br></pre></td></tr></table></figure><h5 id="安装zookeeper"><a href="#安装zookeeper" class="headerlink" title="安装zookeeper"></a>安装zookeeper</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.8.tar.gz &amp;&amp; ln -s zookeeper-3.4.8 zookeeper</span><br><span class="line">cd /home/hadoop/apache-hadoop/zookeeper &amp;&amp;  mkdir -p var/&#123;data,datalog&#125;</span><br><span class="line">cd conf</span><br><span class="line">echo &quot;JAVA_HOME=/usr/java/jdk1.8.0_45&quot; &gt; java.env</span><br><span class="line">echo &quot;export JAVA_OPTS=\&quot;-Xms1000m -Xmx1000m\&quot;&quot; &gt;&gt; java.env</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">sed -i &apos;/^dataDir=/c dataDir=/home/hadoop/apache-hadoop/zookeeper/var/data&apos; zoo.cfg</span><br><span class="line">echo &quot;dataLogDir=/home/hadoop/apache-hadoop/zookeeper/var/datalog&quot; &gt;&gt; zoo.cfg </span><br><span class="line">echo &quot;maxClientCnxns=300&quot; &gt;&gt; zoo.cfg </span><br><span class="line"></span><br><span class="line">在 zoo.cfg 里添加</span><br><span class="line">server.1=namenode00.host-shining.com:2888:3888</span><br><span class="line">server.2=namenode01.host-shining.com:2888:3888</span><br><span class="line">server.3=datanode00.host-shining.com:2888:3888</span><br><span class="line">server.4=datanode01.host-shining.com:2888:3888</span><br><span class="line">server.5=datanode02.host-shining.com:2888:3888</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /home/hadoop/apache-hadoop/zookeeper/var/data/myid </span><br><span class="line">每台机器安顺序排，这个文件是不一样的。   1、2、3、4、5</span><br></pre></td></tr></table></figure><p>用supervisor守护zookeeper   (supervisor需要重启)  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[program:zookeeper]</span><br><span class="line">command = /home/hadoop/apache-hadoop/zookeeper/bin/zkServer.sh start-foreground</span><br><span class="line">autostart = true</span><br><span class="line">autorestart = true</span><br><span class="line">startsecs = 3</span><br><span class="line">startretries = 3</span><br><span class="line">stopwaitsecs = 5</span><br><span class="line">user = hadoop</span><br><span class="line">redirect_stderr = true</span><br><span class="line">stdout_logfile = /home/shining/logs/supervisor/zookeeper.log</span><br><span class="line">stdout_logfile_maxbytes = 500MB</span><br><span class="line">stdout_logfile_backups = 5</span><br></pre></td></tr></table></figure><h4 id="安装hdfs"><a href="#安装hdfs" class="headerlink" title="安装hdfs"></a>安装hdfs</h4><h6 id="创建目录"><a href="#创建目录" class="headerlink" title="创建目录"></a>创建目录</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop/apache-hadoop</span><br><span class="line">tar -zxvf hadoop-2.7.2.tar.gz &amp;&amp; ln -s hadoop-2.7.2 hadoop</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/jn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/dn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/dfs/nn</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs</span><br><span class="line">mkdir -p /home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs/dn_PORT</span><br><span class="line">chmod -R 755 /home/hadoop/apache-hadoop/hadoop/var/</span><br></pre></td></tr></table></figure><h5 id="开始修改配置文件"><a href="#开始修改配置文件" class="headerlink" title="开始修改配置文件"></a>开始修改配置文件</h5><h6 id="hdfs-site-xml-配置文件"><a href="#hdfs-site-xml-配置文件" class="headerlink" title="hdfs-site.xml 配置文件"></a>hdfs-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.nameservices&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.shininghadoop&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.shininghadoop.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode00.host-shining.com:8020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.rpc-address.shininghadoop.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode01.host-shining.com:8020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.shininghadoop.nn1&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode00.host-shining.com:50070&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address.shininghadoop.nn2&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;namenode01.host-shining.com:50070&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;qjournal://namenode00.host-shining.com:8485;namenode01.host-shining.com:8485;datanode00.host-shining.com:8485;datanode01.host-shining.com:8485;datanode02.host-shining.com:8485/shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/dfs/jn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/data00/data,/data01/data,/data02/data,/data03/data,/data04/data,/data05/data,/data06/data,/data07/data,/data08/data,/data09/data,/data10/data,/data11/data&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/dfs/nn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.client.failover.proxy.provider.shininghadoop&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;sshfence(hadoop)</span><br><span class="line">           shell(bin/true)&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/home/hadoop/.ssh/id_rsa&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60000&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;Default block replication. The actual number of replications can be specified when the file is created. The default is used if replication is not specified in create time&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.handler.count&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;60&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.balance.bandwidthPerSec&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;20971520&lt;/value&gt;</span><br><span class="line">    &lt;final&gt;true&lt;/final&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.block.size&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">    &lt;final&gt;true&lt;/final&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.max.xcievers&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;8192&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions.superusergroup&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.support.append&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.read.shortcircuit&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.domain.socket.path&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/run/hadoop-hdfs/dn_PORT&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.client.read.shortcircuit.skip.checksum&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;dfs.ha.automatic-failover.enabled.appcluster&lt;/name&gt; </span><br><span class="line">  &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="core-site-xml-配置文件"><a href="#core-site-xml-配置文件" class="headerlink" title="core-site.xml 配置文件"></a>core-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;hdfs://shininghadoop&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">   &lt;description&gt;Number of minutes between trash checkpoints.If zero, the trash feature is disabled.&lt;/description&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;namenode00.host-shining.com:2181,namenode01.host-shining.com:2181,datanode00.host-shining.com:2181,datanode01.host-shining.com:2181,datanode02.host-shining.com:2181&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10080&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.native.lib.available&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.DefaultCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzoCodec,</span><br><span class="line">        com.hadoop.compression.lzo.LzopCodec,</span><br><span class="line">        org.apache.hadoop.io.compress.BZip2Codec</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.hadoop.hosts&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property&gt;  </span><br><span class="line">    &lt;name&gt;hadoop.proxyuser.hadoop.groups&lt;/name&gt;  </span><br><span class="line">    &lt;value&gt;*&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.permissions.umask-mode&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;022&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="hadoop-env-sh-配置文件-（去掉了注释部分）"><a href="#hadoop-env-sh-配置文件-（去掉了注释部分）" class="headerlink" title="hadoop-env.sh 配置文件 （去掉了注释部分）"></a>hadoop-env.sh 配置文件 （去掉了注释部分）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">JVM_OPTS=&quot;-server -verbose:gc</span><br><span class="line">  -XX:+PrintGCDateStamps</span><br><span class="line">  -XX:+PrintGCDetails</span><br><span class="line">  -XX:+UseGCLogFileRotation</span><br><span class="line">  -XX:NumberOfGCLogFiles=9</span><br><span class="line">  -XX:GCLogFileSize=20m&quot;</span><br><span class="line">export HADOOP_NAMENODE_OPTS=&quot;-Xmx40g -Xms10g -Xmn4g $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/nn_error_gc.log -Xloggc:$HADOOP_LOG_DIR/nn_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/nn_error.hprof&quot;</span><br><span class="line">export HADOOP_DATANODE_OPTS=&quot;-Xmx4g -Xms512m   $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/dn_error_gc.log -Xloggc:$HADOOP_LOG_DIR/dn_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/dn_error.hprof &quot;</span><br><span class="line">export HADOOP_JOB_HISTORYSERVER_OPTS=&quot;-Xmx4g -Xms2g   $JVM_OPTS -XX:ErrorFile=$HADOOP_LOG_DIR/log_error_gc.log -Xloggc:$HADOOP_LOG_DIR/log_gc.log -XX:HeapDumpPath=$HADOOP_LOG_DIR/log_error.hprof &quot;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-&quot;/etc/hadoop&quot;&#125;</span><br><span class="line">for f in $HADOOP_HOME/contrib/capacity-scheduler/*.jar; do</span><br><span class="line">  if [ &quot;$HADOOP_CLASSPATH&quot; ]; then</span><br><span class="line">    export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$f</span><br><span class="line">  else</span><br><span class="line">    export HADOOP_CLASSPATH=$f</span><br><span class="line">  fi</span><br><span class="line">done</span><br><span class="line">export HADOOP_OPTS=&quot;$HADOOP_OPTS -Djava.net.preferIPv4Stack=true&quot;</span><br><span class="line">export HADOOP_NAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_NAMENODE_OPTS&quot;</span><br><span class="line">export HADOOP_DATANODE_OPTS=&quot;-Dhadoop.security.logger=ERROR,RFAS $HADOOP_DATANODE_OPTS&quot;</span><br><span class="line">export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Dhadoop.security.logger=$&#123;HADOOP_SECURITY_LOGGER:-INFO,RFAS&#125; -Dhdfs.audit.logger=$&#123;HDFS_AUDIT_LOGGER:-INFO,NullAppender&#125; $HADOOP_SECONDARYNAMENODE_OPTS&quot;</span><br><span class="line">export HADOOP_NFS3_OPTS=&quot;$HADOOP_NFS3_OPTS&quot;</span><br><span class="line">export HADOOP_PORTMAP_OPTS=&quot;-Xmx2048m $HADOOP_PORTMAP_OPTS&quot;</span><br><span class="line">export HADOOP_CLIENT_OPTS=&quot;-Xmx2048m $HADOOP_CLIENT_OPTS&quot;</span><br><span class="line">export HADOOP_SECURE_DN_USER=$&#123;HADOOP_SECURE_DN_USER&#125;</span><br><span class="line">export HADOOP_SECURE_DN_LOG_DIR=$&#123;HADOOP_LOG_DIR&#125;/$&#123;HADOOP_HDFS_USER&#125;</span><br><span class="line">export HADOOP_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br><span class="line">export HADOOP_SECURE_DN_PID_DIR=$&#123;HADOOP_PID_DIR&#125;</span><br><span class="line">export HADOOP_IDENT_STRING=$USER</span><br><span class="line">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64:/usr/local/lib/</span><br></pre></td></tr></table></figure><h6 id="mapred-site-xml-配置文件"><a href="#mapred-site-xml-配置文件" class="headerlink" title="mapred-site.xml 配置文件"></a>mapred-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Put site-specific property overrides in this file. --&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.staging-dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:10020&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:19888&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.task.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/task&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;  </span><br><span class="line">  &lt;name&gt;yarn.app.mapreduce.am.resource.mb&lt;/name&gt;  </span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4096&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.map.java.opts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-Xmx3400m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.reduce.java.opts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;-Xmx3400m&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.compress.map.output&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.map.output.compression.codec&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapred.child.env&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;LD_LIBRARY_PATH=/usr/lib64&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxmaps&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;9&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxreduces&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 默认值为一个数据块的大小--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.ubertask.maxbytes&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;67108864&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="yarn-site-xml-配置文件"><a href="#yarn-site-xml-配置文件" class="headerlink" title="yarn-site.xml 配置文件"></a>yarn-site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span><br><span class="line">  you may not use this file except in compliance with the License.</span><br><span class="line">  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"></span><br><span class="line">  Unless required by applicable law or agreed to in writing, software</span><br><span class="line">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line">  See the License for the specific language governing permissions and</span><br><span class="line">  limitations under the License. See accompanying LICENSE file.</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;true&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;shininghadoop-yarn&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;rm1,rm2&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com&lt;/value&gt; </span><br><span class="line">&lt;/property&gt; </span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;   </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;   </span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com:2181,namenode01.host-shining.com:2181,datanode00.host-shining.com:2181,datanode01.host-shining.com:2181,datanode02.host-shining.com:2181&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- &lt;property&gt;</span><br><span class="line"> &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line"> &lt;value&gt;namenode01.host-shining.com&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt; </span><br><span class="line">&lt;property&gt; </span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8031&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8032&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8030&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8033&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode01.host-shining.com:8088&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; --&gt;</span><br><span class="line"></span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;true&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/local-dir&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.log-dirs&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs&lt;/value&gt;</span><br><span class="line"> &lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.aggregation-enable&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;83968&lt;/value&gt;</span><br><span class="line">    &lt;discription&gt;每个节点可用内存,单位MB&lt;/discription&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.resource.cpu-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;18&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">   &lt;description&gt;Where to aggregate logs to.&lt;/description&gt;</span><br><span class="line">   &lt;name&gt;yarn.nodemanager.remote-app-log-dir&lt;/name&gt;</span><br><span class="line">   &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/var/yarn/logs/apps&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.application.classpath&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;$HADOOP_CONF_DIR,</span><br><span class="line">         $HADOOP_COMMON_HOME/share/hadoop/common/*,</span><br><span class="line">         $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,</span><br><span class="line">         $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,</span><br><span class="line">         $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/yarn/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/yarn/lib/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/mapreduce/*,</span><br><span class="line">         $YARN_HOME/share/hadoop/mapreduce/lib/*</span><br><span class="line">  &lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.fair.allocation.file&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hadoop/etc/hadoop/fair-scheduler.xml&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.log.server.url&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;http://namenode01.host-shining.com:19888/jobhistory/logs&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1024&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.maximum-allocation-mb&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;16384&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.minimum-allocation-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.maximum-allocation-vcores&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;6&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;1.8&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.scheduler.fair.preemption&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="yarn-env-sh-配置文件"><a href="#yarn-env-sh-配置文件" class="headerlink" title="yarn-env.sh 配置文件"></a>yarn-env.sh 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_YARN_USER=$&#123;HADOOP_YARN_USER:-yarn&#125;</span><br><span class="line">export YARN_CONF_DIR=&quot;$&#123;YARN_CONF_DIR:-$HADOOP_YARN_HOME/conf&#125;&quot;</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">export YARN_LOG_DIR=$&#123;HADOOP_HOME&#125;/logs</span><br><span class="line">JVM_OPTS=&quot;-server -verbose:gc</span><br><span class="line">  -XX:+PrintGCDateStamps</span><br><span class="line">  -XX:+PrintGCDetails</span><br><span class="line">  -XX:+UseGCLogFileRotation</span><br><span class="line">  -XX:NumberOfGCLogFiles=9</span><br><span class="line">  -XX:GCLogFileSize=256m&quot;</span><br><span class="line">RESOURCEMANAGER_OPTS=&quot;-Xmx30g -Xms5g -Xmn2g $JVM_OPTS  -Xloggc:$YARN_LOG_DIR/rm_gc.log&quot;</span><br><span class="line">NODEMANAGER_OPTS=&quot;-Xmx2048m -Xms1024m -Xmn512m $JVM_OPTS  -Xloggc:$YARN_LOG_DIR/nm_gc.log&quot;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">if [ &quot;$JAVA_HOME&quot; != &quot;&quot; ]; then</span><br><span class="line">  #echo &quot;run java in $JAVA_HOME&quot;</span><br><span class="line">  JAVA_HOME=$JAVA_HOME</span><br><span class="line">fi</span><br><span class="line">  </span><br><span class="line">if [ &quot;$JAVA_HOME&quot; = &quot;&quot; ]; then</span><br><span class="line">  echo &quot;Error: JAVA_HOME is not set.&quot;</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line">JAVA=$JAVA_HOME/bin/java</span><br><span class="line">JAVA_HEAP_MAX=-Xmx4096m </span><br><span class="line">if [ &quot;$YARN_HEAPSIZE&quot; != &quot;&quot; ]; then</span><br><span class="line">  JAVA_HEAP_MAX=&quot;-Xmx&quot;&quot;$YARN_HEAPSIZE&quot;&quot;m&quot;</span><br><span class="line">fi</span><br><span class="line">IFS=</span><br><span class="line">if [ &quot;$YARN_LOG_DIR&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_LOG_DIR=&quot;$HADOOP_YARN_HOME/logs&quot;</span><br><span class="line">fi</span><br><span class="line">if [ &quot;$YARN_LOGFILE&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_LOGFILE=&apos;yarn.log&apos;</span><br><span class="line">fi</span><br><span class="line">if [ &quot;$YARN_POLICYFILE&quot; = &quot;&quot; ]; then</span><br><span class="line">  YARN_POLICYFILE=&quot;hadoop-policy.xml&quot;</span><br><span class="line">fi</span><br><span class="line">unset IFS</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.log.dir=$YARN_LOG_DIR&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.log.dir=$YARN_LOG_DIR&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.log.file=$YARN_LOGFILE&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.log.file=$YARN_LOGFILE&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.home.dir=$YARN_COMMON_HOME&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.id.str=$YARN_IDENT_STRING&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dhadoop.root.logger=$&#123;YARN_ROOT_LOGGER:-INFO,console&#125;&quot;</span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.root.logger=$&#123;YARN_ROOT_LOGGER:-INFO,console&#125;&quot;</span><br><span class="line">if [ &quot;x$JAVA_LIBRARY_PATH&quot; != &quot;x&quot; ]; then</span><br><span class="line">  YARN_OPTS=&quot;$YARN_OPTS -Djava.library.path=$JAVA_LIBRARY_PATH&quot;</span><br><span class="line">fi  </span><br><span class="line">YARN_OPTS=&quot;$YARN_OPTS -Dyarn.policy.file=$YARN_POLICYFILE&quot;</span><br></pre></td></tr></table></figure><h6 id="slave-配置文件"><a href="#slave-配置文件" class="headerlink" title="slave 配置文件"></a>slave 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">datanode00.host-shining.com</span><br><span class="line">datanode01.host-shining.com</span><br><span class="line">datanode02.host-shining.com</span><br><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br><span class="line">datanode07.host-shining.com</span><br><span class="line">datanode08.host-shining.com</span><br><span class="line">datanode09.host-shining.com</span><br></pre></td></tr></table></figure><h5 id="apache-maven-安装"><a href="#apache-maven-安装" class="headerlink" title="apache maven 安装"></a>apache maven 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /home/hadoop</span><br><span class="line">wget http://mirror.bit.edu.cn/apache/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz</span><br><span class="line">tar -zxvf apache-maven-3.3.9-bin.tar.gz</span><br></pre></td></tr></table></figure><h5 id="设置环境变量-（每台机器都需要配置）"><a href="#设置环境变量-（每台机器都需要配置）" class="headerlink" title="设置环境变量 （每台机器都需要配置）"></a>设置环境变量 （每台机器都需要配置）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">export MAVEN_OPTS=&quot;-Xms1024m -Xmx1024m -Xss1m&quot;</span><br><span class="line"></span><br><span class="line">export OOZIE_HOME=/home/hadoop/apache-hadoop/oozie</span><br><span class="line">export MAVEN_HOME=/home/hadoop/apache-maven-3.3.9</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export ZK_HOME=/home/hadoop/apache-hadoop/zookeeper</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HIVE_HOME=/home/hadoop/apache-hadoop/hive</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HDFS_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_HOME&#125;/lib/native </span><br><span class="line">export SCALA_HOME=/home/hadoop/apache-hadoop/scala</span><br><span class="line">export MAHOUT_HOME=/home/hadoop/apache-hadoop/mahout</span><br><span class="line">export MAHOUT_CONF_DIR=$MAHOUT_HOME/conf</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_HOME=/home/hadoop/apache-hadoop/spark</span><br><span class="line"></span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;JAVA_HOME&#125;/jre/bin:$&#123;ZK_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;HBASE_HOME&#125;/bin:$MAHOUT_HOME/bin:$&#123;HIVE_HOME&#125;/bin:$&#123;SCALA_HOME&#125;/bin:$&#123;SPARK_HOME&#125;/bin:$&#123;MAVEN_HOME&#125;/bin:$&#123;OOZIE_HOME&#125;/bin:$PATH</span><br><span class="line">export classpath=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib:$MAHOUT_HOME/lib:$PIG_HOME/lib:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar:$HADOOP_CONF_DIR:$SPARK_HOME/lib:$HBASE_HOME/lib/native/Linux-amd64-64:/usr/local/lib:$HADOOP_HOME/lib/native</span><br><span class="line">export HBASE_LIBRARY_PATH=$&#123;HBASE_LIBRARY_PATH&#125;:$&#123;HBASE_HOME&#125;/lib/native/Linux-amd64-64:/usr/local/lib</span><br><span class="line">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HADOOP_HOME/lib/native/Linux-amd64-64:$HADOOP_HOME/lib/native:/usr/local/lib</span><br></pre></td></tr></table></figure><h5 id="protobuf-安装"><a href="#protobuf-安装" class="headerlink" title="protobuf 安装"></a>protobuf 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf protobuf-2.5.0.tar.gz   （每台机器安装）</span><br><span class="line">cd protobuf-2.5.0</span><br><span class="line">./configure          (root用户执行)</span><br><span class="line">make                 (root用户执行)</span><br><span class="line">make install         (root用户执行)</span><br></pre></td></tr></table></figure><h5 id="编译hdfs源码，-lib库-（编译以前需要安装maven，下载之后解压，设置环境变量即可-不需要没台机器都安装）"><a href="#编译hdfs源码，-lib库-（编译以前需要安装maven，下载之后解压，设置环境变量即可-不需要没台机器都安装）" class="headerlink" title="编译hdfs源码， lib库 （编译以前需要安装maven，下载之后解压，设置环境变量即可,不需要没台机器都安装）"></a>编译hdfs源码， lib库 （编译以前需要安装maven，下载之后解压，设置环境变量即可,不需要没台机器都安装）</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">wget http://apache.fayea.com/hadoop/common/hadoop-2.7.2/hadoop-2.7.2-src.tar.gz</span><br><span class="line">tar -zxvf hadoop-2.7.2-src.tar.gz</span><br><span class="line">cd hadoop-2.7.2-src</span><br><span class="line">mvn package -Pdist,native -DskipTests -Dtar</span><br><span class="line">cp -a hadoop-dist/target/hadoop-2.7.2/lib/native/* ~/apache-hadoop/hadoop/lib/native/</span><br></pre></td></tr></table></figure><h5 id="编译lzo压缩格式"><a href="#编译lzo压缩格式" class="headerlink" title="编译lzo压缩格式"></a>编译lzo压缩格式</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">下载 lzo  https://codeload.github.com/twitter/hadoop-lzo/zip/master</span><br><span class="line">unzip hadoop-lzo-master.zip </span><br><span class="line">cd hadoop-lzo-master</span><br><span class="line">vim pom.xml     （修改hadoop版本）</span><br><span class="line">  &lt;properties&gt;</span><br><span class="line">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;</span><br><span class="line">    &lt;hadoop.current.version&gt;2.7.2&lt;/hadoop.current.version&gt;</span><br><span class="line">    &lt;hadoop.old.version&gt;1.0.4&lt;/hadoop.old.version&gt;</span><br><span class="line">  &lt;/properties&gt;</span><br><span class="line"></span><br><span class="line">export CFLAGS=-m64</span><br><span class="line">export CXXFLAGS=-m64</span><br><span class="line">mvn clean package -Dmaven.test.skip=true</span><br><span class="line">cp target/native/Linux-amd64-64/lib/*  ~/apache-hadoop/hadoop/lib/native/</span><br><span class="line">cp target/hadoop-lzo-0.4.20-SNAPSHOT.jar  ~/apache-hadoop/hadoop/share/hadoop/common/lib/</span><br></pre></td></tr></table></figure><h4 id="安装hbase"><a href="#安装hbase" class="headerlink" title="安装hbase"></a>安装hbase</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf  hbase-1.2.2-bin.tar.gz &amp;&amp; ln -s hbase-1.2.2 hbase</span><br><span class="line">cd hbase/conf</span><br><span class="line">cp $HADOOP_HOME/etc/hadoop/core-site.xml  $HBASE_HOME/conf/</span><br><span class="line">cp $HADOOP_HOME/etc/hadoop/hdfs-site.xml  $HBASE_HOME/conf/</span><br><span class="line">对于Hbase启用LZO</span><br><span class="line">cp -a $HADOOP_HOME/lib/native $HBASE_HOME/lib</span><br></pre></td></tr></table></figure><h5 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h5><h6 id="hbase-site-xml-配置文件"><a href="#hbase-site-xml-配置文件" class="headerlink" title="hbase.site.xml 配置文件"></a>hbase.site.xml 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version=&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;!--</span><br><span class="line">/**</span><br><span class="line"> *</span><br><span class="line"> * Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line"> * or more contributor license agreements.  See the NOTICE file</span><br><span class="line"> * distributed with this work for additional information</span><br><span class="line"> * regarding copyright ownership.  The ASF licenses this file</span><br><span class="line"> * to you under the Apache License, Version 2.0 (the</span><br><span class="line"> * &quot;License&quot;); you may not use this file except in compliance</span><br><span class="line"> * with the License.  You may obtain a copy of the License at</span><br><span class="line"> *</span><br><span class="line"> *     http://www.apache.org/licenses/LICENSE-2.0</span><br><span class="line"> *</span><br><span class="line"> * Unless required by applicable law or agreed to in writing, software</span><br><span class="line"> * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span><br><span class="line"> * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span><br><span class="line"> * See the License for the specific language governing permissions and</span><br><span class="line"> * limitations under the License.</span><br><span class="line"> */</span><br><span class="line">--&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;hdfs://shininghadoop/hbase&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.rest.port&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;60050&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.tmp.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/hbase/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com,namenode01.host-shining.com,datanode00.host-shining.com,datanode01.host-shining.com,datanode02.host-shining.com&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.master&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;namenode00.host-shining.com,namenode01.host-shining.com&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;3&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/home/hadoop/apache-hadoop/zookeeper&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Property from ZooKeeper&apos;sconfigzoo.cfg.The directory where the snapshot is stored.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;2181&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;Property from ZooKeeper&apos;sconfigzoo.cfg.Theport at which the clients will connect.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codecs&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.BZip2Codec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;io.compression.codec.lzo.class&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.hadoop.compression.lzo.LzoCodec&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h6 id="hbase-env-sh-修改配置文件-（取消了注释的内容）"><a href="#hbase-env-sh-修改配置文件-（取消了注释的内容）" class="headerlink" title="hbase-env.sh 修改配置文件 （取消了注释的内容）"></a>hbase-env.sh 修改配置文件 （取消了注释的内容）</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HBASE_HEAPSIZE=4096</span><br><span class="line">export HBASE_OPTS=&quot;-XX:+UseConcMarkSweepGC&quot;</span><br><span class="line">export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xms1024m -Xmx4096m&quot;</span><br><span class="line">export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xms1024m -Xmx4096m&quot;</span><br><span class="line">export HBASE_LOG_DIR=/home/hadoop/apache-hadoop/hbase/logs</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line">export LD_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64</span><br><span class="line">export HBASE_LIBRARY_PATH=$HADOOP_HOME/lib/native:/usr/lib64</span><br><span class="line">export CLASSPATH=$CLASSPATH:$HBASE_LIBRARY_PATH</span><br></pre></td></tr></table></figure><h6 id="regionservers-配置文件修改"><a href="#regionservers-配置文件修改" class="headerlink" title="regionservers 配置文件修改"></a>regionservers 配置文件修改</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br></pre></td></tr></table></figure><h6 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">hbase-env.sh 中   </span><br><span class="line">#export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot;</span><br><span class="line">#export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -XX:PermSize=128m -XX:MaxPermSize=128m&quot;</span><br><span class="line">改为</span><br><span class="line">export HBASE_MASTER_OPTS=&quot;$HBASE_MASTER_OPTS -Xms1024m -Xmx1024m&quot;</span><br><span class="line">export HBASE_REGIONSERVER_OPTS=&quot;$HBASE_REGIONSERVER_OPTS -Xms1024m -Xmx1024m&quot; </span><br><span class="line"></span><br><span class="line">////  jdk 用1.8 的 PermSize MaxPermSize 参数没有，需要用 xmx</span><br></pre></td></tr></table></figure><h4 id="hive安装"><a href="#hive安装" class="headerlink" title="hive安装"></a>hive安装</h4><h6 id="数据库创建与授权"><a href="#数据库创建与授权" class="headerlink" title="数据库创建与授权"></a>数据库创建与授权</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mysql -uroot -p</span><br><span class="line">create database shininghadoop;</span><br><span class="line"></span><br><span class="line">GRANT ALL PRIVILEGES ON shininghadoop.* TO &apos;shininghadoop&apos;@&quot;192.168.77.158&quot; IDENTIFIED BY &apos;shininghadoop&apos;WITH GRANT OPTION;</span><br><span class="line">GRANT ALL PRIVILEGES ON shininghadoop.* TO &apos;shininghadoop&apos;@&quot;namenode00.host-shining.com&quot; IDENTIFIED BY &apos;shininghadoop&apos;WITH GRANT OPTION;</span><br><span class="line">......   兜圈所有hadoop节点的IP地址</span><br><span class="line"></span><br><span class="line">FLUSH PRIVILEGES;</span><br></pre></td></tr></table></figure><h6 id="导入hive元数据库"><a href="#导入hive元数据库" class="headerlink" title="导入hive元数据库"></a>导入hive元数据库</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-2.1.0-bin.tar.gz &amp;&amp; ln -s apache-hive-2.1.0-bin hive</span><br><span class="line">创建源数据</span><br><span class="line">cd $HIVE_HOME/scripts/metastore/upgrade/mysql/</span><br><span class="line">mysql -h数据库地址 -ushininghadoop -p</span><br><span class="line">use shininghadoop;</span><br><span class="line">source hive-schema-2.1.0.mysql.sql;</span><br></pre></td></tr></table></figure><h6 id="替换java-jdbc-jar"><a href="#替换java-jdbc-jar" class="headerlink" title="替换java jdbc jar"></a>替换java jdbc jar</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive 需要 java jar</span><br><span class="line">cd $HIVE_HOME/lib</span><br><span class="line">把 mysql-connector-java-5.1.35.jar 放在这里</span><br><span class="line">ln -s mysql-connector-java-5.1.35.jar mysql-connector-java.jar</span><br></pre></td></tr></table></figure><h6 id="hive-site-xml-配置文件-根据环境配置，线上用的是default文件"><a href="#hive-site-xml-配置文件-根据环境配置，线上用的是default文件" class="headerlink" title="hive-site.xml 配置文件  (根据环境配置，线上用的是default文件)"></a>hive-site.xml 配置文件  (根据环境配置，线上用的是default文件)</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;jdbc:mysql://192.168.53.101:3306/testhadoop&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the URL of the MySQL database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;testhadoop&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;testhadoop&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.autoCreateSchema&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.fixedDatastore&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt; </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;datanucleus.autoStartMechanism&lt;/name&gt; </span><br><span class="line">  &lt;value&gt;SchemaTable&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;  </span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.enabled&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/user/hive/warehouse&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveAuthorizationProvider&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;The Hive client authorization manager class name.</span><br><span class="line">  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveAuthorizationProvider.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.metastore.authorization.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;authorization manager class name to be used in the metastore for authorization.</span><br><span class="line">  The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authenticator.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;hive client authenticator manager class name.</span><br><span class="line">  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.metastore.authenticator.manager&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;authenticator manager class name to be used in the metastore for authentication.</span><br><span class="line">  The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.group.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to some groups whenever a table gets created. </span><br><span class="line">   An example like &quot;groupX,groupY:select;groupZ:create&quot; will grant select privilege to groupX and groupY, </span><br><span class="line">   and grant create privilege to groupZ whenever a new table created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.role.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to some roles whenever a table gets created. </span><br><span class="line">   An example like &quot;roleX,roleY:select;roleZ:create&quot; will grant select privilege to roleX and roleY, </span><br><span class="line">   and grant create privilege to roleZ whenever a new table created.&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.security.authorization.createtable.owner.grants&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;ALL&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;the privileges automatically granted to the owner whenever a table gets created. </span><br><span class="line">   An example like &quot;select,drop&quot; will grant select and drop privilege to the owner of the table&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;hive.auto.convert.join&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h6 id="hive-env-sh"><a href="#hive-env-sh" class="headerlink" title="hive-env.sh"></a>hive-env.sh</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_HEAPSIZE=2048</span><br><span class="line">export HIVE_CONF_DIR=/home/hadoop/apache-hadoop/hive/conf</span><br><span class="line">export HIVE_AUX_JARS_PATH=/home/hadoop/apache-hadoop/hive/lib</span><br><span class="line">export HADOOP_PREFIX=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_LIBEXEC_DIR=/home/hadoop/apache-hadoop/hadoop/libexec</span><br><span class="line">export HADOOP_CONF_DIR=/home/hadoop/apache-hadoop/hadoop/etc/hadoop</span><br><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_HDFS_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_YARN_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apache-hadoop/hadoop</span><br></pre></td></tr></table></figure><h6 id="启动-hive-metastore和-server2"><a href="#启动-hive-metastore和-server2" class="headerlink" title="启动 hive metastore和 server2"></a>启动 hive metastore和 server2</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mkdir $HIVE_HOME/hive-logs</span><br><span class="line">nohup.sh </span><br><span class="line">#!/bin/bash</span><br><span class="line"></span><br><span class="line">nohup  hive --service metastore &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.log 2&gt;&amp;1 &amp; </span><br><span class="line">echo $! &gt; /home/hadoop/apache-hadoop/hive/hive-logs/metastore.pid</span><br><span class="line">nohup hive  --service hiveserver2 &gt; /home/hadoop/apache-hadoop/hive/hive-logs/hiveserver2.log 2&gt;&amp;1 &amp; </span><br><span class="line">echo $! &gt; /home/hadoop/apache-hadoop/hive/hive-logs/hiveserver2.pid</span><br></pre></td></tr></table></figure><h5 id="hadoop启动"><a href="#hadoop启动" class="headerlink" title="hadoop启动"></a>hadoop启动</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 -R /home/hadoop/apache-hadoop</span><br><span class="line">hdfs zkfc -formatZK（格式化zookeeper）</span><br><span class="line">hadoop-daemon.sh start journalnode (启动journalnode)</span><br><span class="line">hdfs namenode -format（格式化namenode metadata）</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">hadoop-daemon.sh start namenode -bootstrapStandby( standy namenode)</span><br><span class="line">如果上面log没有问题</span><br><span class="line">stop-dfs.sh</span><br><span class="line">start-dfs.sh</span><br><span class="line">start-yarn.sh   (yarn的机器上执行)</span><br><span class="line">mr-historyserver-daemon.sh start historyserver  (namenode01 机器上执行)</span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure><h5 id="spark-安装"><a href="#spark-安装" class="headerlink" title="spark 安装"></a>spark 安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://downloads.lightbend.com/scala/2.11.8/scala-2.11.8.tgz    （每台机器安装）</span><br><span class="line">tar -zxvf scala-2.11.8.tgz &amp;&amp; ln -s scala-2.11.8 scala</span><br><span class="line"></span><br><span class="line">wget http://apache.fayea.com/mahout/0.12.2/apache-mahout-distribution-0.12.2.tar.gz</span><br><span class="line">tar -zxvf apache-mahout-distribution-0.12.2.tar.gz </span><br><span class="line">ln -s apache-mahout-distribution-0.12.2 mahout </span><br><span class="line">cp ~/apache-hadoop/hadoop/share/hadoop/common/lib/hadoop-lzo-0.4.20-SNAPSHOT.jar ~/apache-hadoop/mahout/lib/</span><br><span class="line"></span><br><span class="line">wget http://mirrors.hust.edu.cn/apache/spark/spark-1.6.2/spark-1.6.2.tgz</span><br><span class="line">tar -zxvf spark-1.6.2.tgz &amp;&amp; ln -s spark-1.6.2.tgz spark-1.6.2</span><br><span class="line">cd spark/conf</span><br></pre></td></tr></table></figure><h6 id="spark-default-conf-配置文件"><a href="#spark-default-conf-配置文件" class="headerlink" title="spark-default.conf  配置文件"></a>spark-default.conf  配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spark.master                     spark://cd-namenode00.host-shining.com:7077</span><br><span class="line">spark.eventLog.enabled           true</span><br><span class="line">spark.eventLog.dir               hdfs://shininghadoop/spark</span><br><span class="line">spark.serializer                 org.apache.spark.serializer.KryoSerializer</span><br><span class="line">spark.driver.memory              5g</span><br><span class="line">spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=&quot;one two three&quot;</span><br></pre></td></tr></table></figure><h6 id="spark-env-sh-配置文件"><a href="#spark-env-sh-配置文件" class="headerlink" title="spark-env.sh 配置文件"></a>spark-env.sh 配置文件</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">export SPARK_WORKER_MEMORY=5G</span><br><span class="line">export SPARK_MEM=$&#123;SPARK_MEM:-5g&#125;</span><br><span class="line">JAVA_OPTS=&quot;$OUR_JAVA_OPTS&quot;</span><br><span class="line">JAVA_OPTS=&quot;$JAVA_OPTS-Xms$SPARK_MEM -Xmx$SPARK_MEM&quot;</span><br><span class="line">JAVA_OPTS=&quot;$JAVA_OPTS-Djava.library.path=$SPARK_LIBRARY_PATH&quot;</span><br><span class="line">export JAVA_HOME=/usr/java/jdk1.8.0_45</span><br><span class="line">export HADOOP_HOME=/home/hadoop/apache-hadoop/hadoop</span><br><span class="line">export HBASE_HOME=/home/hadoop/apache-hadoop/hbase</span><br><span class="line">export HIVE_HOME=/home/hadoop/apache-hadoop/hive</span><br><span class="line">export HADOOP_MAPRED_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_COMMON_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_HDFS_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_YARN_HOME=$&#123;HADOOP_HOME&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export HDFS_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export YARN_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoop</span><br><span class="line">export SPARK_HOME=/home/hadoop/apache-hadoop/spark</span><br><span class="line">export SCALA_HOME=/home/hadoop/apache-hadoop/scala</span><br><span class="line">export classpath=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib:$MAHOUT_HOME/lib:$PIG_HOME/lib:$HIVE_HOME/lib:$JAVA_HOME/lib/tools.jar:$HADOOP_CONF_DIR:$SPARK_HOME/lib:$&#123;HADOOP_HOME&#125;/lib</span><br><span class="line">export HADOOP_CLASSPATH=$JAVA_HOME/lib/dt.jar:$HBASE_HOME/lib/*:$MAHOUT_HOME/lib/*:$PIG_HOME/lib:$HIVE_HOME/lib/*:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/*:$HADOOP_CONF_DIR:$SPARK_HOME/lib/*:$&#123;HADOOP_HOME&#125;/lib/*:$HADOOP_CLASSPATH:</span><br><span class="line">export LD_LIBRARY_PATH=$JAVA_HOME/jre/lib/$OS_ARCH/server:$&#123;HADOOP_HOME&#125;/c++/Linux-$OS_ARCH-$OS_BIT/lib:/usr/local/lib:/usr/lib:$&#123;PBS_HOME&#125;/lib:/usr/lib64</span><br></pre></td></tr></table></figure><h6 id="slave-节点信息"><a href="#slave-节点信息" class="headerlink" title="slave 节点信息"></a>slave 节点信息</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">datanode03.host-shining.com</span><br><span class="line">datanode04.host-shining.com</span><br><span class="line">datanode05.host-shining.com</span><br><span class="line">datanode06.host-shining.com</span><br></pre></td></tr></table></figure><h6 id="spark-启动"><a href="#spark-启动" class="headerlink" title="spark 启动"></a>spark 启动</h6><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd $SPARK_HOME/sbin</span><br><span class="line">./start-all.sh</span><br></pre></td></tr></table></figure><h5 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h5><p>namenode地址    <a href="http://name00.host-shining.com:50070" target="_blank" rel="noopener">http://name00.host-shining.com:50070</a><br>spark 地址        <a href="http://name00.host-shining.com:8080" target="_blank" rel="noopener">http://name00.host-shining.com:8080</a><br>hbase地址        <a href="http://name00.host-shining.com:16010" target="_blank" rel="noopener">http://name00.host-shining.com:16010</a><br>yarn 地址        <a href="http://name01.host-shining.com:8088" target="_blank" rel="noopener">http://name01.host-shining.com:8088</a><br>jobhistory 地址    <a href="http://name01.host-shining.com:19888" target="_blank" rel="noopener">http://name01.host-shining.com:19888</a>  </p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> hadoop </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>awk输出单引号或双引号</title>
      <link href="/2018/11/29/awk%E8%BE%93%E5%87%BA%E5%8D%95%E5%BC%95%E5%8F%B7%E6%88%96%E5%8F%8C%E5%BC%95%E5%8F%B7/"/>
      <url>/2018/11/29/awk%E8%BE%93%E5%87%BA%E5%8D%95%E5%BC%95%E5%8F%B7%E6%88%96%E5%8F%8C%E5%BC%95%E5%8F%B7/</url>
      <content type="html"><![CDATA[<h2 id="awk输出单引号，双引号"><a href="#awk输出单引号，双引号" class="headerlink" title="awk输出单引号，双引号"></a>awk输出单引号，双引号</h2><p>一个文件内容</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1</span><br><span class="line">1       1.1.1.1</span><br><span class="line">2       2.2.2.2</span><br></pre></td></tr></table></figure><h4 id="单引号"><a href="#单引号" class="headerlink" title="单引号"></a>单引号</h4><p>需要用单引号把第二列引起来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1 | awk &apos;&#123;print &quot;&apos;\&apos;&apos;&quot;$2&quot;&apos;\&apos;&apos;&quot;&#125;&apos;</span><br><span class="line">&apos;1.1.1.1&apos;</span><br><span class="line">&apos;2.2.2.2&apos;</span><br></pre></td></tr></table></figure><p>放大分解awk ，让大家看看  （这样大家应该能看清楚了）  </p><font color="#0099ff" size="5" face="黑体">awk ‘{print “ ‘ \ ‘ ‘ “ $2 “ ‘ \ ‘ ‘ “}’</font>  <h4 id="双引号"><a href="#双引号" class="headerlink" title="双引号"></a>双引号</h4><p>用双引号把第二列引起来</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat 1 | awk &apos;&#123;print &quot;\&quot;&quot;$2&quot;\&quot;&quot;&#125;&apos;</span><br><span class="line">&quot;1.1.1.1&quot;</span><br><span class="line">&quot;2.2.2.2&quot;</span><br></pre></td></tr></table></figure><p>放大分解awk  </p><font color="#0099ff" size="5" face="黑体">awk ‘{print “ \ “ “ $2 “ \ “ “}’</font>  <p>希望对大家有帮助！  </p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> shell </category>
          
      </categories>
      
      
        <tags>
            
            <tag> shell </tag>
            
            <tag> awk </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>nginx-1.10.1添加sticky模块实现基于cookie的负载均衡</title>
      <link href="/2018/11/29/nginx-1-10-1%E6%B7%BB%E5%8A%A0sticky%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8Ecookie%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
      <url>/2018/11/29/nginx-1-10-1%E6%B7%BB%E5%8A%A0sticky%E6%A8%A1%E5%9D%97%E5%AE%9E%E7%8E%B0%E5%9F%BA%E4%BA%8Ecookie%E7%9A%84%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
      <content type="html"><![CDATA[<h1 id="nginx-1-10-1-添加sticky模块实现基于cookie的负载均衡"><a href="#nginx-1-10-1-添加sticky模块实现基于cookie的负载均衡" class="headerlink" title="nginx-1.10.1 添加sticky模块实现基于cookie的负载均衡"></a>nginx-1.10.1 添加sticky模块实现基于cookie的负载均衡</h1><p>在多台后台服务器的环境下，我们为了确保一个客户只和一台服务器通信，我们势必使用长连接。使用什么方式来实现这种连接呢，常见的有使用Nginx 自带的ip_hash来做，我想这绝对不是一个好的办法，如果前端是CDN，或者说一个局域网的客户同时访问服务器，导致出现服务器分配不均衡，以及不能 保证每次访问都粘滞在同一台服务器。如果基于cookie会是一种什么情形，想想看, 每台电脑都会有不同的cookie，在保持长连接的同时还保证了服务器的压力均衡，Nginx sticky值得推荐。  </p><p>如果浏览器不支持cookie，那么sticky不生效，毕竟整个模块是给予cookie实现的.</p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>下载 sticky 模块<br>目前共有2个版本，一个是1.0，一个是1.1，1.0已经寿终正寝了.1.1增加了权重的参数.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">下载地址：http://code.google.com/p/nginx-sticky-module/downloads/list</span><br><span class="line"></span><br><span class="line">或 直接下载</span><br><span class="line"></span><br><span class="line">wget https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/nginx-sticky-module/nginx-sticky-module-1.1.tar.gz</span><br></pre></td></tr></table></figure><h5 id="nginx-1-6-之下版本可以直接安装，1-6之上版本可以修改配置文件"><a href="#nginx-1-6-之下版本可以直接安装，1-6之上版本可以修改配置文件" class="headerlink" title="nginx-1.6 之下版本可以直接安装，1.6之上版本可以修改配置文件"></a>nginx-1.6 之下版本可以直接安装，1.6之上版本可以修改配置文件</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1.6之上nginx添加stickty模块会报：</span><br><span class="line"></span><br><span class="line">ngx_http_sticky_module.c: In function ‘ngx_http_get_sticky_peer’:</span><br><span class="line">/ngx_http_sticky_module.c:333: 警告：赋值时将整数赋给指针，未作类型转换</span><br><span class="line">ake[1]: *** [objs/addon/nginx-sticky-module-1.1/ngx_http_sticky_module.o] 错误 1</span><br></pre></td></tr></table></figure><p>修改如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">修改ngx_http_sticky_misc.c 的281行</span><br><span class="line">原来</span><br><span class="line">digest-&gt;len = ngx_sock_ntop(in, digest-&gt;data, len, 1);</span><br><span class="line">修改为：</span><br><span class="line">digest-&gt;len = ngx_sock_ntop(in, sizeof(struct sockaddr_in), digest-&gt;data, len, 1);</span><br></pre></td></tr></table></figure><p>修改 ngx_http_sticky_module.c文件 （主要是1.9.x或之上版本会出现这问题）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1、 第六行添加：</span><br><span class="line">    #include &lt;nginx.h&gt;</span><br></pre></td></tr></table></figure><p><img src="/images/nginx-sticky-1.jpeg" alt="image"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">2、第340行左右修改（iphp-&gt;rrp.current = iphp-&gt;selected_peer;）为：</span><br><span class="line">    #if defined(nginx_version) &amp;&amp; nginx_version &gt;= 1009000</span><br><span class="line">    iphp-&gt;rrp.current = peer;</span><br><span class="line">    #else</span><br><span class="line">    iphp-&gt;rrp.current = iphp-&gt;selected_peer;</span><br><span class="line">    #endif</span><br></pre></td></tr></table></figure><p><img src="/images/nginx-sticky-2.jpeg" alt="image">  </p><p>nginx 编译安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/home/shining/nginx-1.10.1 --with-http_ssl_module --with-http_gzip_static_module --with-http_stub_status_module --with-pcre --with-http_realip_module --with-http_addition_module --with-http_dav_module --with-stream --with-stream_ssl_module --pid-path=/var/run/nginx.pid --add-module=../nginx-sticky-module-1.1/  </span><br><span class="line">make  </span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>添加cookie负载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">在upstream中添加sticky;</span><br><span class="line">http &#123;</span><br><span class="line">    upstream myproject&#123;</span><br><span class="line">        #添加sticky模块后加入此配置</span><br><span class="line">        sticky;</span><br><span class="line">        #被代理的服务</span><br><span class="line">        server 192.168.1.100:8081;</span><br><span class="line">        server 192.168.1.101:8080;</span><br><span class="line">    &#125;</span><br><span class="line">    #  其他配置都是一样的</span><br></pre></td></tr></table></figure><h2 id="在现有nginx上添加模块"><a href="#在现有nginx上添加模块" class="headerlink" title="在现有nginx上添加模块"></a>在现有nginx上添加模块</h2><p>nginx已经安装好， 只是添加模块的话，只需要重新编译，copy nginx 命令即可。  </p><p>sticky模块修改好之后，<br>先查看原来编译的命令：<br>/home/shining/nginx-1.10.1/sbin/nginx -V  </p><p>拿到编译的命令再重新编译：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/home/shining/nginx-1.10.1 --with-http_ssl_module --with-http_gzip_static_module --with-http_stub_status_module --with-pcre --with-http_realip_module --with-http_addition_module --with-http_dav_module --with-stream --with-stream_ssl_module --pid-path=/var/run/nginx.pid --add-module=../nginx-sticky-module-1.1/  </span><br><span class="line">make   ## 这之后就不有熬执行make install，否则会覆盖其他配置</span><br><span class="line"></span><br><span class="line"># 备份原来的nginx命令</span><br><span class="line">cp /home/shining/nginx-1.10.1/sbin/nginx /home/shining/nginx-1.10.1/sbin/nginx_backup</span><br><span class="line"># 替换新编译好的nginx命令</span><br><span class="line">cp ./objs/nginx /home/shining/nginx-1.10.1/sbin/</span><br></pre></td></tr></table></figure><p>查看编译：<br>/home/shining/nginx-1.10.1/sbin/nginx -t<br>生效<br>/home/shining/nginx-1.10.1/sbin/nginx -s stop<br>/home/shining/nginx-1.10.1/sbin/nginx  </p><h3 id="验证-sticky-模块是否生效"><a href="#验证-sticky-模块是否生效" class="headerlink" title="验证 sticky 模块是否生效"></a>验证 sticky 模块是否生效</h3><p>当配置完sticky策略之后，访问页面的时候再 Cookies 里看到 『route』信息  </p><p>如 : </p><p><img src="/images/nginx-sticky-3.jpeg" alt="images"></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> nginx </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon 约束 constraints 限制</title>
      <link href="/2018/11/28/marathon-%E7%BA%A6%E6%9D%9F-constraints-%E9%99%90%E5%88%B6/"/>
      <url>/2018/11/28/marathon-%E7%BA%A6%E6%9D%9F-constraints-%E9%99%90%E5%88%B6/</url>
      <content type="html"><![CDATA[<h2 id="marathon-约束-Constraints-限制"><a href="#marathon-约束-Constraints-限制" class="headerlink" title="marathon 约束 Constraints 限制"></a>marathon 约束 Constraints 限制</h2><p>Constraints控制在何处运行的应用程序，可以根据constraints属性，控制容器可以在哪个Agent节点上运行。</p><p>（往上看来好多文档写的都很类似，我就写一些不一样的吧。）</p><h3 id="mesos-agent-自定义-constraints-属性"><a href="#mesos-agent-自定义-constraints-属性" class="headerlink" title="mesos agent 自定义 constraints 属性"></a>mesos agent 自定义 constraints 属性</h3><p>rpm 安装的 mesos 可以在 /etc/mesos-slave/attributes 下定义这台agent。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;project:ppp1;IDC:BJ;oam:ops1&quot; &gt; /etc/mesos-slave/attributes</span><br></pre></td></tr></table></figure><p>重启mesos-slave服务即可，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start mesos-slave</span><br></pre></td></tr></table></figure><p>这样这台Agent就有利3个属性。</p><p>分别是：</p><ul><li>project:ppp1</li><li>IDC:BJ</li><li>oam:ops1</li></ul><p>另外其他的Agent机器耶可以有相同的属性，这样就可以变成一组。（容器可以固定在相同属性的Agent机器上）</p><p>源码安装，值需要加上 –attributes 参数即可，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/home/mesos/mesos-slave/sbin/mesos-agent --master=zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --executor_registration_timeout=5mins --work_dir=/home/mesos/mesos --hostname=logstash00 --attributes=project:ppp1;IDC:BJ;oam:ops1</span><br></pre></td></tr></table></figure><h3 id="marathon-使用-constraints"><a href="#marathon-使用-constraints" class="headerlink" title="marathon 使用 constraints"></a>marathon 使用 constraints</h3><p>好了， 现在说说marathon 应该怎么使用 constraints</p><h4 id="容器固定在指定的Agent机器上"><a href="#容器固定在指定的Agent机器上" class="headerlink" title="容器固定在指定的Agent机器上"></a>容器固定在指定的Agent机器上</h4><p>可以通过 hostaname 来指定容器在哪台Agent上运行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123;</span><br><span class="line">    &quot;id&quot;: &quot;sleep-cluster&quot;,</span><br><span class="line">    &quot;cmd&quot;: &quot;sleep 60&quot;,</span><br><span class="line">    &quot;instances&quot;: 3,</span><br><span class="line">    &quot;constraints&quot;: [[&quot;hostname&quot;, &quot;CLUSTER&quot;, &quot;host-name.test.com&quot;]]</span><br><span class="line">  &#125;&apos;</span><br></pre></td></tr></table></figure><h4 id="每个Agent上运行一个容器"><a href="#每个Agent上运行一个容器" class="headerlink" title="每个Agent上运行一个容器"></a>每个Agent上运行一个容器</h4><p>所有应用程序的任务中强制执行属性的唯一性。 确保每个主机上只运行一个应用程序任务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;hostname&quot;,</span><br><span class="line">        &quot;UNIQUE&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><h4 id="利用自定义属性约束"><a href="#利用自定义属性约束" class="headerlink" title="利用自定义属性约束"></a>利用自定义属性约束</h4><p>根据自定义属性来约束在哪些Agent节点上运行容器。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;IDC&quot;,</span><br><span class="line">        &quot;LIKE&quot;,</span><br><span class="line">        &quot;BJ&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><p>这样就可以在有 IDC:BJ 的Agent节点上运行容器了，当然，耶可以写多个约束限制天剑的。</p><h5 id="不在哪个节点上"><a href="#不在哪个节点上" class="headerlink" title="不在哪个节点上"></a>不在哪个节点上</h5><p>指定条件，不在哪个节点运行容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;IDC&quot;,</span><br><span class="line">        &quot;UNLIKE&quot;,</span><br><span class="line">        &quot;BJ&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><h4 id="支持正则"><a href="#支持正则" class="headerlink" title="支持正则"></a>支持正则</h4><p>LIKE 接受一个正则表达式作为参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&quot;constraints&quot;: [</span><br><span class="line">    [</span><br><span class="line">        &quot;project&quot;,</span><br><span class="line">        &quot;LIKE&quot;,</span><br><span class="line">        &quot;ppp[1-3]&quot;</span><br><span class="line">    ]</span><br><span class="line">],</span><br></pre></td></tr></table></figure><p>mesos 可以自定义 constraints 这个就很灵活了，我们在初始化 Agent 节点的时候，就可以定义很多属性来。容器发布的时候我们就很容易约束了。</p><p>官方文档：<a href="https://mesosphere.github.io/marathon/docs/constraints.html" target="_blank" rel="noopener">https://mesosphere.github.io/marathon/docs/constraints.html</a></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker ulimit 配置</title>
      <link href="/2018/11/21/docker-ulimit-%E9%85%8D%E7%BD%AE/"/>
      <url>/2018/11/21/docker-ulimit-%E9%85%8D%E7%BD%AE/</url>
      <content type="html"><![CDATA[<h1 id="docker-ulimit-配置"><a href="#docker-ulimit-配置" class="headerlink" title="docker ulimit 配置"></a>docker ulimit 配置</h1><p>docker  设置 ulimit 方法</p><h3 id="一：通过docker-run-–ulimit-参数设置这个容器的-ulimit-值"><a href="#一：通过docker-run-–ulimit-参数设置这个容器的-ulimit-值" class="headerlink" title="一：通过docker run –ulimit 参数设置这个容器的 ulimit 值"></a>一：通过docker run –ulimit 参数设置这个容器的 ulimit 值</h3><p>如<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --ulimit nofile=1024:1024 --rm debian sh -c &quot;ulimit -n&quot;</span><br></pre></td></tr></table></figure></p><p>官网说明：<br><a href="https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container---ulimit" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/run/#set-ulimits-in-container—ulimit</a> </p><h3 id="二：-修改-docker-服务的-默认设置"><a href="#二：-修改-docker-服务的-默认设置" class="headerlink" title="二： 修改 docker 服务的 默认设置"></a>二： 修改 docker 服务的 默认设置</h3><p>vim /usr/lib/systemd/system/docker.service</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[Service]</span><br><span class="line">LimitNOFILE=1048576</span><br><span class="line">LimitNPROC=1048576</span><br><span class="line">LimitCORE=infinity</span><br></pre></td></tr></table></figure><p>systemctl daemon-reload<br>systemctl restart docker</p><p>文档： <a href="https://blog.csdn.net/signmem/article/details/51365006" target="_blank" rel="noopener">https://blog.csdn.net/signmem/article/details/51365006</a></p><ul><li>注释 设置为 infinity 时，值为 65536 ，如 LimitNOFILE=infinity，ulimit -n 值为 65536 </li></ul><h3 id="三-：-daemon-json-文件中配置"><a href="#三-：-daemon-json-文件中配置" class="headerlink" title="三 ： daemon.json 文件中配置"></a>三 ： daemon.json 文件中配置</h3><p>vim /etc/docker/daemon.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        &quot;default-ulimits&quot;: &#123;</span><br><span class="line">                &quot;nofile&quot;: &#123;</span><br><span class="line">                        &quot;Name&quot;: &quot;nofile&quot;,</span><br><span class="line">                        &quot;Hard&quot;: 64000,</span><br><span class="line">                        &quot;Soft&quot;: 64000</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>systemctl restart docker</p><p>文档 <a href="https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file" target="_blank" rel="noopener">https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-configuration-file</a></p><ul><li>注释： daemon.json  和 docker.service 两个都配置 ，daemon.json 中的配置生效。</li></ul><h4 id="For-Example"><a href="#For-Example" class="headerlink" title="For Example"></a>For Example</h4><ul><li>一个es memlock 的例子</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> ulimit </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Elasticsearch 解决 memory_lock 问题</title>
      <link href="/2018/11/21/ES-%E8%A7%A3%E5%86%B3-memory-lock-%E9%97%AE%E9%A2%98/"/>
      <url>/2018/11/21/ES-%E8%A7%A3%E5%86%B3-memory-lock-%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="Elasticsearch-解决-memory-lock-问题"><a href="#Elasticsearch-解决-memory-lock-问题" class="headerlink" title="Elasticsearch 解决 memory_lock 问题"></a>Elasticsearch 解决 memory_lock 问题</h1><p>我是docker运行的es，数据目录存放到本地磁盘上。最近做优化，想加上 memory_lock 参数，发现有问题。发现很多人有类似的问题。但都没说在docker上怎么解决。我整理一下解决办法，现在看来挺简单的。</p><h3 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h3><p>docker 运行 es 6.5</p><p>run docker container</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>Error Message: Unable to lock JVM Memory: error=12, reason=Cannot allocate memory</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[2018-11-21T07:03:05,384][WARN ][o.e.b.JNANatives         ] [unknown] Unable to lock JVM Memory: error=12, reason=Cannot allocate memory</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] This can result in part of the JVM being swapped out.</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] Increase RLIMIT_MEMLOCK, soft limit: 65536, hard limit: 65536</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] These can be adjusted by modifying /etc/security/limits.conf, for example: </span><br><span class="line">        # allow user &apos;elasticsearch&apos; mlockall</span><br><span class="line">        elasticsearch soft memlock unlimited</span><br><span class="line">        elasticsearch hard memlock unlimited</span><br><span class="line">[2018-11-21T07:03:05,385][WARN ][o.e.b.JNANatives         ] [unknown] If you are logged in interactively, you will have to re-login for the new limits to take effect.</span><br></pre></td></tr></table></figure><p>我们去系统里添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/secruity/limits.conf</span><br><span class="line"></span><br><span class="line">* soft  memlock  unlimited</span><br><span class="line">* hard memlock  unlimited</span><br><span class="line">elasticsearch soft memlock unlimited</span><br><span class="line">elasticsearch hard memlock unlimited</span><br></pre></td></tr></table></figure><p>之后再运行容器，问题依旧存在。<br>在网上找了好多这个问题的文章，有好多说法，我先说我解决的办法</p><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><p>只需要在docker container 运行时加上ulime的设置就可以了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run --rm -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e &quot;bootstrap.memory_lock=true&quot; --ulimit memlock=-1:-1 docker.elastic.co/elasticsearch/elasticsearch:6.5.0</span><br></pre></td></tr></table></figure><p>因为 docker 默认没有加载系统的 ulimit ，所以我们在系统上配置是没有用的，需要传进去。<br>这块可以查看 docker ulimit 的相关文档。</p><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><p>检查 memory_lock 是否生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://172.16.124.110:9200/_nodes?filter_path=**.mlockall</span><br></pre></td></tr></table></figure><p>返回内容为 true  及生效</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;nodes&quot;:&#123;&quot;pSYaXnT1SsStth5Dbww2yA&quot;:&#123;&quot;process&quot;:&#123;&quot;mlockall&quot;:true&#125;&#125;&#125;&#125;</span><br></pre></td></tr></table></figure><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>如果你是二进制安装或源码，可以再系统中修改，  </p><p>添加 elasticsearch 帐号。 </p><p>limits.conf中添加<br>elasticsearch soft memlock unlimited<br>elasticsearch hard memlock unlimited  </p><p>重新登入，重新启动es。  </p><p>还有人说 在 ES_JAVA_OPS 中添加  -Djna.tmpdir=/目录<br>如</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export ES_JAVA_OPTS=&quot;$ES_JAVA_OPTS -Djava.io.tmpdir=/path/to/temp/dir&quot;</span><br><span class="line">./bin/elasticsearch</span><br></pre></td></tr></table></figure><p>二进制安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">systemd:  </span><br><span class="line">/usr/lib/systemd/system/elasticsearch.service set: LimitMEMLOCK=infinity  </span><br><span class="line">SysV:  </span><br><span class="line">/etc/sysconfig/elasticsearch set: MAX_LOCKED_MEMORY=unlimited  </span><br><span class="line">Upstart:  </span><br><span class="line">/etc/default/elasticsearch set: MAX_LOCKED_MEMORY=unlimited  </span><br><span class="line">Then restart Elasticsearch.</span><br></pre></td></tr></table></figure><p>二进制和源码安装方式我没有验证过。来源于网络。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> elasticsearch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> elasticsearch </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux 限制用户命令方法</title>
      <link href="/2018/08/20/linux-%E9%99%90%E5%88%B6%E7%94%A8%E6%88%B7%E5%91%BD%E4%BB%A4%E6%96%B9%E6%B3%95/"/>
      <url>/2018/08/20/linux-%E9%99%90%E5%88%B6%E7%94%A8%E6%88%B7%E5%91%BD%E4%BB%A4%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="linux-限制用户命令方法"><a href="#linux-限制用户命令方法" class="headerlink" title="linux 限制用户命令方法"></a>linux 限制用户命令方法</h2><p>linux 上想限制用户可以执行的命令，可以通过环境变量和安装lshell工具方式。</p><h4 id="环境变量的方式"><a href="#环境变量的方式" class="headerlink" title="环境变量的方式"></a>环境变量的方式</h4><p>脚本放在 /etc/profile.d/ 下，每个用户登入的时候都交脚在这里的环境变量。<br>脚本中判断当前用户，root 用户不受权限。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/profile.d/login.sh</span><br><span class="line"></span><br><span class="line">#!/bin/bash</span><br><span class="line">m=`whoami`</span><br><span class="line"></span><br><span class="line">if [[ &quot;$&#123;m&#125;&quot; != &quot;root&quot; ]];then</span><br><span class="line">    echo -e &quot;\e[01;33m* ** 你目前登录的账户是: \e[01;31m$LOGNAME\e[00m  ** \e[00m&quot;</span><br><span class="line">    echo -e &quot;\e[01;33m* ** 可以登入你申请的主机\e[00m&quot;</span><br><span class="line">    mkdir -p $HOME/bin</span><br><span class="line">    rm -f $HOME/bin/*</span><br><span class="line">    ln -s /bin/ls $HOME/bin</span><br><span class="line">    ln -s /bin/ping $HOME/bin</span><br><span class="line">    ln -s /usr/bin/ssh $HOME/bin/</span><br><span class="line">#    ln -s /usr/bin/ssh-copy-id $HOME/bin/</span><br><span class="line">    ln -s /usr/bin/ssh-keygen $HOME/bin/</span><br><span class="line">    ln -s /usr/bin/expect $HOME/bin/</span><br><span class="line">    ln -s /bin/grep $HOME/bin/</span><br><span class="line">    cat &lt;&lt; EOF &gt; $HOME/.newbash_profile</span><br><span class="line">HISTFILESIZE=500000000</span><br><span class="line">HISTSIZE=99999999</span><br><span class="line">HISTTIMEFORMAT=&quot;%Y/%m/%d_%H:%M:%S :&quot;</span><br><span class="line">PATH=$HOME/bin</span><br><span class="line">#export TMOUT=600</span><br><span class="line">export PATH</span><br><span class="line">EOF</span><br><span class="line">    chown $&#123;m&#125;:$&#123;m&#125; $HOME/.newbash_profile</span><br><span class="line">    exec bash --restricted --noprofile --rcfile $HOME/.newbash_profile</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>将允许用户执行的命令软连到用户家目录的bin下，这样用户只能执行特定的命令。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">exec bash --restricted --noprofile --rcfile $HOME/.newbash_profile </span><br><span class="line"></span><br><span class="line">这句话的意思是使用restricted模式，并且不加载系统默认的profile文件，而加载我们定义的profile文件$HOME/.newbash_profile</span><br><span class="line"></span><br><span class="line">这句话也可以添加到 $HOME/.ssh/authorized_keys，在前面加上语句：</span><br><span class="line">command=&quot;bash --restricted --noprofile --rcfile $HOME/.newbash_profile&quot; ssh-rsa ......</span><br></pre></td></tr></table></figure><h4 id="lshell-工具"><a href="#lshell-工具" class="headerlink" title="lshell 工具"></a>lshell 工具</h4><p>也可以安装 lshell 工具来实现对用户命令的限制， lshell不仅可以限制用户执行的命令，还可以限制用户对目录的限制等。   </p><p>lshell 的github地址：<a href="https://github.com/ghantoos/lshell" target="_blank" rel="noopener">https://github.com/ghantoos/lshell</a></p><p>安装  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install lshell  # 需要 epel 源</span><br></pre></td></tr></table></figure><p>lshell 使用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lshell --help</span><br><span class="line">Usage: lshell [OPTIONS]</span><br><span class="line"> --config &lt;file&gt; : Config file location (default /etc/lshell.conf) #指定配置文件</span><br><span class="line"> --log &lt;dir&gt; : Log files directory #指定日志目录 </span><br><span class="line"> -h, --help : Show this help message #显示帮助信息</span><br><span class="line"> --version : Show version #显示版本信息</span><br></pre></td></tr></table></figure><p>配置文件<br>配置文件分：  </p><ul><li>User configuration</li><li>Group configuration</li><li>Default configuration</li></ul><p>看一个官网的例子，很简单</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># CONFIGURATION START</span><br><span class="line">[global]</span><br><span class="line">logpath         : /var/log/lshell/</span><br><span class="line">loglevel        : 2</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">allowed         : [&apos;ls&apos;,&apos;pwd&apos;]</span><br><span class="line">forbidden       : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;] </span><br><span class="line">warning_counter : 2</span><br><span class="line">timer           : 0</span><br><span class="line">path            : [&apos;/etc&apos;, &apos;/usr&apos;]</span><br><span class="line">env_path        : &apos;:/sbin:/usr/foo&apos;</span><br><span class="line">scp             : 1 # or 0</span><br><span class="line">sftp            : 1 # or 0</span><br><span class="line">overssh         : [&apos;rsync&apos;,&apos;ls&apos;]</span><br><span class="line">aliases         : &#123;&apos;ls&apos;:&apos;ls --color=auto&apos;,&apos;ll&apos;:&apos;ls -l&apos;&#125;</span><br><span class="line"></span><br><span class="line">[grp:users]</span><br><span class="line">warning_counter : 5</span><br><span class="line">overssh         : - [&apos;ls&apos;]</span><br><span class="line"></span><br><span class="line">[foo]</span><br><span class="line">allowed         : &apos;all&apos; - [&apos;su&apos;]</span><br><span class="line">path            : [&apos;/var&apos;, &apos;/usr&apos;] - [&apos;/usr/local&apos;]</span><br><span class="line">home_path       : &apos;/home/users&apos;</span><br><span class="line"></span><br><span class="line">[bar]</span><br><span class="line">allowed         : + [&apos;ping&apos;] - [&apos;ls&apos;] </span><br><span class="line">path            : - [&apos;/usr/local&apos;]</span><br><span class="line">strict          : 1</span><br><span class="line">scpforce        : &apos;/home/bar/uploads/&apos;</span><br><span class="line"># CONFIGURATION END</span><br></pre></td></tr></table></figure><p>我简单使用的一个案例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# cat /etc/lshell.conf </span><br><span class="line"># lshell.py configuration file</span><br><span class="line">#</span><br><span class="line"># $Id: lshell.conf,v 1.27 2010/10/18 19:05:17 ghantoos Exp $</span><br><span class="line"></span><br><span class="line">[global]</span><br><span class="line">##  log directory (default /var/log/lshell/ )</span><br><span class="line">logpath         : /var/log/lshell/</span><br><span class="line">##  set log level to 0, 1, 2, 3 or 4  (0: no logs, 1: least verbose,</span><br><span class="line">##                                                 4: log all commands)</span><br><span class="line">loglevel        : 2</span><br><span class="line">##  configure log file name (default is %u i.e. username.log)</span><br><span class="line">#logfilename     : %y%m%d-%u</span><br><span class="line">#logfilename     : syslog</span><br><span class="line"></span><br><span class="line">##  in case you are using syslog, you can choose your logname</span><br><span class="line">#syslogname      : myapp</span><br><span class="line"></span><br><span class="line">[default]</span><br><span class="line">##  a list of the allowed commands or &apos;all&apos; to allow all commands in user&apos;s PATH</span><br><span class="line">allowed         : [&apos;ls&apos;,&apos;echo&apos;,&apos;cd&apos;,&apos;ll&apos;]</span><br><span class="line"></span><br><span class="line">##  a list of forbidden character or commands</span><br><span class="line">#forbidden       : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;,&apos;`&apos;,&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br><span class="line">forbidden       : [&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br><span class="line"></span><br><span class="line">##  a list of allowed command to use with sudo(8)</span><br><span class="line">#sudo_commands   : [&apos;ls&apos;, &apos;more&apos;]</span><br><span class="line"></span><br><span class="line">##  number of warnings when user enters a forbidden value before getting </span><br><span class="line">##  exited from lshell, set to -1 to disable.</span><br><span class="line">warning_counter : 2</span><br><span class="line"></span><br><span class="line">##  command aliases list (similar to bash’s alias directive)</span><br><span class="line">aliases         : &#123;&apos;ll&apos;:&apos;ls -l&apos;, &apos;vi&apos;:&apos;vim&apos;&#125;</span><br><span class="line"></span><br><span class="line">##  introduction text to print (when entering lshell)</span><br><span class="line">intro           : &quot;线上环境请谨慎执行命令\n执行help或者?\n列出可执行的命令\n执行lpath\n查看允许访问的路径&quot;</span><br><span class="line"></span><br><span class="line">##  configure your promt using %u or %h (default: username)</span><br><span class="line">prompt          : &quot;%u@%h&quot;</span><br><span class="line"></span><br><span class="line">##  a value in seconds for the session timer</span><br><span class="line">timer           : 0</span><br><span class="line"></span><br><span class="line">##  list of path to restrict the user &quot;geographicaly&quot;</span><br><span class="line">#path            : [&apos;/home/bla/&apos;,&apos;/etc&apos;]</span><br><span class="line"></span><br><span class="line">##  set the home folder of your user. If not specified the home_path is set to </span><br><span class="line">##  the $HOME environment variable</span><br><span class="line">#home_path       : &apos;/home/bla/&apos;</span><br><span class="line"></span><br><span class="line">##  update the environment variable $PATH of the user</span><br><span class="line">env_path        : &apos;:/usr/local/bin:/usr/sbin:/bin&apos;</span><br><span class="line"></span><br><span class="line">##  add environment variables</span><br><span class="line">#env_vars        : &#123;&apos;foo&apos;:1, &apos;bar&apos;:&apos;helloworld&apos;&#125;</span><br><span class="line"></span><br><span class="line">##  allow or forbid the use of scp (set to 1 or 0)</span><br><span class="line">#scp             : 1</span><br><span class="line"></span><br><span class="line">## forbid scp upload</span><br><span class="line">#scp_upload       : 0</span><br><span class="line"></span><br><span class="line">## forbid scp download</span><br><span class="line">#scp_download     : 0</span><br><span class="line"></span><br><span class="line">##  allow of forbid the use of sftp (set to 1 or 0)</span><br><span class="line">#sftp            : 1</span><br><span class="line"></span><br><span class="line">##  list of command allowed to execute over ssh (e.g. rsync, rdiff-backup, etc.)</span><br><span class="line">overssh         : [&apos;ls&apos;,&apos;sed&apos;,&apos;cp&apos;,&apos;mkdir&apos;,&apos;date&apos;,&apos;&gt;&apos;,&apos;;&apos;,&apos;&amp;&amp;&apos; ]</span><br><span class="line"></span><br><span class="line">##  logging strictness. If set to 1, any unknown command is considered as </span><br><span class="line">##  forbidden, and user&apos;s warning counter is decreased. If set to 0, command is</span><br><span class="line">##  considered as unknown, and user is only warned (i.e. *** unknown synthax)</span><br><span class="line">#strict          : 1</span><br><span class="line"></span><br><span class="line">##  force files sent through scp to a specific directory</span><br><span class="line">#scpforce        : &apos;/home/bla/uploads/&apos;</span><br><span class="line"></span><br><span class="line">##  history file maximum size </span><br><span class="line">history_size     : 9999</span><br><span class="line"></span><br><span class="line">##  set history file name (default is /home/%u/.lhistory)</span><br><span class="line">#history_file     : &quot;/home/%u/.lshell_history&quot;</span><br><span class="line"></span><br><span class="line">[rd]</span><br><span class="line">allowed : [ &apos;ls&apos;,&apos;cd&apos;,&apos;ll&apos;,&apos;ifconfig&apos;,&apos;less&apos;,&apos;echo&apos;,&apos;ip&apos;,&apos;&gt;&apos;,&apos;date&apos;,&apos;grep&apos;,&apos;cat&apos;,&apos;awk&apos;,&apos;|&apos;,&apos;telnet&apos;,&apos;ps&apos;,&apos;ping&apos;,&apos;netstat&apos;,&apos;more&apos;,&apos;jps&apos;,&apos;free&apos;,&apos;du&apos;,&apos;df&apos;,&apos;top&apos;,&apos;tail&apos;,&apos;sed&apos;,&apos;curl&apos;,&apos;date&apos;,&apos;iostat&apos;,&apos;iotop&apos;,&apos;pwd&apos;,&apos;diff&apos;,&apos;uptime&apos;,&apos;hostname&apos;,&apos;nslookup&apos; ]</span><br><span class="line">home_path : &apos;/home/rd&apos;    # 用户的家目录</span><br><span class="line">env_path : &apos;:/usr/local/bin:/usr/sbin:/sbin:/bin:/usr/local/sbin:/ust/bin&apos;</span><br><span class="line">path : [ &apos;/home/testdir&apos;,&apos;/home/rd&apos; ]  # 允许用户访问的目录</span><br><span class="line"></span><br><span class="line">#forbidden : [&apos;;&apos;, &apos;&amp;&apos;, &apos;|&apos;,&apos;`&apos;,&apos;&gt;&apos;,&apos;&lt;&apos;, &apos;$(&apos;, &apos;$&#123;&apos;]</span><br></pre></td></tr></table></figure><p>修改用户shell为lshell</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">usermod rd -s /usr/bin/lshell</span><br><span class="line">或</span><br><span class="line">chsh -s /usr/bin/lshell rd</span><br><span class="line"></span><br><span class="line">新用户</span><br><span class="line">useradd rd -d /home/rd -s /usr/bin/lshell</span><br></pre></td></tr></table></figure><p>添加组 （方便记录日志）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usermod -aG lshell rd</span><br></pre></td></tr></table></figure><p>记录日志： (我没有使用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">groupadd --system lshell</span><br><span class="line">mkdir /var/log/lshell</span><br><span class="line">chown :lshell /var/log/lshell</span><br><span class="line">chmod 770 /var/log/lshell</span><br></pre></td></tr></table></figure><p>注释： lshell 没有重启， 随时修改配置文件，用户重新登入即可生效。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> command </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>parted 分区方法</title>
      <link href="/2018/08/10/parted-%E5%88%86%E5%8C%BA%E6%96%B9%E6%B3%95/"/>
      <url>/2018/08/10/parted-%E5%88%86%E5%8C%BA%E6%96%B9%E6%B3%95/</url>
      <content type="html"><![CDATA[<h2 id="使用说明"><a href="#使用说明" class="headerlink" title="使用说明"></a>使用说明</h2><p>parted 支持2TB以上的磁盘分区，并且允许调整分区的大小。</p><h2 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h2><h4 id="MBR分区表：（MBR含义：主引导记录）"><a href="#MBR分区表：（MBR含义：主引导记录）" class="headerlink" title="MBR分区表：（MBR含义：主引导记录）"></a>MBR分区表：（MBR含义：主引导记录）</h4><ul><li>所支持的最大卷：2T （T; terabytes,1TB=1024GB）</li><li>对分区的设限：最多4个主分区或3个主分区加一个扩展分区。</li></ul><h4 id="GPT分区表：（GPT含义：GUID分区表）"><a href="#GPT分区表：（GPT含义：GUID分区表）" class="headerlink" title="GPT分区表：（GPT含义：GUID分区表）"></a>GPT分区表：（GPT含义：GUID分区表）</h4><ul><li>支持最大卷：18EB，（E：exabytes,1EB=1024TB）  </li><li>每个磁盘最多支持128个分区</li></ul><h2 id="使用案例"><a href="#使用案例" class="headerlink" title="使用案例"></a>使用案例</h2><h4 id="大于2T的整个磁盘分一个分区"><a href="#大于2T的整个磁盘分一个分区" class="headerlink" title="大于2T的整个磁盘分一个分区"></a>大于2T的整个磁盘分一个分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb</span><br><span class="line">可以输入p打印磁盘信息，查看分区的情况，找到起始和结束位置。</span><br><span class="line"></span><br><span class="line">mklabel gpt</span><br><span class="line">　　设置分区类型为gpt</span><br><span class="line"></span><br><span class="line">mkpart primary 0% 100%</span><br><span class="line">　　primary指分区类型为主分区，0是分区开始位置，100%是分区结束位置。相同的命令为：mkpart primary 0 -1 或者是：mkpart  primary 0  XXX 结束的空间</span><br><span class="line"></span><br><span class="line">print</span><br><span class="line">　　打印当前分区,查看分区设置是否正确</span><br><span class="line">　　</span><br><span class="line">quit</span><br><span class="line">　　退出</span><br><span class="line">　　</span><br><span class="line">mkfs.xfs /dev/sdb1</span><br><span class="line">格式化</span><br></pre></td></tr></table></figure><h4 id="设置lvm分区"><a href="#设置lvm分区" class="headerlink" title="设置lvm分区"></a>设置lvm分区</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">parted /dev/sdb</span><br><span class="line">mklabel gpt</span><br><span class="line">mkpart primary xfs 0G 300G      # 300G的磁盘</span><br><span class="line">mkpart primary xfs 300G 500G    # 200G的磁盘</span><br><span class="line">mkpart primary xfs 500G -0G   # 500G到剩余所有空间的分区</span><br><span class="line">print</span><br><span class="line">set 1 lvm on   # 设置 第一个分区为 lvm 文件系统</span><br><span class="line">print</span><br><span class="line">rm 2       # 删除 2 分区  </span><br><span class="line">quit</span><br><span class="line"></span><br><span class="line">mkfs.xfs /dev/sdb2</span><br><span class="line">mkfs.xfs /dev/sdb3</span><br></pre></td></tr></table></figure><h4 id="批量分区"><a href="#批量分区" class="headerlink" title="批量分区"></a>批量分区</h4><p>自己用的一个批量分区脚本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br><span class="line">yum install -y expect parted</span><br><span class="line"></span><br><span class="line">#for letter in b c d e f g h i j k l m</span><br><span class="line">for letter in `fdisk -l | grep 4000 | awk &apos;&#123;print $2&#125;&apos; | cut -c 8 | sort`</span><br><span class="line">do</span><br><span class="line">expect -c &apos;set timeout -1;</span><br><span class="line">spawn parted /dev/sd&apos;$letter&apos;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;mklabel gpt\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;unit GB\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;mkpart primary 0 -1\n&quot;;</span><br><span class="line">expect &quot;(parted)&quot;;</span><br><span class="line">send &quot;quit\n&quot;;</span><br><span class="line">interact&apos;</span><br><span class="line"></span><br><span class="line">nohup mkfs.xfs /dev/sd$&#123;letter&#125;1 &gt; sd$letter.out 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line">done</span><br></pre></td></tr></table></figure><h4 id="非交互式模式"><a href="#非交互式模式" class="headerlink" title="非交互式模式"></a>非交互式模式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># parted /dev/sdb mklabel gpt </span><br><span class="line"># parted /dev/sdb mkpart primary 0 300G</span><br><span class="line"># parted /dev/sdb mkpart primary 300G 1000G </span><br><span class="line"># parted /dev/sdb mkpart logical 1000G -0G</span><br><span class="line"># parted /dev/sdb p</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> parted </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>源码安装mesos</title>
      <link href="/2018/08/06/%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85mesos/"/>
      <url>/2018/08/06/%E6%BA%90%E7%A0%81%E5%AE%89%E8%A3%85mesos/</url>
      <content type="html"><![CDATA[<h1 id="源码安装mesos"><a href="#源码安装mesos" class="headerlink" title="源码安装mesos"></a>源码安装mesos</h1><p>源码安装mesos</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">wget http://archive.apache.org/dist/mesos/1.3.0/mesos-1.3.0.tar.gz</span><br><span class="line"></span><br><span class="line">需要 mvn 环境 export MAVEN_HOME</span><br><span class="line">yum groupinstall -y &quot;Development Tools&quot;</span><br><span class="line">yum install apr* patch libcurl libcurl-devel path python-devel java-1.7.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel subversion subversion-devel systemtap systemtap-client zlib-devel</span><br><span class="line"></span><br><span class="line">tar -zxvf mesos-1.3.0.tar.gz</span><br><span class="line">cd mesos-1.3.0</span><br><span class="line">./configure --prefix=/home/mesos/mesos-slave</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>官网安装说明 ：   <a href="http://mesos.apache.org/documentation/latest/building/" target="_blank" rel="noopener">http://mesos.apache.org/documentation/latest/building/</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"># Install a few utility tools</span><br><span class="line">$ sudo yum install -y tar wget git</span><br><span class="line"></span><br><span class="line"># Fetch the Apache Maven repo file.</span><br><span class="line">$ sudo wget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo</span><br><span class="line"></span><br><span class="line"># Install the EPEL repo so that we can pull in &apos;libserf-1&apos; as part of our</span><br><span class="line"># subversion install below.</span><br><span class="line">$ sudo yum install -y epel-release</span><br><span class="line"></span><br><span class="line"># &apos;Mesos &gt; 0.21.0&apos; requires &apos;subversion &gt; 1.8&apos; devel package,</span><br><span class="line"># which is not available in the default repositories.</span><br><span class="line"># Create a WANdisco SVN repo file to install the correct version:</span><br><span class="line">$ sudo bash -c &apos;cat &gt; /etc/yum.repos.d/wandisco-svn.repo &lt;&lt;EOF</span><br><span class="line">[WANdiscoSVN]</span><br><span class="line">name=WANdisco SVN Repo 1.9</span><br><span class="line">enabled=1</span><br><span class="line">baseurl=http://opensource.wandisco.com/centos/7/svn-1.9/RPMS/\$basearch/</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=http://opensource.wandisco.com/RPM-GPG-KEY-WANdisco</span><br><span class="line">EOF&apos;</span><br><span class="line"></span><br><span class="line"># Parts of Mesos require systemd in order to operate. However, Mesos</span><br><span class="line"># only supports versions of systemd that contain the &apos;Delegate&apos; flag.</span><br><span class="line"># This flag was first introduced in &apos;systemd version 218&apos;, which is</span><br><span class="line"># lower than the default version installed by centos. Luckily, centos</span><br><span class="line"># 7.1 has a patched &apos;systemd &lt; 218&apos; that contains the &apos;Delegate&apos; flag.</span><br><span class="line"># Explicity update systemd to this patched version.</span><br><span class="line">$ sudo yum update systemd</span><br><span class="line"></span><br><span class="line"># Install essential development tools.</span><br><span class="line">$ sudo yum groupinstall -y &quot;Development Tools&quot;</span><br><span class="line"></span><br><span class="line"># Install other Mesos dependencies.</span><br><span class="line">$ sudo yum install -y apache-maven python-devel python-six python-virtualenv java-1.8.0-openjdk-devel zlib-devel libcurl-devel openssl-devel cyrus-sasl-devel cyrus-sasl-md5 apr-devel subversion-devel apr-util-devel</span><br><span class="line"></span><br><span class="line"># Change working directory.</span><br><span class="line">$ cd mesos</span><br><span class="line"></span><br><span class="line"># Bootstrap (Only required if building from git repository).</span><br><span class="line">$ ./bootstrap</span><br><span class="line"></span><br><span class="line"># Configure and build.</span><br><span class="line">$ mkdir build</span><br><span class="line">$ cd build</span><br><span class="line">$ ../configure</span><br><span class="line">$ make</span><br><span class="line"></span><br><span class="line"># Run test suite.</span><br><span class="line">$ make check</span><br><span class="line"></span><br><span class="line"># Install (Optional).</span><br><span class="line">$ make install</span><br></pre></td></tr></table></figure><p>用super启动mesos-agent</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@logstash00 conf]# cat mesos-agent.ini </span><br><span class="line">[program:mesos-agent]</span><br><span class="line">command = /home/mesos/mesos-slave/sbin/mesos-agent --master=zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos --log_dir=/var/log/mesos --containerizers=docker,mesos --executor_registration_timeout=5mins --work_dir=/home/mesos/mesos --hostname=logstash00 --attributes=nginx:ok;service:nginx;test:ok;app:nginx;db:ok;server:nginx</span><br><span class="line">autostart = true</span><br><span class="line">autorestart = true</span><br><span class="line">startsecs = 3</span><br><span class="line">startretries = 3</span><br><span class="line">stopwaitsecs = 5</span><br><span class="line">user = root</span><br><span class="line">redirect_stderr = true</span><br><span class="line">stdout_logfile = /home/mesos/logs/supervisor/mesos-agent.log</span><br><span class="line">stdout_logfile_maxbytes = 500MB</span><br><span class="line">stdout_logfile_backups = 3</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>logstash 配置文件写法</title>
      <link href="/2018/08/06/logstash-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%86%99%E6%B3%95/"/>
      <url>/2018/08/06/logstash-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E5%86%99%E6%B3%95/</url>
      <content type="html"><![CDATA[<h1 id="logstash-配置文件"><a href="#logstash-配置文件" class="headerlink" title="logstash 配置文件"></a>logstash 配置文件</h1><h5 id="开启http接口，并把收集到的日志放入ES中。"><a href="#开启http接口，并把收集到的日志放入ES中。" class="headerlink" title="开启http接口，并把收集到的日志放入ES中。"></a>开启http接口，并把收集到的日志放入ES中。</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    http &#123;</span><br><span class="line">         host =&gt; &quot;0.0.0.0&quot;</span><br><span class="line">         port =&gt; 7881   # 开启端口</span><br><span class="line">         codec =&gt; json  # 格式化 json</span><br><span class="line">          add_field =&gt; &#123;   # 添加字段，在接受到的每条日志中添加 marathon：base-marathon 一个字段</span><br><span class="line">            &quot;marathon&quot; =&gt; &quot;base-marathon&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">##  判断日志中包含 oam_type 的key 放到相应的ES索引中。192.168.5</span><br><span class="line">output &#123;</span><br><span class="line">     if [oam_type] == &quot;hadoop&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-hadoop-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else if [oam_type] == &quot;kafka&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-kafka-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else if [oam_type] == &quot;es&quot; &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-es-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">     &#125; else &#123;</span><br><span class="line">         elasticsearch &#123;</span><br><span class="line">              hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">              index =&gt; &quot;logstash-cd-marathon-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">              flush_size =&gt; 10000</span><br><span class="line">              idle_flush_time =&gt; 60</span><br><span class="line">              template_overwrite =&gt; true</span><br><span class="line">          &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="从kafka中取出nginx日志，放入到HDFS上"><a href="#从kafka中取出nginx日志，放入到HDFS上" class="headerlink" title="从kafka中取出nginx日志，放入到HDFS上"></a>从kafka中取出nginx日志，放入到HDFS上</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;10.10.110.122:2181,10.10.110.123:2181,10.10.110.124:2181/kafka&quot;</span><br><span class="line">        group_id =&gt; &quot;logstash-kafka-hdfs&quot;</span><br><span class="line">        topic_id =&gt; &quot;prd_nginx_access&quot;</span><br><span class="line">        codec =&gt; plain</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 格式化，将十二个月转换成数字。</span><br><span class="line">filter &#123;</span><br><span class="line">        grok &#123;</span><br><span class="line">                match =&gt; &#123;</span><br><span class="line">                        &quot;message&quot; =&gt;&quot;^(?&lt;hostname&gt;.+?)\s(?&lt;modulname&gt;.+?)\s(?&lt;remote_addr&gt;.+?)\s\-\s(?&lt;remote_user&gt;.+?)\s\[(?&lt;Day&gt;.+?)/(?&lt;Month&gt;.+?)/(?&lt;Year&gt;.+?):(?&lt;Hour&gt;.+?):&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        if [Month] == &quot;Jan&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;01&quot;]</span><br><span class="line">                        &#125; </span><br><span class="line">        &#125; else if [Month] == &quot;Feb&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;02&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Mar&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;03&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Apr&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;04&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;May&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;05&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Jun&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;06&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Jul&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;07&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Aug&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;08&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Sep&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;09&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Oct&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;10&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Nov&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;11&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; else if [Month] == &quot;Dec&quot; &#123;</span><br><span class="line">                        mutate &#123;</span><br><span class="line">                                update =&gt; [&quot;Month&quot;,&quot;12&quot;]</span><br><span class="line">                        &#125;</span><br><span class="line">        &#125; </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        if [modulname] &#123;</span><br><span class="line">           webhdfs &#123;</span><br><span class="line">                workers =&gt; 1</span><br><span class="line">                host =&gt; &quot;namenode-master.host.com&quot;</span><br><span class="line">                port =&gt; 14000</span><br><span class="line">                user =&gt; &quot;hadoop&quot;</span><br><span class="line">                path =&gt; &quot;/Data/Logs/domain=%&#123;modulname&#125;/dt=%&#123;Year&#125;%&#123;Month&#125;%&#123;Day&#125;/hour=%&#123;Hour&#125;/%&#123;modulname&#125;_%&#123;Year&#125;%&#123;Month&#125;%&#123;Day&#125;%&#123;Hour&#125;.log&quot;</span><br><span class="line">                flush_size =&gt; 5000</span><br><span class="line">                compression =&gt; &quot;gzip&quot;</span><br><span class="line">                idle_flush_time =&gt; 6</span><br><span class="line">                retry_interval =&gt; 3</span><br><span class="line">                retry_times =&gt; 3</span><br><span class="line">                codec =&gt; line &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">           file &#123;</span><br><span class="line">                path =&gt; &quot;/home/logs/supervisor/logstash_prd_kafka_hdfs_error.log&quot;</span><br><span class="line">                codec =&gt;  line &#123; format =&gt; &quot;custom format: %&#123;message&#125;&quot; &#125;</span><br><span class="line">           &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="收集nginx日志放到kafka中"><a href="#收集nginx日志放到kafka中" class="headerlink" title="收集nginx日志放到kafka中"></a>收集nginx日志放到kafka中</h5><p> 日志格式为文本, logstash 放到kafka中会变成一个大的json串</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/nginx/logs/accesslog/**/*.log&quot;]</span><br><span class="line">        exclude =&gt; [&quot;/home/nginx/logs/accesslog/11.test.com/*.log&quot;,&quot;/home/nginx/logs/accesslog/2.test.com/*.log&quot;,&quot;/home/nginx/logs/accesslog/3.test.com/*.log&quot;]</span><br><span class="line">        sincedb_path =&gt; &quot;/home/optools/logstash/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        discover_interval =&gt; 10</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 5</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        bootstrap_servers =&gt; &quot;kafka00:9092,kafka01:9092,kafka02:9092&quot;</span><br><span class="line">        topic_id =&gt; &quot;prd_nginx_access&quot;</span><br><span class="line">        compression_type =&gt; &quot;gzip&quot;</span><br><span class="line">        codec =&gt; plain &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="处理json日志"><a href="#处理json日志" class="headerlink" title="处理json日志"></a>处理json日志</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/logs/v4-weblog/*.log&quot;]</span><br><span class="line">        sincedb_path =&gt; &quot;/home/logstash/conf/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        codec =&gt; &quot;json&quot;  # 往后端传是json，如果后端要文本，codec =&gt; &quot;plain&quot;</span><br><span class="line">        discover_interval =&gt; 10</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 5</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        kafka &#123;</span><br><span class="line">        topic_id =&gt; &quot;weblogv4_mx_wandafilm&quot;</span><br><span class="line">        bootstrap_servers =&gt; &quot;192.168.5.30:9092,192.168.5.38:9092,192.168.5.48:9092&quot;</span><br><span class="line">        codec =&gt; plain &#123;</span><br><span class="line">                        format =&gt; &quot;%&#123;message&#125;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">     &#125;</span><br><span class="line">    </span><br><span class="line">    #stdout&#123;</span><br><span class="line">    #    codec =&gt; rubydebug</span><br><span class="line">    #&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.18:2181,192.168.5.28:2181,192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;huawei_hard_monitor&quot;</span><br><span class="line">        topic_id =&gt; &quot;huawei_hard_monitor&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        grok &#123;</span><br><span class="line">                match =&gt; &#123;</span><br><span class="line">                        &quot;SNMPv2-SMI::enterprises.2011.23.2.1&quot; =&gt; &quot;^Location:(?&lt;Location&gt;.*?); Time:(?&lt;Time&gt;.*?); Sensor:(?&lt;Sensor&gt;.*?); Severity:(?&lt;Severity&gt;.*?); Code:(?&lt;Code&gt;.*?); Description:(?&lt;Description&gt;.*?)$&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        mutate &#123;</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-MIB::sysUpTime.0&quot;, &quot;SNMPv2-MIB--sysUpTime-0&quot;]</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-MIB::snmpTrapOID.0&quot;, &quot;SNMPv2-MIB--snmpTrapOID-0&quot;]</span><br><span class="line">                rename =&gt; [&quot;SNMPv2-SMI::enterprises.2011.23.2.1&quot;, &quot;SNMPv2-SMI--enterprises_2011_23_2_1&quot;]</span><br><span class="line">         &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">           elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-huawei_hard_monitor-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 50000</span><br><span class="line">                idle_flush_time =&gt; 30</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">            &#125;</span><br><span class="line">#        stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="nginx-日志收集"><a href="#nginx-日志收集" class="headerlink" title="nginx 日志收集"></a>nginx 日志收集</h5><p>中文转码，\x 转为Xx \\x 转为 XXx<br>添加字段，nginx access 和 error 日志放在不同索引中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181,192.168.5.18:2181,192.168.5.28:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;logstash-docker-nginx&quot;</span><br><span class="line">        topic_id =&gt; &quot;test_for_docker&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 4  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        ruby &#123;</span><br><span class="line">                code =&gt; &quot;</span><br><span class="line">                  event[&apos;log&apos;] = event[&apos;log&apos;].gsub(&apos;\x&apos;,&apos;Xx&apos;)</span><br><span class="line">                  event[&apos;log&apos;] = event[&apos;log&apos;].gsub(&apos;\\x&apos;,&apos;XXx&apos;)</span><br><span class="line">                  &quot;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        if &quot;http_cookie&quot; in [log] &#123;</span><br><span class="line">                mutate &#123; add_tag =&gt; &quot;nginx-access&quot; &#125;</span><br><span class="line">                json &#123;</span><br><span class="line">                         source =&gt; &quot;log&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                mutate &#123;</span><br><span class="line">                        convert =&gt; [</span><br><span class="line">                        &quot;status&quot;, &quot;integer&quot;,</span><br><span class="line">                        &quot;body_bytes_sent&quot; , &quot;integer&quot;,</span><br><span class="line">                        &quot;upstream_response_time&quot;, &quot;float&quot;,</span><br><span class="line">                        &quot;request_time&quot;, &quot;float&quot;</span><br><span class="line">                        ]</span><br><span class="line">                        remove_field =&gt; &quot;log&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                geoip &#123;</span><br><span class="line">                        source =&gt; &quot;ip&quot;</span><br><span class="line">                &#125;</span><br><span class="line">                date &#123;</span><br><span class="line">                        match =&gt; [&quot;time_local&quot;, &quot;ISO8601&quot;]</span><br><span class="line">                        locale =&gt;&quot;en&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">                 mutate &#123; add_tag =&gt; &quot;nginx-error&quot; &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">        if &quot;nginx-access&quot;  in [tags] &#123;</span><br><span class="line">           elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-nginxaccess-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 10240</span><br><span class="line">                idle_flush_time =&gt; 30</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">                &#125;</span><br><span class="line">         &#125; else if &quot;nginx-error&quot; in [tags] &#123;</span><br><span class="line">             elasticsearch &#123;</span><br><span class="line">                workers =&gt; 4</span><br><span class="line">                hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">                index =&gt; &quot;logstash-nginxerror-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">                flush_size =&gt; 100</span><br><span class="line">                idle_flush_time =&gt; 5</span><br><span class="line">                template_overwrite =&gt; true</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">#               stdout&#123;codec =&gt; rubydebug&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="转换时间"><a href="#转换时间" class="headerlink" title="转换时间"></a>转换时间</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        zk_connect =&gt; &quot;192.168.5.18:2181,192.168.5.28:2181,192.168.5.30:2181,192.168.5.38:2181,192.168.5.48:2181&quot;</span><br><span class="line">        group_id =&gt; &quot;es-hdfs&quot;</span><br><span class="line">        topic_id =&gt; &quot;logdata-es&quot;</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        reset_beginning =&gt; false # boolean (optional)， default: false</span><br><span class="line">        consumer_threads =&gt; 1  # number (optional)， default: 1</span><br><span class="line">        decorate_events =&gt; false # boolean (optional)， default: false</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">        date &#123;</span><br><span class="line">                match =&gt; [ &quot;time&quot; , &quot;yyyy-MM-dd HH:mm:ss&quot; ]</span><br><span class="line">                locale =&gt; &quot;zh&quot;</span><br><span class="line">                timezone =&gt; &quot;-00:00:00&quot;</span><br><span class="line">                target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">     elasticsearch &#123;</span><br><span class="line">        hosts =&gt; [&quot;192.168.5.27:9250&quot;,&quot;192.168.5.28:9250&quot;,&quot;192.168.5.29:9250&quot;]</span><br><span class="line">        index =&gt; &quot;logstash-%&#123;app&#125;-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">        #document_type =&gt; &quot;%&#123;type&#125;&quot;</span><br><span class="line">        flush_size =&gt; 3840</span><br><span class="line">        idle_flush_time =&gt; 10</span><br><span class="line">        template_overwrite =&gt; true</span><br><span class="line">    &#125;</span><br><span class="line">#  stdout &#123; codec =&gt; rubydebug &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="一个nginx-json-日志收集的例子"><a href="#一个nginx-json-日志收集的例子" class="headerlink" title="一个nginx json 日志收集的例子"></a>一个nginx json 日志收集的例子</h4><p>nginx 配置日志格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">log_format main   &apos;&#123;&quot;@timestamp&quot;:&quot;$time_iso8601&quot;,&apos;</span><br><span class="line">                    &apos;&quot;@source&quot;:&quot;$server_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;hostname&quot;:&quot;$hostname&quot;,&apos;</span><br><span class="line">                    &apos;&quot;remote_user&quot;:&quot;$remote_user&quot;,&apos;</span><br><span class="line">                    &apos;&quot;ip&quot;:&quot;$http_x_forwarded_for&quot;,&apos;</span><br><span class="line">                    &apos;&quot;client&quot;:&quot;$remote_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_method&quot;:&quot;$request_method&quot;,&apos;</span><br><span class="line">                    &apos;&quot;scheme&quot;:&quot;$scheme&quot;,&apos;</span><br><span class="line">                    &apos;&quot;domain&quot;:&quot;$server_name&quot;,&apos;</span><br><span class="line">                    &apos;&quot;referer&quot;:&quot;$http_referer&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request&quot;:&quot;$request_uri&quot;,&apos;</span><br><span class="line">                    &apos;&quot;requesturl&quot;:&quot;$request&quot;,&apos;</span><br><span class="line">                    &apos;&quot;args&quot;:&quot;$args&quot;,&apos;</span><br><span class="line">                    &apos;&quot;size&quot;:$body_bytes_sent,&apos;</span><br><span class="line">                    &apos;&quot;status&quot;: $status,&apos;</span><br><span class="line">                    &apos;&quot;responsetime&quot;:$request_time,&apos;</span><br><span class="line">                    &apos;&quot;upstreamtime&quot;:&quot;$upstream_response_time&quot;,&apos;</span><br><span class="line">                    &apos;&quot;upstreamaddr&quot;:&quot;$upstream_addr&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_user_agent&quot;:&quot;$http_user_agent&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_cookie&quot;:&quot;$http_cookie&quot;,&apos;</span><br><span class="line">                    &apos;&quot;https&quot;:&quot;$https&quot;,&apos;</span><br><span class="line">                    &apos;&quot;request_body&quot;:&quot;$request_body&quot;,&apos;</span><br><span class="line">                    &apos;&quot;http_x_clientid&quot;:&quot;$http_x_clientid&quot;&apos;</span><br><span class="line">                    &apos;&#125;&apos;;</span><br></pre></td></tr></table></figure><p>logstash 手机nginx日志，并处理转码问题。（需要2.4版本之上，才能支持这个正则）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    file &#123;</span><br><span class="line">        path =&gt; [&quot;/home/nginx/logs/accesslog/**/*.log&quot;]</span><br><span class="line">        codec =&gt; json</span><br><span class="line">        sincedb_path =&gt; &quot;/home/logstash/sincedb&quot;</span><br><span class="line">        start_position =&gt; &quot;beginning&quot;</span><br><span class="line">        discover_interval =&gt; 30</span><br><span class="line">        close_older =&gt; 3600</span><br><span class="line">        ignore_older =&gt; 86400</span><br><span class="line">        sincedb_write_interval =&gt; 10</span><br><span class="line">        stat_interval =&gt; 1</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;</span><br><span class="line">ruby &#123;</span><br><span class="line">                code =&gt; &quot;if event.get(&apos;message&apos;).include?(&apos;\x&apos;) then</span><br><span class="line">    event.set(&apos;message&apos;, event.get(&apos;message&apos;).gsub(/\\x([0-9A-F]&#123;2&#125;)/) &#123;</span><br><span class="line">        case $1</span><br><span class="line">            when &apos;22&apos;</span><br><span class="line">                &apos;\\&quot;&apos;</span><br><span class="line">            when &apos;0D&apos;</span><br><span class="line">                &apos;\\r&apos;</span><br><span class="line">            when &apos;0A&apos;</span><br><span class="line">                &apos;\\n&apos;</span><br><span class="line">            when &apos;27&apos;</span><br><span class="line">                &apos;\\\&apos;&apos;</span><br><span class="line">            when &apos;5C&apos;</span><br><span class="line">                &apos;\\\\&apos;</span><br><span class="line">            else</span><br><span class="line">                $1.hex.chr</span><br><span class="line">        end</span><br><span class="line">    &#125;)</span><br><span class="line">end&quot;</span><br><span class="line">        &#125;</span><br><span class="line">        json &#123;</span><br><span class="line">                source =&gt; &quot;message&quot;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">mutate &#123;</span><br><span class="line">        remove_field =&gt;[&quot;message&quot;]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    kafka &#123;</span><br><span class="line">        bootstrap_servers =&gt; &quot;192.168.0.2:9092,192.168.0.3:9092,192.168.0.4:9092&quot;</span><br><span class="line">        topic_id =&gt; &quot;mtopic_name&quot;</span><br><span class="line">        compression_type =&gt; &quot;gzip&quot;</span><br><span class="line">    &#125;</span><br><span class="line">#stdout &#123; codec =&gt; rubydebug &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> logstash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logstash </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon事件日志收集</title>
      <link href="/2018/07/26/marathon%E4%BA%8B%E4%BB%B6%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/"/>
      <url>/2018/07/26/marathon%E4%BA%8B%E4%BB%B6%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86/</url>
      <content type="html"><![CDATA[<h2 id="marathon-事件日志收集到ES中"><a href="#marathon-事件日志收集到ES中" class="headerlink" title="marathon 事件日志收集到ES中"></a>marathon 事件日志收集到ES中</h2><p>  marathon 有配置可以主动把 marathon 的事件日志发送到 http 接口上。这里的事件包括，发布容器的json内容，docker容器死掉、启动，运行、kill，健康检查等日志。也可以说 marathon 所有的动作日志这上都会有。所有我要把这个日志收集起来，并可以做监控，历史查询等等。<br>  先说一下我的方案：<br>  启动一个logstash实例，并配置http接口，把收集到的日志存到ES索引中。<br>  marathon 配置 logstash 的 http 接口地址，把日志发送到这里即可。<br>  后期可以在ES中查询你想要的事件日志，并报警，或用kibana出图等。  </p><h3 id="logstash-配置"><a href="#logstash-配置" class="headerlink" title="logstash 配置"></a>logstash 配置</h3><p>#####logstash 配置文件<br>vim /etc/logstash.conf  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">    http &#123;</span><br><span class="line">         host =&gt; &quot;0.0.0.0&quot;</span><br><span class="line">         port =&gt; 7882</span><br><span class="line">         codec =&gt; json</span><br><span class="line">          add_field =&gt; &#123;</span><br><span class="line">            &quot;marathon&quot; =&gt; &quot;base-marathon&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">     elasticsearch &#123;</span><br><span class="line">          hosts =&gt; [&quot;10.214.193.27:9250&quot;,&quot;10.214.193.28:9250&quot;,&quot;10.214.193.29:9250&quot;]</span><br><span class="line">          index =&gt; &quot;logstash-marathon-%&#123;+YYYY.MM.dd&#125;&quot;</span><br><span class="line">          flush_size =&gt; 10000</span><br><span class="line">          idle_flush_time =&gt; 60</span><br><span class="line">          template_overwrite =&gt; true</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其中 add_field 添加一个字段  “marathon”:”base-marathon”<br>这样 ES的一个索引所可以放多个marathon的日志，但是需要启动多个logstash，每个配置不同，因为marathon的日志是一样的，区别不出来是哪个集群的日志。  </p><h5 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h5><p>可以用supervisor或docker方式启动。就不在这里说了。</p><h3 id="marathon-配置"><a href="#marathon-配置" class="headerlink" title="marathon 配置"></a>marathon 配置</h3><p>我用的是 yum 安装的marathon，版本是 1.4.9 , 配置是:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /etc/marathon/conf/</span><br><span class="line"></span><br><span class="line">echo &quot;http_callback&quot; &gt; /etc/marathon/conf/event_subscriber</span><br><span class="line">echo &quot;http://logstashIP:7882&quot; &gt; /etc/marathon/conf/http_endpoints</span><br></pre></td></tr></table></figure><p>二进制配置的参数是一样的。  </p><p>重启服务加载配置  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart marathon</span><br></pre></td></tr></table></figure><h4 id="ES-上查询日志"><a href="#ES-上查询日志" class="headerlink" title="ES 上查询日志"></a>ES 上查询日志</h4><p>这样配置完成之后，等marathon有动作的话，ES上就会有数据了，   </p><p>如：<br><img src="/images/marathon-es-data.png" alt="marathon-es-data"></p><p>这只是一个事件，还还有好多时间呢！</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Unix / Linux ssh-add Command Examples to Add SSH Key to Agent (转)</title>
      <link href="/2018/07/11/Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent-%E8%BD%AC/"/>
      <url>/2018/07/11/Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent-%E8%BD%AC/</url>
      <content type="html"><![CDATA[<h3 id="Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent"><a href="#Unix-Linux-ssh-add-Command-Examples-to-Add-SSH-Key-to-Agent" class="headerlink" title="Unix / Linux ssh-add Command Examples to Add SSH Key to Agent"></a>Unix / Linux ssh-add Command Examples to Add SSH Key to Agent</h3><p>ssh-add is a helper program for ssh-agent.<br>ssh-add adds RSA or DSA identity files to the ssh agent. For ssh-add to work properly, the agent should be running, and have the SSH_AUTH_SOCK environment variable set.  </p><ol><li>Fix “Could not Open” Error (and Add Default RSA/DSA identities)<br>By default, when you try to execute the ssh-add command, you might get “Could not open a connection to your authentication agent.” error message as shown below.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">Could not open a connection to your authentication agent.</span><br><span class="line">The reason is ssh-agent is not running.  </span><br><span class="line">But, if you start the ssh-agent as shown below, you’ll still get the same error.  </span><br><span class="line"></span><br><span class="line">$ ssh-agent</span><br><span class="line">SSH_AUTH_SOCK=/tmp/ssh-cYYsc14689/agent.14689; export SSH_AUTH_SOCK;</span><br><span class="line">SSH_AGENT_PID=14690; export SSH_AGENT_PID;</span><br><span class="line">echo Agent pid 14690;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">Could not open a connection to your authentication agent.</span><br><span class="line">In order to fix the issue, you should start the ssh-agent as shown below.  </span><br><span class="line"></span><br><span class="line">$ exec ssh-agent bash</span><br><span class="line">Now, when you execute the ssh-add, it will add the ~/.ssh/id_rsa, ~/.ssh/id_dsa and ~/.ssh/identity files to ssh-agent, and will not throw any error message.</span><br><span class="line"></span><br><span class="line">$ ssh-add</span><br><span class="line">Identity added: /home/ramesh/.ssh/id_rsa (/home/ramesh/.ssh/id_rsa)</span><br><span class="line">Identity added: /home/ramesh/.ssh/id_dsa (/home/ramesh/.ssh/id_dsa)</span><br></pre></td></tr></table></figure><ol start="2"><li>Display the entries loaded in ssh-agent<br>Use either -l or -L as shown below to display all the RSA and DSA entries that are currently loaded into the ssh-agent.<br>The following examples shows that there are two entries currently loaded to the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">2048 34:36:63:c2:7d:a5:13:e4 /home/ramesh/.ssh/id_rsa (RSA)</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br><span class="line"></span><br><span class="line">$ ssh-add -L</span><br><span class="line">ssh-rsa A2EAAAABIwAAAQEAtVRcaEnxOef0n5WLr9DV1JsLpx4E+P2Zf/N9JBLBbVKDD1BZf</span><br><span class="line">eRmLK8hZZKf0iva8+q1VNyxQB5oTfKGr79ll7KDRwfIgErw== /home/ramesh/.ssh/id_rsa</span><br><span class="line"></span><br><span class="line">ssh-dsa 8WDTpyJiLUNlIXSfCRe7nOjeMlgyn8vM3cWsosO0x4eMDYEMvefzhev0RAtbhyBvs</span><br><span class="line">WLLCwkaVzCZdZvsDa2cl7zKRd+3zLSfBQRa1wpMjJaeJbCg== /home/ramesh/.ssh/id_dsa</span><br></pre></td></tr></table></figure><ol start="3"><li>Delete all entries from ssh-agent<br>Use option -D as shown below to remove all the ssh entries from the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -D</span><br><span class="line">All identities removed.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">The agent has no identities.</span><br></pre></td></tr></table></figure><ol start="4"><li>Delete specific entries from ssh-agent<br>Using -d option, you can specify exactly what entries you like to delete.<br>The following example will remove only the default RSA entry from the ssh-agent.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -l</span><br><span class="line">2048 34:36:63:c2:7d:a5:13:e4 /home/ramesh/.ssh/id_rsa (RSA)</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br><span class="line"></span><br><span class="line">$ ssh-add -d /home/ramesh/.ssh/id_rsa</span><br><span class="line">Identity removed: /home/ramesh/.ssh/id_rsa (/home/ramesh/.ssh/id_rsa.pub)</span><br><span class="line"></span><br><span class="line">$ ssh-add -l</span><br><span class="line">1024 ee:60:11:bf:1b:31:3b:fb /home/ramesh/.ssh/id_dsa (DSA)</span><br></pre></td></tr></table></figure><ol start="5"><li>Lock (or) Unlock the SSH Agent<br>You can lock the ssh agent as shown below using -x option. Once you lock the agent, you cannot add, delete, or list entries in the ssh agent without a password.  </li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add -x</span><br><span class="line">Enter lock password:</span><br><span class="line">Again:</span><br><span class="line">Agent locked.</span><br><span class="line">After locking, if you try to add, you’ll se SSH_AGENT_FAILURE message as shown below.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-add</span><br><span class="line">SSH_AGENT_FAILURE</span><br><span class="line">SSH_AGENT_FAILURE</span><br><span class="line">Could not add identity: /home/ramesh/.ssh/id_rsa</span><br><span class="line">To unlock an agent, use -X option as shown below. Make sure you enter the same password that you gave while locking the agent. If you give a wrong password, you’ll set “Failed to unlock agent.” message.</span><br><span class="line"></span><br><span class="line">$ ssh-add -X</span><br><span class="line">Enter lock password:</span><br><span class="line">Agent unlocked.</span><br></pre></td></tr></table></figure>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ssh </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>cgroup 服务限制进程资源</title>
      <link href="/2018/07/05/cgroup-%E6%9C%8D%E5%8A%A1%E9%99%90%E5%88%B6%E8%BF%9B%E7%A8%8B%E8%B5%84%E6%BA%90/"/>
      <url>/2018/07/05/cgroup-%E6%9C%8D%E5%8A%A1%E9%99%90%E5%88%B6%E8%BF%9B%E7%A8%8B%E8%B5%84%E6%BA%90/</url>
      <content type="html"><![CDATA[<h2 id="用-cgroup-服务限制进程资源"><a href="#用-cgroup-服务限制进程资源" class="headerlink" title="用 cgroup 服务限制进程资源"></a>用 cgroup 服务限制进程资源</h2><h3 id="CGroup-功能及组成"><a href="#CGroup-功能及组成" class="headerlink" title="CGroup 功能及组成"></a>CGroup 功能及组成</h3><p>  CGroup 是将任意进程进行分组化管理的 Linux 内核功能。CGroup 本身是提供将进程进行分组化管理的功能和接口的基础结构，I/O 或内存的分配控制等具体的资源管理功能是通过这个功能来实现的。这些具体的资源管理功能称为 CGroup 子系统或控制器。CGroup 子系统有控制内存的 Memory 控制器、控制进程调度的 CPU 控制器等。运行中的内核可以使用的 Cgroup 子系统由/proc/cgroup 来确认。  </p><p>  CGroup 提供了一个 CGroup 虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用 CGroup，必须挂载 CGroup 文件系统。这时通过挂载选项指定使用哪个子系统。</p><h4 id="安装cgroup服务"><a href="#安装cgroup服务" class="headerlink" title="安装cgroup服务"></a>安装cgroup服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">centos 6</span><br><span class="line">yum install -y libcgroup</span><br><span class="line"></span><br><span class="line">centos 7 </span><br><span class="line">yum install -y libcgroup libcgroup-tools</span><br></pre></td></tr></table></figure><h4 id="配置cgroup配置"><a href="#配置cgroup配置" class="headerlink" title="配置cgroup配置"></a>配置cgroup配置</h4><p>  这里需要是限制cpu<br>  先创建一个组，把需要限制的进程，启动的时候放到这组下。  </p><p>vim /etc/cgconfig.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">mount &#123;</span><br><span class="line">#       cpuset  = /cgroup/cpuset;</span><br><span class="line">        cpu     = /cgroup/cpu;</span><br><span class="line">#       cpuacct = /cgroup/cpuacct;</span><br><span class="line">#       memory  = /cgroup/memory;</span><br><span class="line">#       devices = /cgroup/devices;</span><br><span class="line">#       freezer = /cgroup/freezer;</span><br><span class="line">#       net_cls = /cgroup/net_cls;</span><br><span class="line">#       blkio   = /cgroup/blkio;</span><br><span class="line">&#125;</span><br><span class="line">group yarn &#123;       # yarn 为组名</span><br><span class="line">   perm &#123;</span><br><span class="line">    task &#123;</span><br><span class="line">        uid = hadoop;     # 权限设置，为hadoop</span><br><span class="line">        gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">    admin &#123;</span><br><span class="line">       uid = hadoop;</span><br><span class="line">       gid = hadoop;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">   cpu &#123;   # 可以用 cpu.cfs_period_us 和 cpu.cfs_quota_us 来限制该组中的所有进程在单位时间里可以使用的 cpu 时间。</span><br><span class="line">          cpu.cfs_period_us= 100000;  # 就是时间周期，默认为 100000，即百毫秒  值的范围： 1000-100000 </span><br><span class="line">          cpu.cfs_quota_us= 2160000;  # cpu.cfs_quota_us 就是在这期间内可使用的 cpu 时间，默认 -1，即无限制</span><br><span class="line">   &#125;   # 现在这个设置代表，这个组可以用的cpu为21.6盒，2160000/100000 </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="启动cgroup服务"><a href="#启动cgroup服务" class="headerlink" title="启动cgroup服务"></a>启动cgroup服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">service cgroup restart</span><br><span class="line">chkconfig cgroup on</span><br></pre></td></tr></table></figure><h4 id="启动-yarn-服务"><a href="#启动-yarn-服务" class="headerlink" title="启动 yarn 服务"></a>启动 yarn 服务</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - hadoop</span><br><span class="line">yarn-daemon.sh stop nodemanager</span><br><span class="line">cgexec -g cpu:yarn yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure><p>注释： 用cgexe启动的服务，他的子进程也会在这个cgroup组下。总体cpu加和不会超过组的设置。</p><p>查看进程在哪个组下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@db-datanode09 ~]$ ps -eo pid,cgroup,cmd</span><br><span class="line">153514 cpu:/                               /sbin/udevd -d</span><br><span class="line">153515 cpu:/                               /sbin/udevd -d</span><br><span class="line">154089 cpu:/yarn                           /usr/java/jdk1.8.0_45/bin/java -Dproc_nodemanager -Xmx4096m -Dhadoop.log.dir=/home/hadoop/apache-hadoop/hadoop/logs</span><br><span class="line"></span><br><span class="line"># 注释： cpu:/  代表在cgroup根配置下，cpu:/yarn 代表在根的yarn的配置下</span><br></pre></td></tr></table></figure><p>检查服务  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /sys/fs/cgroup/cpu/yarn</span><br><span class="line">cat tasks | grep &quot;nodemanager PID&quot;</span><br></pre></td></tr></table></figure><p>这是在 /cgroup/cpu/ 目录下就会出现 yarn 目录， 权限是hadoop用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[root@db-datanode09 cpu]# cd /cgroup/cpu/yarn</span><br><span class="line">[root@db-datanode09 yarn]# ls -l</span><br><span class="line">total 0</span><br><span class="line">--w--w---- 1 hadoop hadoop 0 Aug  1 14:19 cgroup.event_control</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cgroup.procs</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.cfs_period_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.cfs_quota_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.rt_period_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.rt_runtime_us</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.shares</span><br><span class="line">-r--r--r-- 1 hadoop hadoop 0 Aug  1 14:19 cpu.stat</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:19 notify_on_release</span><br><span class="line">-rw-rw-r-- 1 hadoop hadoop 0 Aug  1 14:22 tasks</span><br></pre></td></tr></table></figure><hr><h2 id="cgroup服务几种模式介绍"><a href="#cgroup服务几种模式介绍" class="headerlink" title="cgroup服务几种模式介绍"></a>cgroup服务几种模式介绍</h2><h4 id="cgroup-配置文件说明"><a href="#cgroup-配置文件说明" class="headerlink" title="cgroup 配置文件说明"></a>cgroup 配置文件说明</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">mount &#123;</span><br><span class="line">       cpuset  = /cgroup/cpuset;为cgroup中的任务分配独立的cpu</span><br><span class="line">       cpu     = /cgroup/cpu;使用调度程序对cpu的使用控制</span><br><span class="line">       cpuacct = /cgroup/cpuacct;自动生成cgroup中的cpu使用的报告</span><br><span class="line">       memory  = /cgroup/memory;管理任务的内存</span><br><span class="line">       devices = /cgroup/devices;允许或拒绝cgroup中的任务访问设备</span><br><span class="line">       freezer = /cgroup/freezer;挂起或者恢复任务</span><br><span class="line">       net_cls = /cgroup/net_cls;控制网络流量</span><br><span class="line">       blkio   = /cgroup/blkio;为块设备输入输出设置，比如物理设备(磁盘，usb等)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="cpu限制"><a href="#cpu限制" class="headerlink" title="cpu限制"></a>cpu限制</h4><p>cgroup中对cpu资源控制的方式大约有三种：  </p><p>1.通过cpu子系统中的cpu quote方式</p><p>2.通过cpu子系统中的cpu share方式</p><p>3.通过cpuset子系统中的cpuset 将任务绑定到相应的cpu核上</p><p>cpuset的方式是限定任务可以在哪些cpu上运行；cpu share的方式，是在控制群组中设置权重，通过权重和任务等来分配能够使用cpu的资源；</p><h6 id="通过cpu-quote方式来限制"><a href="#通过cpu-quote方式来限制" class="headerlink" title="通过cpu quote方式来限制"></a>通过cpu quote方式来限制</h6><p>启动cgroup服务后，可以在/cgroup/cpu目录下看到如下文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">total 0</span><br><span class="line">--w--w--w- 1 root   root   0 Jul 26 11:44 cgroup.event_control</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cgroup.procs</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.cfs_period_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.cfs_quota_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.rt_period_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.rt_runtime_us</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 cpu.shares</span><br><span class="line">-r--r--r-- 1 root   root   0 Jul 26 11:44 cpu.stat</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 notify_on_release</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 release_agent</span><br><span class="line">-rw-r--r-- 1 root   root   0 Jul 26 11:44 tasks</span><br></pre></td></tr></table></figure><p>这里做一下说明：</p><p>cpu.cfs_period_us： 单位是微秒，最大值是1s，最小值是1毫秒(ms),取值范围为1000-1000000</p><p>cpu.cfs_quota_us 单位是微秒，意思是在 cpu.cfs_period_us的时间内，用户可以占用的时间。对于单核来说，最大等于 cpu.cfs_period_us的值，对于多核来说，可以理解为最多可使用的cpu核数</p><p>cpu.stat:</p><p>nr_periods 时间间隔， 指经过了多少个cpu.cfs_period_us的时间间隔 nr_throttled 被限制运行的次数 throttled_time 总共被限制的时间，微秒</p><p>在多核的系统中， cpu.cfs_quota_us/cpu.cfs_period_us 的值就是可以使用的最大的cpu的核数</p><p>tasks 将需要控制的任务的id写入到tasks文件中，就可以控制资源了</p><h6 id="分组"><a href="#分组" class="headerlink" title="分组"></a>分组</h6><p>cpu限制也有分组、分层的概念，<br>如： /cgroup/cpu  这是cpu的根级，默认不限制cpu使用量，<br>  /cgroup/cpu/yarn 这是cpu下的一层，也是一个组，他的cpu使用量不能大于上一层设置。<br>  还可以在/cgroup/cpu/yarn 下创建其他层。</p><h6 id="进程添加到控制组"><a href="#进程添加到控制组" class="headerlink" title="进程添加到控制组"></a>进程添加到控制组</h6><ol><li>单一pid添加到某个控制组<br> echo pid &gt; /cgroup/cpu/yarn/tasks</li><li>cgrule服务<br>用法：  </li></ol><p>user hierarchies control_group<br>user:command hierarchies control_group<br>当在user 使用前缀时代表是一个组群而不是单独用户例如@admins 是admins组群中的所有用户<br>cgrule配置文件在/etc/cgrule.conf,配置好启动服务后就可以根据规则自动将任务附加到控制群组了。</p><p>如：<br>vim /etc/cgrule.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"># Example:</span><br><span class="line">#&lt;user&gt;         &lt;controllers&gt;   &lt;destination&gt;</span><br><span class="line">#@student       cpu,memory      usergroup/student/</span><br><span class="line">#peter          cpu             test1/</span><br><span class="line">#%              memory          test2/</span><br><span class="line">rd           cpu             yarn     # rd 用户所有进程的cpu限制都在yarn这个组里</span><br><span class="line">@hadoopcpuyarn  # hadoop 组里所有用户的进程cpu限制都在yarn这个组里</span><br><span class="line">mtime:scpcpuyarn  # mtime的scp命令的cpu限制在yarn这个组里</span><br></pre></td></tr></table></figure><p>启动服务：  </p><p>/etc/init.d/cgred restart  </p><ol start="3"><li>cgexec 命令启动服务<br>用法：<br>cgexec -g subsystems:path_to_cgroup command arguments<br>如：<br>cgexec -g cpu:yarn yarn-daemon.sh start nodemanager</li></ol><h3 id="redhat-cgroup"><a href="#redhat-cgroup" class="headerlink" title="redhat cgroup"></a>redhat cgroup</h3><p>关于其他资源 如 memory、network等限制，可以参考 radhat cgroup的介绍<br>地址：<br>centos 6<br> <a href="https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups" target="_blank" rel="noopener">https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/6/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups</a></p><p>centos 7<br><a href="https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/7/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups" target="_blank" rel="noopener">https://access.redhat.com/documentation/zh-CN/Red_Hat_Enterprise_Linux/7/html-single/Resource_Management_Guide/index.html#chap-Introduction_to_Control_Groups</a></p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> cgroup </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>kafka修改分区和副本数</title>
      <link href="/2018/06/19/kafka%E4%BF%AE%E6%94%B9%E5%88%86%E5%8C%BA%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0/"/>
      <url>/2018/06/19/kafka%E4%BF%AE%E6%94%B9%E5%88%86%E5%8C%BA%E5%92%8C%E5%89%AF%E6%9C%AC%E6%95%B0/</url>
      <content type="html"><![CDATA[<p>##kafka修改分区和副本数 </p><p>查看现在副本分配情况</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-topics.sh --zookeeper 127.0.0.1:2181 --describe  --topic test1</span><br><span class="line"></span><br><span class="line">Topic:test1       PartitionCount:3        ReplicationFactor:2     Configs:</span><br><span class="line">        Topic: test1      Partition: 0    Leader: 2       Replicas: 2,4   Isr: 2,4</span><br><span class="line">        Topic: test1      Partition: 1    Leader: 3       Replicas: 3,5   Isr: 3,5</span><br><span class="line">        Topic: test1      Partition: 2    Leader: 4       Replicas: 4,1   Isr: 4,1</span><br></pre></td></tr></table></figure><h3 id="topic-分区扩容"><a href="#topic-分区扩容" class="headerlink" title="topic 分区扩容"></a>topic 分区扩容</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-topics.sh --zookeeper 127.0.0.1:2181 -alter --partitions 4 --topic test1</span><br></pre></td></tr></table></figure><h3 id="修改备份数量"><a href="#修改备份数量" class="headerlink" title="修改备份数量"></a>修改备份数量</h3><h4 id="这个文件自己创建-格式按照下面的格式就可以了"><a href="#这个文件自己创建-格式按照下面的格式就可以了" class="headerlink" title="这个文件自己创建 格式按照下面的格式就可以了"></a>这个文件自己创建 格式按照下面的格式就可以了</h4><p>根据topic的分区情况自行修改 partitions-topic.json 文件配置  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">        &quot;partitions&quot;:</span><br><span class="line">                [</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 0,</span><br><span class="line">                        &quot;replicas&quot;: [1,2]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 1,</span><br><span class="line">                        &quot;replicas&quot;: [0,3]</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                        &quot;topic&quot;: &quot;test1&quot;,</span><br><span class="line">                        &quot;partition&quot;: 2,</span><br><span class="line">                        &quot;replicas&quot;: [4,5]</span><br><span class="line">                &#125;</span><br><span class="line">                ],</span><br><span class="line">        &quot;version&quot;:1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="执行副本搬迁"><a href="#执行副本搬迁" class="headerlink" title="执行副本搬迁"></a>执行副本搬迁</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file partitions-topic.json --execute</span><br></pre></td></tr></table></figure><h4 id="查看迁移情况："><a href="#查看迁移情况：" class="headerlink" title="查看迁移情况："></a>查看迁移情况：</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">../bin/kafka-reassign-partitions.sh --zookeeper 127.0.0.1:2181 --reassignment-json-file partitions-topic.json --verify</span><br><span class="line"></span><br><span class="line">Status of partition reassignment:</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,0] is still in progress</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,1] completed successfully</span><br><span class="line">Reassignment of partition [mx_prd_nginx_access,2] is still in progress</span><br></pre></td></tr></table></figure><h4 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h4><p>kafka-reassign-partitions.sh工具来重新分布分区。该工具有三种使用模式：  </p><ol><li>generate模式，给定需要重新分配的Topic，自动生成reassign plan（并不执行）</li><li>execute模式，根据指定的reassign plan重新分配Partition</li><li>verify模式，验证重新分配Partition是否成功</li></ol><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> kafka </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>docker搭建macvlan网络</title>
      <link href="/2018/06/14/docker%E6%90%AD%E5%BB%BAmacvlan%E7%BD%91%E7%BB%9C/"/>
      <url>/2018/06/14/docker%E6%90%AD%E5%BB%BAmacvlan%E7%BD%91%E7%BB%9C/</url>
      <content type="html"><![CDATA[<h2 id="docker-搭建macvlan-网络"><a href="#docker-搭建macvlan-网络" class="headerlink" title="docker 搭建macvlan 网络"></a>docker 搭建macvlan 网络</h2><p>  简单说，macvlan就是在宿主的网卡设置多个vlan信息，根据走的网卡不同，并带有不行的vlan标记。  </p><h3 id="交换机需要支持"><a href="#交换机需要支持" class="headerlink" title="交换机需要支持"></a>交换机需要支持</h3><p>macvlan需要交换机上有几个设置：  </p><ul><li>连接宿主的交换机接口需要改为 Trunk 模式。（这样才能多vlan通过这个口通讯）</li><li>交换机上添加macvlan设置的相应vlan信息。</li><li>三层交换机上设置各个vlan的网关地址。并实现vlan间互联。</li></ul><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>环境介绍  </p><table><thead><tr><th>宿主机IP</th><th>宿主机vlan</th><th>macvlan IP</th><th>vlan 号</th></tr></thead><tbody><tr><td>192.168.53.11</td><td>233</td><td>172.20.30.x</td><td>30</td></tr><tr><td>192.168.53.12</td><td>233</td><td>172.20.19.x</td><td>19</td></tr></tbody></table><h4 id="实时生效安装"><a href="#实时生效安装" class="headerlink" title="实时生效安装"></a>实时生效安装</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">yum install -y epel-release</span><br><span class="line">yum install -y vconfig</span><br><span class="line">加载模块哦</span><br><span class="line">modprobe 8021q</span><br><span class="line">lsmod |grep -i 8021q</span><br><span class="line">网卡开启混合模式</span><br><span class="line">ip link set em1 promisc on</span><br><span class="line">使用vconfig命令配置vlan </span><br><span class="line">vconfig add em1 233 </span><br><span class="line">vconfig add em1 30   # 另外一台设置  vconfig add em1 19</span><br><span class="line">在em1接口上配置两个VLAN </span><br><span class="line">vconfig set_flag em1.233 1 1 </span><br><span class="line">vconfig set_flag em1.30 1 1   # 另外一台 vconfig set_flag em1.19 1 1</span><br><span class="line"></span><br><span class="line">ifconfig em1 0.0.0.0 </span><br><span class="line">ifconfig em1.233 192.168.53.11 netmask 255.255.255.0 up </span><br><span class="line">ifconfig em1.30 172.20.30.2 netmask 255.255.255.0 up</span><br></pre></td></tr></table></figure><p>这样一个临时配置就可以了， 配置docker的网络就可以，docker配置网络的命令后面一起发吧，  </p><p>上面属于临时配置，机器重启配置就没有了，不适合生产。</p><h4 id="永久配置"><a href="#永久配置" class="headerlink" title="永久配置"></a>永久配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">um install -y epel-release</span><br><span class="line">yum install -y vconfig</span><br></pre></td></tr></table></figure><p>添加模块<br>vim /etc/rc.d/rc.local 添加  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/sbin/modprobe 8021q</span><br></pre></td></tr></table></figure><p>网卡开启混合模式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;PROMISC=yes&quot; &gt;&gt; /etc/sysconfig/network-scripts/ifcfg-em1</span><br></pre></td></tr></table></figure><p>修改王凯配置文件<br>vim /etc/sysconfig/network-scripts/ifcfg-em1</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1</span><br><span class="line">NAME=em1</span><br><span class="line">TYPE=Ethernet</span><br><span class="line">BONDING_MASTER=yes</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=none</span><br><span class="line">PEERDNS=yes</span><br><span class="line">PROMISC=yes</span><br></pre></td></tr></table></figure><p>生成 macvlan 网卡</p><p>vim /etc/sysconfig/network-scripts/ifcfg-em1.233</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1.233</span><br><span class="line">NAME=em1.233</span><br><span class="line">ONBOOT=yes</span><br><span class="line">IPADDR=192.168.53.11</span><br><span class="line">NETMASK=255.255.255.0</span><br><span class="line">GATEWAY=192.168.53.1</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">VLAN=yes</span><br><span class="line">NM_CONTROLLED=no</span><br></pre></td></tr></table></figure><p>vim /etc/sysconfig/network-scripts/ifcfg-em1.30</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DEVICE=em1.30</span><br><span class="line">NAME=em1.30</span><br><span class="line">ONBOOT=yes</span><br><span class="line">BOOTPROTO=static</span><br><span class="line">VLAN=yes</span><br><span class="line">NM_CONTROLLED=no</span><br></pre></td></tr></table></figure><p>另外一台 其他配置都一样， ifcfg-em1.30 网卡信息修改为 ifcfg-em1.19 即可。  </p><p>之后重启网卡，如果配置没有问题，网络是可以连接的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/etc/init.d/network restart</span><br></pre></td></tr></table></figure><p>以后新添加vlan的时候，也可以先做好配置文件。直接ifup即可。<br>ifup /etc/sysconfig/network-scripts/ifcfg-em1.19</p><p>网络信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">[root@wd-slave01 ~]# ifconfig</span><br><span class="line">em1: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 108408  bytes 17234693 (16.4 MiB)</span><br><span class="line">        RX errors 0  dropped 11508  overruns 0  frame 0</span><br><span class="line">        TX packets 24225  bytes 4849942 (4.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line">        </span><br><span class="line">docker0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 172.17.0.1  netmask 255.255.0.0  broadcast 0.0.0.0</span><br><span class="line">        inet6 fe80::42:87ff:fecd:c222  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether 02:42:87:cd:c2:22  txqueuelen 0  (Ethernet)</span><br><span class="line">        RX packets 458940  bytes 71009715 (67.7 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 198525  bytes 55224280 (52.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">em1.233: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet 192.168.53.12  netmask 255.255.255.0  broadcast 192.168.53.255</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 108408  bytes 17234693 (16.4 MiB)</span><br><span class="line">        RX errors 0  dropped 11508  overruns 0  frame 0</span><br><span class="line">        TX packets 24225  bytes 4849942 (4.6 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br><span class="line"></span><br><span class="line">em1.30: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500</span><br><span class="line">        inet6 fe80::d6be:d9ff:feae:80cf  prefixlen 64  scopeid 0x20&lt;link&gt;</span><br><span class="line">        ether d4:be:d9:ae:80:cf  txqueuelen 1000  (Ethernet)</span><br><span class="line">        RX packets 2133458  bytes 245138875 (233.7 MiB)</span><br><span class="line">        RX errors 0  dropped 0  overruns 0  frame 0</span><br><span class="line">        TX packets 1343034  bytes 151915911 (144.8 MiB)</span><br><span class="line">        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0</span><br></pre></td></tr></table></figure><h4 id="docker-配置网络"><a href="#docker-配置网络" class="headerlink" title="docker 配置网络"></a>docker 配置网络</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=172.20.30.0 --gateway=172.20.30.1 -o parent=em1.30 mac_net1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker network ls   查看网络情况    </span><br><span class="line">docker network inspect 074ebc238447  查看网络详细信息及ip地址分配清凉</span><br></pre></td></tr></table></figure><p>启动容器   指定IP 指定网络  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --name test1 --ip=172.55.55.10 --network mac_net1 nginx-nettools:1.13  </span><br><span class="line">或动态分配</span><br><span class="line">docker run -d --name test2  --network mac_net1 nginx-nettools:1.13</span><br></pre></td></tr></table></figure><p>限制分配ip地址池</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker network create -d macvlan --subnet=172.20.30.0/24 --gateway=172.20.30.1 --ip-range=172.20.30.48/30 -o parent=em1.20 mac_net30</span><br><span class="line">这样只能分配4个ip地址</span><br><span class="line">172.20.30.128/25 也就是 128-255 可得 128个ip地址</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> docker </tag>
            
            <tag> macvlan </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>linux 删除乱码文件</title>
      <link href="/2018/06/04/linux-%E5%88%A0%E9%99%A4%E4%B9%B1%E7%A0%81%E6%96%87%E4%BB%B6/"/>
      <url>/2018/06/04/linux-%E5%88%A0%E9%99%A4%E4%B9%B1%E7%A0%81%E6%96%87%E4%BB%B6/</url>
      <content type="html"><![CDATA[<h2 id="linux-利用-inum-删除乱码文件"><a href="#linux-利用-inum-删除乱码文件" class="headerlink" title="linux 利用 inum 删除乱码文件"></a>linux 利用 inum 删除乱码文件</h2><p>  当系统中产生一些乱码文件的时候，rm直接是删除不掉的。如 “-，&amp;”等一些特殊字符。<br>  这时候我们可以利用linux 的inum 号来找到这个文件，并删除。</p><p>  例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# ll -i</span><br><span class="line">总用量 4</span><br><span class="line">   213388 -rw-r--r--. 1 root root    0 6月   4 07:40 -c</span><br><span class="line">134938544 drwxr-xr-x. 2 root root   23 12月 18 05:12 123</span><br><span class="line">   213391 -rw-r--r--. 1 root root    0 6月   4 07:40 --poolmetadata</span><br><span class="line">   213390 -rw-r--r--. 1 root root    0 6月   4 07:40 --thinpool</span><br><span class="line">   213387 -rw-r--r--. 1 root root    0 6月   4 07:40 --zero</span><br></pre></td></tr></table></figure><p>利用inum 号删除文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">删除文件或文件夹</span><br><span class="line">find ./ -inum 213388 -print -exec rm &#123;&#125; -rf \;</span><br><span class="line">删除文件</span><br><span class="line">find ./ -inum 213388 -delete;</span><br></pre></td></tr></table></figure><p>也可以重命名乱码文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find ./ -inum 213388 -exec mv &#123;&#125; newfile \;</span><br></pre></td></tr></table></figure><p>文件名字就改为了 newfile</p>]]></content>
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> linux </tag>
            
            <tag> rm </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>logstash out file to HDFS</title>
      <link href="/2018/05/29/logstash-out-file-to-HDFS/"/>
      <url>/2018/05/29/logstash-out-file-to-HDFS/</url>
      <content type="html"><![CDATA[<h2 id="logstash-out-file-to-HDFS"><a href="#logstash-out-file-to-HDFS" class="headerlink" title="logstash out file to HDFS"></a>logstash out file to HDFS</h2><p>  logstash 直接把文件内容写入 hdfs 中， 并支持 hdfs 压缩格式。<br>  logstash 需要安装第三方插件，webhdfs插件，通过hdfs的web接口写入。<br>  即 <a href="http://namenode00:50070/webhdfs/v1/" target="_blank" rel="noopener">http://namenode00:50070/webhdfs/v1/</a>  接口  </p><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p>  可以在官网找到相应的版本， 我们用的是2.3.1，下载地址：  </p><pre><code>https://www.elastic.co/downloads/past-releases  </code></pre><p>  webhdfs插件地址  </p><pre><code>github地址：  git clone  https://github.com/heqin5136/logstash-output-webhdfs-discontinued.git官网地址及使用说明：  https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html</code></pre><p>插件安装方式：</p><pre><code>logstash 安装在 /home/mtime/logstash-2.3.1git clone  https://github.com/heqin5136/logstash-output-webhdfs-discontinued.gitcd logstash-output-webhdfs-discontinued/home/mtime/logstash-2.3.1/bin/plugin install logstash-output-webhdfs-discontinued</code></pre><p>检查hdfs的webhds接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">   curl -i  &quot;http://namenode:50070/webhdfs/v1/?user.name=hadoop&amp;op=LISTSTATUS&quot;   </span><br><span class="line">   </span><br><span class="line">HTTP/1.1 200 OK</span><br><span class="line">Cache-Control: no-cache</span><br><span class="line">Expires: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Date: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Expires: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Date: Thu, 13 Jul 2017 04:53:39 GMT</span><br><span class="line">Pragma: no-cache</span><br><span class="line">Content-Type: application/json</span><br><span class="line">Set-Cookie: hadoop.auth=&quot;u=hadoop&amp;p=hadoop&amp;t=simple&amp;e=1499957619679&amp;s=KSxdSAtjXAllhn73vh1MAurG9Bk=&quot;; Path=/; Expires=Thu, 13-Jul-2017 14:53:39 GMT; HttpOnly</span><br><span class="line">Transfer-Encoding: chunked</span><br><span class="line">Server: Jetty(6.1.26)</span><br></pre></td></tr></table></figure><p>注释： active namenode 返回是200 ，standby namenode 返回是403.</p><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>添加 logstash 一个配置文件</p><p>vim /home/mtime/logstash-2.3.1/conf/hdfs.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  kafka &#123;</span><br><span class="line">    zk_connect =&gt; &quot;192.168.51.191:2181,192.168.51.192:2181,192.168.51.193:2181&quot;   ## kafka zk 地址 </span><br><span class="line">    group_id =&gt; &apos;hdfs&apos;   # 消费者组</span><br><span class="line">    topic_id =&gt; &apos;tracks&apos;  # topic 名字</span><br><span class="line">    consumer_threads =&gt; 1  </span><br><span class="line">    codec =&gt; &apos;json&apos;  </span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">filter &#123;   ##  为解决 插入hdfs时间相差8小时， </span><br><span class="line">        date &#123;  </span><br><span class="line">                match =&gt; [ &quot;time&quot; , &quot;yyyy-MM-dd HH:mm:ss&quot; ]</span><br><span class="line">                locale =&gt; &quot;zh&quot;</span><br><span class="line">                timezone =&gt; &quot;-00:00:00&quot;</span><br><span class="line">                target =&gt; &quot;@timestamp&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">output &#123;</span><br><span class="line">    webhdfs &#123;</span><br><span class="line">           workers =&gt; 1</span><br><span class="line">           host =&gt; &quot;namenode&quot;</span><br><span class="line">           standby_host =&gt; &quot;standbynamenode&quot;</span><br><span class="line">           port =&gt; 14000</span><br><span class="line">           user =&gt; &quot;loguser&quot;</span><br><span class="line">           path =&gt; &quot;/Service-Data/%&#123;+YYYY&#125;-%&#123;+MM&#125;-%&#123;+dd&#125;/%&#123;app&#125;/logstash-%&#123;+HH&#125;.log&quot;</span><br><span class="line">           flush_size =&gt; 10000</span><br><span class="line">           idle_flush_time =&gt; 10</span><br><span class="line">           compression =&gt; &quot;gzip&quot;</span><br><span class="line">           retry_interval =&gt; 3</span><br><span class="line">           codec =&gt; &apos;json&apos;   # 解决 写入hdfs文件是json格式，否则内容为 %&#123;message&#125;</span><br><span class="line">       &#125;</span><br><span class="line">  # stdout &#123; codec =&gt; rubydebug &#125; # 打开日志</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于hdfs部分配置，可以在 <a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-webhdfs.html" target="_blank" rel="noopener">plugins-outputs-webhdfs</a>  官网找到。</p><h3 id="启动-logstart"><a href="#启动-logstart" class="headerlink" title="启动 logstart"></a>启动 logstart</h3><pre><code>cd /home/mtime/logstash-2.3.1/bin/./logstash -f ../conf/hdfs.conf    # 为前台启动 </code></pre><h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><ul><li>新版logstash已经支持webhdfs插件，可以直接安装啦。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./logstash-plugin install logstash-output-webhdfs</span><br></pre></td></tr></table></figure><ul><li>报错处理</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[WARN ][logstash.outputs.webhdfs ] Failed to flush outgoing items &#123;:outgoing_count=&gt;160, :exception=&gt;&quot;WebHDFS::IOError&quot;,</span><br></pre></td></tr></table></figure><p>我将hdfs端口 由原来的50070 改为 14000 端口，就在不报错了。<br>官方提供的例子中用的就是50070端口，一直没有尝试14000端口。</p><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">because this file lease is currently owned by DFSClient</span><br></pre></td></tr></table></figure><p>hadoop 租约问题，后期正常就没有了。<br> 执行recoverLease来释放文件的锁</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hdfs debug recoverLease -path /logstash/2017/02/10/go-03.log</span><br></pre></td></tr></table></figure><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">:message=&gt;&quot;webhdfs write caused an exception: &#123;\&quot;RemoteException\&quot;:&#123;\&quot;message\&quot;:\&quot;Failed to APPEND_FILE</span><br></pre></td></tr></table></figure><p>当一个进程在读写这个文件的时候，另一个进程应该是不能同时写入的。<br>我们由原来3个logstash同时消费，改为了1个logstash消费，不在报错了。<br>这个应该也可以通过有话写入hdfs参数来解决。</p><p>还有：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Max write retries reached. Exception: initialize: name or service not known &#123;:level=&gt;:error&#125;</span><br></pre></td></tr></table></figure><p>losgstash 需要能解析所有 hadoop 集群所有节点的主机名。</p><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> logstash </category>
          
      </categories>
      
      
        <tags>
            
            <tag> logstash </tag>
            
            <tag> hdfs </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos容器映射端口限制</title>
      <link href="/2018/05/29/mesos%E5%AE%B9%E5%99%A8%E6%98%A0%E5%B0%84%E7%AB%AF%E5%8F%A3%E9%99%90%E5%88%B6/"/>
      <url>/2018/05/29/mesos%E5%AE%B9%E5%99%A8%E6%98%A0%E5%B0%84%E7%AB%AF%E5%8F%A3%E9%99%90%E5%88%B6/</url>
      <content type="html"><![CDATA[<h2 id="mesos-容器映射端口限制"><a href="#mesos-容器映射端口限制" class="headerlink" title="mesos 容器映射端口限制"></a>mesos 容器映射端口限制</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>  mesos 在启动容器的时候，理念是容器内的端都映射到宿主的随机端口。<br>  在容器的时代，其实这样的理念是很好，当容器多的时候，固定端口肯定是有一定的局限性的。可以通过注册中心、mesos-dns、marathon-lb等服务来找到你要的服务地址和端口。<br>  但是有时候有一些服务需要一些固定端口。比如cadvisor、还有我们自己写的容器，可能会映射一些其他端口。  </p><h3 id="默认端口限制"><a href="#默认端口限制" class="headerlink" title="默认端口限制"></a>默认端口限制</h3><p>  默认mesos的端口也是可以指定的，只是范围比较小。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">31000 - 32000</span><br></pre></td></tr></table></figure><p>  marahotn 的json 文件中，你可以写。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;portMappings&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;containerPort&quot;: 80,</span><br><span class="line">    &quot;hostPort&quot;: 31000,  # 一般设置 0 为随机端口，</span><br><span class="line">    &quot;servicePort&quot;: 0,</span><br><span class="line">    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>docker 启动时候就是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                            NAMES</span><br><span class="line">70314cd31714        nginx-nettools:1.13   &quot;nginx -g &apos;daemon ...&quot;   24 minutes ago      Up 24 minutes       443/tcp, 0.0.0.0:31000-&gt;80/tcp   mesos-07a768f1-f635-4517-9b60-4e86bfef658e</span><br></pre></td></tr></table></figure><h3 id="配置mesos"><a href="#配置mesos" class="headerlink" title="配置mesos"></a>配置mesos</h3><p>yum 安装的meoss 添加配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;ports(*):[1024-65534]&quot; &gt; /etc/mesos-slave/resources</span><br></pre></td></tr></table></figure><p>重启 mesos-slave 服务</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart mesos-slave</span><br></pre></td></tr></table></figure><p>二进制安装的mesos 在启动命令中添加</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--resources=ports(*):[1024-65534]</span><br></pre></td></tr></table></figure><p>这样你的端口就是在 1024 - 65524 中间随意指定了。</p><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>marathon json文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&quot;portMappings&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;containerPort&quot;: 80,</span><br><span class="line">    &quot;hostPort&quot;: 8080,</span><br><span class="line">    &quot;servicePort&quot;: 0,</span><br><span class="line">    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>docker 启动时候是</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@test00 ~]# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                 COMMAND                  CREATED             STATUS              PORTS                           NAMES</span><br><span class="line">1235513ee658        nginx-nettools:1.13   &quot;nginx -g &apos;daemon ...&quot;   6 minutes ago       Up 6 minutes        443/tcp, 0.0.0.0:8080-&gt;80/tcp   mesos-655d4923-0d1f-4130-8d61-aab824df3f25-S13.9e0c2cfb-3d07-467f-ac47-08e492703263</span><br></pre></td></tr></table></figure><h3 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h3><p>如果mesos上运行过容器，在你修改配置文件之后重启会有问题。  </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你可以通过</span><br><span class="line">journalctl -xe</span><br><span class="line">或</span><br><span class="line">查看mesos的log日志 找到问题</span><br></pre></td></tr></table></figure><p>解决方法： 日志中会有提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">To remedy this do as follows:</span><br><span class="line">Step 1: rm -f /home/mtime/mesos/meta/slaves/latest</span><br><span class="line">        This ensures agent doesn&apos;t recover old live executors.</span><br><span class="line">  ep 2: Restart the agent.</span><br></pre></td></tr></table></figure><p>rm -f /home/mtime/mesos/meta/slaves/latest<br>删除之后在重启即可。</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
            <tag> port </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon私有仓库用户名和密码方式</title>
      <link href="/2018/05/28/marathon%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E5%AF%86%E7%A0%81%E6%96%B9%E5%BC%8F/"/>
      <url>/2018/05/28/marathon%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E7%94%A8%E6%88%B7%E5%90%8D%E5%92%8C%E5%AF%86%E7%A0%81%E6%96%B9%E5%BC%8F/</url>
      <content type="html"><![CDATA[<h1 id="marathon-使用仓库用户名和密码方式"><a href="#marathon-使用仓库用户名和密码方式" class="headerlink" title="marathon 使用仓库用户名和密码方式"></a>marathon 使用仓库用户名和密码方式</h1><h2 id="首先需要本地手动登入镜像仓库。"><a href="#首先需要本地手动登入镜像仓库。" class="headerlink" title="首先需要本地手动登入镜像仓库。"></a>首先需要本地手动登入镜像仓库。</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># docker login registry.inc-test.com</span><br><span class="line">   Username: admin </span><br><span class="line">   Password: Default@123</span><br></pre></td></tr></table></figure><p>登入成功之后会在当前用户的家目录创建一个隐藏目录 ~/.docker ，打包这么目录，放在一个目录下， 并让marathon启动容器的时候引用这个文件即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cd ~</span><br><span class="line"># tar czf docker.tar.gz .docker</span><br><span class="line"></span><br><span class="line"># cp docker.tar.gz /etc/</span><br></pre></td></tr></table></figure><h2 id="marathon-json-启动容器引用验证文件"><a href="#marathon-json-启动容器引用验证文件" class="headerlink" title="marathon json 启动容器引用验证文件"></a>marathon json 启动容器引用验证文件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;uris&quot;: [</span><br><span class="line">   &quot;file:///etc/docker.tar.gz&quot;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>注释：  </p><ul><li>这样需要每台mesos slave机器都需要放置这个文件，实际操作很不灵活，</li><li>而且用户切换也不好做，每台机器需要放不不用户的验证文件。</li><li>如果用户密码修改，还需要批量修改每台slave机器上的验证文件。</li></ul><h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><ul><li>把这个文件放在http页面上，只要网络通就可以访问，不需要每台机器都配置验证文件，修改也比较访问。</li></ul><p>把docker.tar.gz文件放在http页面中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scp /etc/docker.tar.gz 10.10.130.201:/var/www/html/download/docker_img/harbor-admin.tar.gz</span><br><span class="line"></span><br><span class="line"># 一个用户手动生成一个文件，如需要切换用户的时候指定不同文件即可。</span><br></pre></td></tr></table></figure><h2 id="例如："><a href="#例如：" class="headerlink" title="例如："></a>例如：</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;nginx&quot;,</span><br><span class="line">    &quot;cpus&quot;: 0.2,</span><br><span class="line">    &quot;mem&quot;: 128,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">    &quot;constraints&quot;: [</span><br><span class="line">        [</span><br><span class="line">            &quot;hostname&quot;,</span><br><span class="line">            &quot;CLUSTER&quot;,</span><br><span class="line">            &quot;es02.host-test.com&quot;</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    &quot;uris&quot;: [</span><br><span class="line">        &quot;http://10.10.130.201/download/docker_img/harbor-admin.tar.gz&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;registry.inc-test.com/web-lb/nginx:1.13&quot;,</span><br><span class="line">            &quot;network&quot;: &quot;BRIDGE&quot;,</span><br><span class="line">            &quot;portMappings&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 80,</span><br><span class="line">                    &quot;hostPort&quot;: 31009,</span><br><span class="line">                    &quot;servicePort&quot;: 0,</span><br><span class="line">                    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>marathon 官网说明 <a href="https://mesosphere.github.io/marathon/docs/native-docker-private-registry.html" target="_blank" rel="noopener">https://mesosphere.github.io/marathon/docs/native-docker-private-registry.html</a></p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos解决sandbox日志切分问题</title>
      <link href="/2018/05/28/mesos%E8%A7%A3%E5%86%B3sandbox%E6%97%A5%E5%BF%97%E5%88%87%E5%88%86%E9%97%AE%E9%A2%98/"/>
      <url>/2018/05/28/mesos%E8%A7%A3%E5%86%B3sandbox%E6%97%A5%E5%BF%97%E5%88%87%E5%88%86%E9%97%AE%E9%A2%98/</url>
      <content type="html"><![CDATA[<h1 id="mesos-解决sandbox日志切分问题"><a href="#mesos-解决sandbox日志切分问题" class="headerlink" title="mesos 解决sandbox日志切分问题"></a>mesos 解决sandbox日志切分问题</h1><p>mesos运行的docker容器，容器打印到前台console的日志会记录到mesos的work目录中容器沙箱中stdout和stderr文件中，容器不重启，日志会一直变大，这样会到只宿主空间变大。  </p><p>另外这份日志还会日志到系统的/var/log/messages 文件中。  </p><p>首先关于 mesos-slave 的 work-dir 中设置的目录，里面存放的docker容器的沙箱目录，会有 stderr\stdout等文件，其中这两个文件是记录容器console的日志，会一直保留，直到容器销毁，这样日志文件会持续增大。</p><p>为解决这个问题问题。mesos 没有明确的配置。 <a href="http://mesos.apache.org/documentation/latest/logging/" target="_blank" rel="noopener">http://mesos.apache.org/documentation/latest/logging/</a> 文章中有提到沙箱大小的设置，但是没有测试成功。</p><p>我的解决办法：利用系统的 logrotate 模块做日志的切分和删除。</p><p>如：添加配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; /etc/logrotate.d/mesos  &lt;&lt; EOF</span><br><span class="line">/home/mtime/mesos/slaves/*/frameworks/*/executors/*/runs/latest/stderr</span><br><span class="line">/home/mtime/mesos/slaves/*/frameworks/*/executors/*/runs/latest/stdout </span><br><span class="line">&#123;</span><br><span class="line">        daily</span><br><span class="line">        missingok</span><br><span class="line">        copytruncate</span><br><span class="line">        notifempty</span><br><span class="line">        size 102400</span><br><span class="line">        dateext</span><br><span class="line">        rotate 7</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>这样每天都会切分 大于 100Mb的日志了， 并保留7天。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">/usr/sbin/logrotate -d -v -f /etc/logrotate.conf</span><br><span class="line"></span><br><span class="line">-d  测试配置文件，不是真正执行。</span><br></pre></td></tr></table></figure><p>crontab  中 已经添加，logrotate 会每天执行的。/etc/cron.daily/logrotate </p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>marathon-lb配置及nginx负载</title>
      <link href="/2018/05/28/marathon-lb%E9%85%8D%E7%BD%AE%E5%8F%8Anginx%E8%B4%9F%E8%BD%BD/"/>
      <url>/2018/05/28/marathon-lb%E9%85%8D%E7%BD%AE%E5%8F%8Anginx%E8%B4%9F%E8%BD%BD/</url>
      <content type="html"><![CDATA[<h1 id="marathon-lb配置"><a href="#marathon-lb配置" class="headerlink" title="marathon-lb配置"></a>marathon-lb配置</h1><h2 id="marathon-lb-get-images"><a href="#marathon-lb-get-images" class="headerlink" title="marathon-lb get images"></a>marathon-lb get images</h2><p>Marathon-lb既是一个服务发现工具，也是负载均衡工具，它集成了haproxy，自动获取各个app的信息，为每一组app生成haproxy配置，通过servicePort或者web虚拟主机提供服务。</p><p>要使用marathonn-lb，每组app必须设置HAPROXY_GROUP标签。</p><p>Marathon-lb运行时绑定在各组app定义的服务端口（servicePort，如果app不定义servicePort，marathon会随机分配端口号）上，可以通过marathon-lb所在节点的相关服务端口访问各组app。</p><p>例如：marathon-lb部署在slave5，test-app 部署在slave1，test-app 的servicePort是10004，那么可以在slave5的 10004端口访问到test-app提供的服务。</p><p>由于servicePort 非80、443端口（80、443端口已被marathon-lb中的 haproxy独占），对于web服务来说不太方便，可以使用 haproxy虚拟主机解决这个问题：</p><p>在提供web服务的app配置里增加HAPROXY_{n}_VHOST（WEB虚拟主机）标签，marathon-lb会自动把这组app的WEB集群服务发布在marathon-lb所在节点的80和443端口上，用户设置DNS后通过虚拟主机名来访问。</p><h3 id="官方下载镜像"><a href="#官方下载镜像" class="headerlink" title="官方下载镜像"></a>官方下载镜像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">images url :</span><br><span class="line">https://store.docker.com/community/images/mesosphere/marathon-lb</span><br><span class="line"></span><br><span class="line">docker pull mesosphere/marathon-lb</span><br><span class="line"></span><br><span class="line">github url:</span><br><span class="line">https://github.com/mesosphere/marathon-lb</span><br></pre></td></tr></table></figure><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>docker</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d --privileged -e PORTS=9090 --net=host docker.io/mesosphere/marathon-lb sse -m http://marathon1_ip:8080 -m http://marathon2_ip:8080 -m http://master3_ip:8080  --group external</span><br></pre></td></tr></table></figure><p>marathon</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">vim marathon-lb.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;marathon-lb-testv1&quot;,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">    &quot;constraints&quot;: [</span><br><span class="line">        [</span><br><span class="line">            &quot;hostname&quot;,</span><br><span class="line">            &quot;CLUSTER&quot;,</span><br><span class="line">            &quot;host-hostname.com&quot;</span><br><span class="line">        ]</span><br><span class="line">    ],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;docker.io/mesosphere/marathon-lb:latest&quot;,</span><br><span class="line">            &quot;privileged&quot;: true,</span><br><span class="line">            &quot;network&quot;: &quot;HOST&quot;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;args&quot;: [</span><br><span class="line">        &quot;sse&quot;,</span><br><span class="line">        &quot;-m&quot;,</span><br><span class="line">        &quot;http://10.10.131.78:8080&quot;,</span><br><span class="line">        &quot;--group&quot;,</span><br><span class="line">        &quot;external&quot;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">curl -X POST http://10.10.131.78:8080/v2/apps -d @marathon-lb.json -H &quot;Content-type: application/json&quot;</span><br></pre></td></tr></table></figure><h2 id="marathon-lb-API"><a href="#marathon-lb-API" class="headerlink" title="marathon-lb API"></a>marathon-lb API</h2><table><thead><tr><th>Endpoint</th><th>Description</th></tr></thead><tbody><tr><td>:9090/haproxy?stats</td><td>HAProxy stats endpoint. This produces an HTML page which can be viewed in your browser, providing various statistics about the current HAProxy instance.</td></tr><tr><td>:9090/haproxy?stats;csv</td><td>This is a CSV version of the stats above, which can be consumed by other tools. For example, it’s used in the zdd.py script.</td></tr><tr><td>:9090/_haproxy_health_check</td><td>HAProxy health check endpoint. Returns 200 OK if HAProxy is healthy.</td></tr><tr><td>:9090/_haproxy_getconfig</td><td>Returns the HAProxy config file as it was when HAProxy was started. Implemented in getconfig.lua.</td></tr><tr><td>:9090/_haproxy_getvhostmap</td><td>Returns the HAProxy vhost to backend map. This endpoint returns HAProxy map file only when the –haproxy-map flag is enabled, it returns an empty string otherwise. Implemented in getmaps.lua.</td></tr><tr><td>:9090/_haproxy_getappmap</td><td>Returns the HAProxy app ID to backend map. Like _haproxy_getvhostmap, this requires the –haproxy-map flag to be enabled and returns an empty string otherwise. Also implemented in getmaps.lua.</td></tr><tr><td>:9090/_haproxy_getpids</td><td>Returns the PIDs for all HAProxy instances within the current process namespace. This literally returns $(pidof haproxy). Implemented in getpids.lua. This is also used by the zdd.py script to determine if connections have finished draining during a deploy.</td></tr><tr><td>:9090/_mlb_signal/hup*</td><td>Sends a SIGHUP signal to the marathon-lb process, causing it to fetch the running apps from Marathon and reload the HAProxy config as though an event was received from Marathon.</td></tr><tr><td>:9090/_mlb_signal/usr1*</td><td>Sends a SIGUSR1 signal to the marathon-lb process, causing it to restart HAProxy with the existing config, without checking Marathon for changes.</td></tr></tbody></table><ul><li>API from marathon-lb <a href="https://github.com/mesosphere/marathon-lb" target="_blank" rel="noopener">!github</a></li><li>marathon-lb 文档详解 <a href="https://github.com/mesosphere/marathon-lb/blob/master/Longhelp.md#templates" target="_blank" rel="noopener">!https://github.com/mesosphere/marathon-lb/blob/master/Longhelp.md#templates</a></li></ul><p>如常用： <a href="http://marathon-lb-ip:9090/haproxy?stats" target="_blank" rel="noopener">http://marathon-lb-ip:9090/haproxy?stats</a></p><h2 id="nginx-start"><a href="#nginx-start" class="headerlink" title="nginx start"></a>nginx start</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># vim nginx.json</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;id&quot;: &quot;nginx-test&quot;,</span><br><span class="line">    &quot;cpus&quot;: 0.2,</span><br><span class="line">    &quot;mem&quot;: 128,</span><br><span class="line">    &quot;instances&quot;: 1,</span><br><span class="line">  &quot;labels&quot;: &#123;</span><br><span class="line">     &quot;HAPROXY_GROUP&quot;:&quot;external&quot;</span><br><span class="line">     &quot;HAPROXY_0_VHOST&quot;:&quot;nginx.test.com&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">    &quot;uris&quot;: [</span><br><span class="line">        &quot;http://10.10.130.201/download/docker_img/db-harbor-admin.tar.gz&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;healthChecks&quot;: [&#123; &quot;path&quot;: &quot;/&quot; &#125;],</span><br><span class="line">    &quot;container&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;DOCKER&quot;,</span><br><span class="line">        &quot;docker&quot;: &#123;</span><br><span class="line">            &quot;image&quot;: &quot;nginx:1.13&quot;,</span><br><span class="line">            &quot;network&quot;: &quot;BRIDGE&quot;,</span><br><span class="line">            &quot;portMappings&quot;: [</span><br><span class="line">                &#123;</span><br><span class="line">                    &quot;containerPort&quot;: 80,</span><br><span class="line">                    &quot;hostPort&quot;: 0,</span><br><span class="line">                    &quot;servicePort&quot;: 10000,</span><br><span class="line">                    &quot;protocol&quot;: &quot;tcp&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># curl -X POST http://10.10.131.78:8080/v2/apps -d @nginx.json -H &quot;Content-type: application/json&quot;</span><br></pre></td></tr></table></figure><h2 id="说明："><a href="#说明：" class="headerlink" title="说明："></a>说明：</h2><ol><li>一定要加上HAPROXY_GROUP标签，它填写的是marathon-lb创建时定义的组名 </li><li>HAPROXY_0_VHOST是标签名，对于web服务可以加上VHOST标签，让marathon-lb设置WEB虚拟主机；</li><li>containerPort为80,是指容器内的端口。</li><li>hostPort是当前主机映射到contenterPort的端口，如果hostPort为0的话,则说明是随机的。</li><li>serverPort是marathon-lb需要配置的haproxy代理暴露的端口,这里设置为10000，说明访问marathon-lb机器的10000端口就可为访问这个应用容器的80端口。</li></ol><h2 id="访问marathon-lb"><a href="#访问marathon-lb" class="headerlink" title="访问marathon-lb"></a>访问marathon-lb</h2><p>ip 访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http://marathon-lb_ip:10000/</span><br></pre></td></tr></table></figure><ul><li>访问marathon-lb部署的宿主机ip地址和serverPort的端口。</li></ul><p>域名访问</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">需要添加dns解析，根据 &quot;HAPROXY_0_VHOST&quot;:&quot;nginx.test.com&quot; 设置的配置。</span><br><span class="line">如：</span><br><span class="line">vim /etc/hosts  添加</span><br><span class="line">10.10.131.151nginx.test.com</span><br><span class="line"></span><br><span class="line">这里 10.10.131.151 是 marathon-lb 的ip地址</span><br><span class="line"></span><br><span class="line">curl nginx.test.com  即可</span><br></pre></td></tr></table></figure><h2 id="marathon-lb-代理80端口"><a href="#marathon-lb-代理80端口" class="headerlink" title="marathon-lb 代理80端口"></a>marathon-lb 代理80端口</h2><p>默认marathon-lb 80和443端口是被占用的，所以nginx在发布的时候“serverPort”是不能设置为80和443端口的。  </p><p>为了解决这个问题，需要更改源码，重新生成镜像。  </p><p>首先现在 marathon-lb源码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># git clone https://github.com/mesosphere/marathon-lb.git</span><br><span class="line"># cd marathon-lb</span><br><span class="line"></span><br><span class="line">在这个目录下找到所有80、443端口信息。改为其他端口</span><br><span class="line"></span><br><span class="line"># grep 80 . -R</span><br><span class="line">找到相应文件，80 替换为7080</span><br><span class="line">:%s/80/7080/g</span><br><span class="line"></span><br><span class="line">找到相应文件，443 替换为7443</span><br><span class="line">:%s/443/7443/g</span><br></pre></td></tr></table></figure><p>重新生成镜像</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker build -t marathon-lb-7080 .</span><br><span class="line"></span><br><span class="line">成功之后 docker images 就会多出 marathon-lb-7080 镜像</span><br></pre></td></tr></table></figure><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> marathon </category>
          
      </categories>
      
      
        <tags>
            
            <tag> marathon </tag>
            
            <tag> marathon-lb </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>mesos-dns搭建</title>
      <link href="/2018/05/25/mesos-dns%E6%90%AD%E5%BB%BA/"/>
      <url>/2018/05/25/mesos-dns%E6%90%AD%E5%BB%BA/</url>
      <content type="html"><![CDATA[<h2 id="mesos-dns-搭建"><a href="#mesos-dns-搭建" class="headerlink" title="mesos-dns 搭建"></a>mesos-dns 搭建</h2><p>  Mesos-DNS用来支持Mesos集群上的服务发现，使运行在Mesos上的应用和服务可以通过域名服务器来发现彼此。你只要知道一个Mesos数据中心上运行的应用的名字，就可以通过Mesos-DNS查询到该应用的IP和端口号。  </p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>  官方下载mesos-dns镜像没有提供mesos-dns的HTTP接口出来，所以先用二进制搭建，在自己build镜像。  </p><p>  mesos-dns文件下载：<a href="https://github.com/mesosphere/mesos-dns/releases" target="_blank" rel="noopener">!https://github.com/mesosphere/mesos-dns/releases</a></p><p>  下载 mesos-dns-v0.6.0-linux-amd64 一个二进制文件。</p><p>  准备配置文件：config.json</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;zk&quot;: &quot;zk://10.0.0.52:2181,10.0.0.53:2181,10.0.0.54:2181/mesos&quot;,</span><br><span class="line">  &quot;masters&quot;: [&quot;10.0.0.52:5050&quot;, &quot;10.0.0.53:5050&quot;, &quot;10.0.0.54:5050&quot;],</span><br><span class="line">  &quot;refreshSeconds&quot;: 10,</span><br><span class="line">  &quot;ttl&quot;: 0,</span><br><span class="line">  &quot;domain&quot;: &quot;mesos&quot;,</span><br><span class="line">  &quot;port&quot;: 53,</span><br><span class="line">  &quot;resolvers&quot;: [&quot;10.10.130.5&quot;],</span><br><span class="line">  &quot;timeout&quot;: 5, </span><br><span class="line">  &quot;httpon&quot;: true,</span><br><span class="line">  &quot;dnson&quot;: true,</span><br><span class="line">  &quot;httpport&quot;: 8123,</span><br><span class="line">  &quot;externalon&quot;: true,</span><br><span class="line">  &quot;listener&quot;: &quot;0.0.0.0&quot;,</span><br><span class="line">  &quot;SOAMname&quot;: &quot;docker-test.com&quot;,</span><br><span class="line">  &quot;SOARname&quot;: &quot;root.docker-test.com&quot;,</span><br><span class="line">  &quot;SOARefresh&quot;: 10,</span><br><span class="line">  &quot;SOARetry&quot;:   3,</span><br><span class="line">  &quot;SOAExpire&quot;:  86400,</span><br><span class="line">  &quot;SOAMinttl&quot;: 10,</span><br><span class="line">  &quot;IPSources&quot;: [&quot;netinfo&quot;, &quot;mesos&quot;, &quot;host&quot;]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>启动 mesos-dns</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mv mesos-dns-v0.6.0-linux-amd64 mesos-dns</span><br><span class="line">chmod +x mesos-dns</span><br><span class="line">./mesos-dns -config=config.json -v=2</span><br></pre></td></tr></table></figure><p>mesos-dns 会启动 53 和 8123 两个端口， 53 为dns端口，8123 为http api端口。  </p><h5 id="HTTP-API-接口"><a href="#HTTP-API-接口" class="headerlink" title="HTTP API 接口"></a>HTTP API 接口</h5><table><thead><tr><th>URL</th><th>说明 </th></tr></thead><tbody><tr><td> <a href="http://10.0.0.49:8123/v1/version" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/version</a></td><td>mesos-dns版本信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/config" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/config</a></td><td>mesos-dns配置信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/hosts/{host}" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/hosts/{host}</a></td><td>该host的IP地址信息</td></tr><tr><td> <a href="http://10.0.0.49:8123/v1/services/{service}" target="_blank" rel="noopener">http://10.0.0.49:8123/v1/services/{service}</a></td><td>该service的host、IP、端口信息</td></tr></tbody></table><p> 例子：</p><pre><code>http://10.0.0.49:8123/v1/hosts/nginxqq-nginx.marathon.slave.mesos</code></pre><p>  分析：marathon.slave.mesos 是固定的，mesos是condig.json中domain定义的，在往前是从节点，marathon是框架，nginx是组，nginxqq是appid </p><pre><code>http://10.0.0.49:8123/v1/services/_nginxqq-nginx._tcp.marathon.slave.mesos  </code></pre><p>  分析： _nginxqq-nginx._tcp.marathon.slave.mesos ， nginxqq容器的ID名，nginx为组名，_tcp.marathon.slave.mesos 为固定的。</p><h5 id="dig-获取mesos-dns信息"><a href="#dig-获取mesos-dns信息" class="headerlink" title="dig 获取mesos-dns信息"></a>dig 获取mesos-dns信息</h5><p>查找app所在节点的IP</p><pre><code>dig nginxqq-nginx.marathon.slave.mesos +short</code></pre><p>查找app服务端口号</p><pre><code>dig SRV _nginxqq-nginx._tcp.marathon.slave.mesos +short </code></pre><ul><li>其中 过得到的主机名 mesos-dns 是可以解析的，就是app所在的物理机。</li></ul><h4 id="docker-images"><a href="#docker-images" class="headerlink" title="docker images"></a>docker images</h4><p>创建 docker file 目录，放入所用的文件</p><pre><code>mkdir dockerfile-mesos-dnscd dockerfile-mesos-dnscp ~/mesos-dns .cp ~/config.json .</code></pre><p>编辑 Dockerfile 文件  </p><p>vim Dockerfile</p><pre><code>FROM centos:6WORKDIR /root/ADD mesos-dns /root/ADD config.json /root/EXPOSE 53 8123CMD [&quot;/root/mesos-dns&quot;, &quot;-config=/root/config.json&quot;, &quot;-v=2&quot;]</code></pre><p>生成镜像</p><pre><code>docker build -t stg-mesos-dns:0.6.0 .</code></pre><p>运行镜像</p><pre><code>docker run  -d --name=stg-mesos-dns --net=host stg-mesos-dns:0.6.0</code></pre><p>感觉文章还可以的话，帮忙点点下面的广告哦！ 谢谢支持！</p>]]></content>
      
      <categories>
          
          <category> mesos </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mesos </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
